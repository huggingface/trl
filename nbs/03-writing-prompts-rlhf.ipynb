{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YzUa8EUOcwW"
      },
      "source": [
        "# Tune a language model to generate responses to writing prompts\n",
        "> Optimise a language model to produce responses to reddit writing prompts using a dataset of writing prompt response comparisons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SksLXMnFOcwZ"
      },
      "source": [
        "## Setup experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XesXYwlZOcwa"
      },
      "source": [
        "### Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w74tkjxmOcwa"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aML_uMGEOcwb"
      },
      "outputs": [],
      "source": [
        "# hide\n",
        "import sys\n",
        "sys.path.append(\"..\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "id": "8ohWYe33ofZM",
        "outputId": "bdfc8b65-a13e-43f9-f30a-fdda3416cddd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla V100-SXM2-16GB (UUID: GPU-50fa6b69-6a5b-2dc7-2ac4-59c594b573af)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_v9PnHseoTaP",
        "outputId": "8fb22f40-8bc4-4bb0-c14a-95e0d160262e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5Ms0SU-X1uN",
        "outputId": "cc072d58-c61d-41e5-9308-efaa29449555"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.15)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.8)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.10)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.2.3)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.5.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.10.0+cu111)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (4.1.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "N34o7e3_Ocwc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import wandb\n",
        "import time\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "import torch\n",
        "import collections\n",
        "import time\n",
        "import random\n",
        "tqdm.pandas()\n",
        "\n",
        "from datasets import load_dataset, ClassLabel, load_metric\n",
        "\n",
        "from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForPreTraining\n",
        "from transformers import top_k_top_p_filtering\n",
        "from torch import nn\n",
        "from torch.nn import Identity\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "from transformers import GPT2Tokenizer\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, AdamW, get_scheduler, AutoModelForCausalLM, GPT2PreTrainedModel\n",
        "\n",
        "from accelerate import Accelerator\n",
        "\n",
        "# from trl_custom.model_value_head import LMHeadWithValueModel, respond_to_batch\n",
        "# from trl_custom.ppo import PPOTrainer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_dict(nested, sep='/'):\n",
        "    \"\"\"Flatten dictionary and concatenate nested keys with separator.\"\"\"\n",
        "    def rec(nest, prefix, into):\n",
        "        for k, v in nest.items():\n",
        "            if sep in k:\n",
        "                raise ValueError(f\"separator '{sep}' not allowed to be in key '{k}'\")\n",
        "            if isinstance(v, collections.Mapping):\n",
        "                rec(v, prefix + k + sep, into)\n",
        "            else:\n",
        "                into[prefix + k] = v\n",
        "    flat = {}\n",
        "    rec(nested, '', flat)\n",
        "    return flat\n",
        "\n",
        "def stack_dicts(stats_dicts):\n",
        "    \"\"\"Stack the values of a dict.\"\"\"\n",
        "    results = dict()\n",
        "    for k in stats_dicts[0]:\n",
        "        stats_list = [torch.flatten(d[k]) for d in stats_dicts]\n",
        "        results[k] = torch.stack(stats_list)\n",
        "    return results\n",
        "\n",
        "def add_suffix(input_dict, suffix):\n",
        "    \"\"\"Add suffix to dict keys.\"\"\"\n",
        "    return dict((k + suffix, v) for k,v in input_dict.items())\n",
        "\n",
        "# Cell\n",
        "\n",
        "def pad_to_size(tensor, size, dim=1, padding=50256):\n",
        "    \"\"\"Pad tensor to size.\"\"\"\n",
        "    t_size = tensor.size()[dim]\n",
        "    if t_size==size:\n",
        "        return tensor\n",
        "    else:\n",
        "        return torch.nn.functional.pad(tensor, (0,size-t_size), 'constant', padding)\n",
        "\n",
        "def logprobs_from_logits(logits, labels):\n",
        "    \"\"\"\n",
        "    See: https://github.com/pytorch/pytorch/issues/563#issuecomment-330103591\n",
        "    \"\"\"\n",
        "    logp = F.log_softmax(logits, dim=2)\n",
        "    logpy = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
        "    return logpy\n",
        "\n",
        "\n",
        "def whiten(values, shift_mean=True):\n",
        "    \"\"\"Whiten values.\"\"\"\n",
        "    mean, var = torch.mean(values), torch.var(values)\n",
        "    whitened = (values - mean) * torch.rsqrt(var + 1e-8)\n",
        "    if not shift_mean:\n",
        "        whitened += mean\n",
        "    return whitened\n",
        "\n",
        "def clip_by_value(x, tensor_min, tensor_max):\n",
        "    \"\"\"\n",
        "    Tensor extenstion to torch.clamp\n",
        "    https://github.com/pytorch/pytorch/issues/2793#issuecomment-428784713\n",
        "    \"\"\"\n",
        "    clipped = torch.max(torch.min(x, tensor_max), tensor_min)\n",
        "    return clipped\n",
        "\n",
        "def entropy_from_logits(logits):\n",
        "    \"\"\"Calculate entropy from logits.\"\"\"\n",
        "    pd = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    entropy = torch.logsumexp(logits, axis=-1) - torch.sum(pd*logits, axis=-1)\n",
        "    return entropy\n",
        "\n",
        "\n",
        "def average_torch_dicts(list_of_dicts):\n",
        "    \"\"\"Average values of a list of dicts wiht torch tensors.\"\"\"\n",
        "    average_dict = dict()\n",
        "    for key in list_of_dicts[0].keys():\n",
        "        average_dict[key] = torch.mean(torch.stack([d[key] for d in list_of_dicts]), axis=0)\n",
        "    return average_dict\n",
        "\n",
        "def stats_to_np(stats_dict):\n",
        "    \"\"\"Cast all torch.tensors in dict to numpy arrays.\"\"\"\n",
        "    new_dict = dict()\n",
        "    for k, v in stats_dict.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            new_dict[k] = v.detach().cpu().numpy()\n",
        "        else:\n",
        "            new_dict[k] = v\n",
        "        if np.isscalar(new_dict[k]):\n",
        "            new_dict[k] = float(new_dict[k])\n",
        "    return new_dict"
      ],
      "metadata": {
        "id": "CtaLOJeEYq6H"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ValueHead(nn.Module):\n",
        "    \"\"\"The ValueHead class implements a head for a language model that returns a scalar for each output token.\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.summary_type = config.summary_type if hasattr(config, \"summary_type\") else \"last\"\n",
        "        if self.summary_type == \"attn\":\n",
        "            raise NotImplementedError\n",
        "\n",
        "        self.summary = Identity()\n",
        "        if hasattr(config, \"summary_use_proj\") and config.summary_use_proj:\n",
        "            if hasattr(config, \"summary_proj_to_labels\") and config.summary_proj_to_labels and config.num_labels > 0:\n",
        "                num_classes = config.num_labels\n",
        "            else:\n",
        "                num_classes = config.hidden_size\n",
        "            self.summary = nn.Linear(config.hidden_size, num_classes)\n",
        "\n",
        "        self.activation = Identity()\n",
        "        if hasattr(config, \"summary_activation\") and config.summary_activation == \"tanh\":\n",
        "            self.activation = nn.Tanh()\n",
        "\n",
        "        self.first_dropout = Identity()\n",
        "        if hasattr(config, \"summary_first_dropout\") and config.summary_first_dropout > 0:\n",
        "            self.first_dropout = nn.Dropout(config.summary_first_dropout)\n",
        "\n",
        "        self.last_dropout = Identity()\n",
        "        if hasattr(config, \"summary_last_dropout\") and config.summary_last_dropout > 0:\n",
        "            self.last_dropout = nn.Dropout(config.summary_last_dropout)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, hidden_states, cls_index=None):\n",
        "        output = hidden_states\n",
        "        output = self.first_dropout(output)\n",
        "        output = self.summary(output)\n",
        "        output = self.activation(output)\n",
        "        output = self.last_dropout(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Cell\n",
        "\n",
        "class LMHeadWithValueModel(GPT2PreTrainedModel):\n",
        "    \"\"\"The LMHeadWithValueModel class implements a language model with a secondary, scalar head.\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        config.num_labels = 1\n",
        "        self.transformer = AutoModel.from_config(config)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.v_head = ValueHead(config)\n",
        "\n",
        "        self.init_weights()\n",
        "    \n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        past_key_values=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        mc_token_ids=None,\n",
        "        lm_labels=None,\n",
        "        mc_labels=None,\n",
        "    ):\n",
        "       \n",
        "        transformer_outputs = self.transformer(\n",
        "            input_ids,\n",
        "            past_key_values=past_key_values,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "\n",
        "        hidden_states = transformer_outputs[0]\n",
        "\n",
        "        lm_logits = self.lm_head(hidden_states)\n",
        "        value = self.v_head(hidden_states).squeeze(-1)\n",
        "\n",
        "        outputs = (lm_logits,) + transformer_outputs[1:] + (value,)\n",
        "        \n",
        "        return outputs\n",
        "\n",
        "# Cell\n",
        "\n",
        "def respond_to_batch(model, queries, txt_len=20, top_k=0, top_p=1.0):\n",
        "    \"\"\"Sample text from language model.\"\"\"\n",
        "    input_ids = queries\n",
        "    for i in range(txt_len):\n",
        "        # Get Logits\n",
        "        outputs = model(input_ids)\n",
        "        next_token_logits = outputs[0][:, -1, :]\n",
        "        next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "        # Sample\n",
        "        probs = F.softmax(next_token_logits, dim=-1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
        "        input_ids = torch.cat([input_ids, next_token.unsqueeze(-1)], dim=-1)\n",
        "    return input_ids[:, -txt_len:]"
      ],
      "metadata": {
        "id": "LhLUVZIGYtXM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaptiveKLController:\n",
        "    \"\"\"\n",
        "    Adaptive KL controller described in the paper:\n",
        "    https://arxiv.org/pdf/1909.08593.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, init_kl_coef, target, horizon):\n",
        "        self.value = init_kl_coef\n",
        "        self.target = target\n",
        "        self.horizon = horizon\n",
        "\n",
        "    def update(self, current, n_steps):\n",
        "        target = self.target\n",
        "        proportional_error = np.clip(current / target - 1, -0.2, 0.2)\n",
        "        mult = 1 + proportional_error * n_steps / self.horizon\n",
        "        self.value *= mult\n",
        "\n",
        "# Cell\n",
        "\n",
        "class FixedKLController:\n",
        "    \"\"\"Fixed KL controller.\"\"\"\n",
        "    def __init__(self, kl_coef):\n",
        "        self.value = kl_coef\n",
        "\n",
        "    def update(self, current, n_steps):\n",
        "        pass\n",
        "\n",
        "# Cell\n",
        "\n",
        "class PPOTrainer:\n",
        "    \"\"\"\n",
        "    The PPO_trainer uses Proximal Policy Optimization to optimise language models.\n",
        "    \"\"\"\n",
        "\n",
        "    default_params = {\n",
        "        \"lr\": 1.41e-5,\n",
        "        \"adap_kl_ctrl\": True,\n",
        "        \"init_kl_coef\":0.2,\n",
        "        \"target\": 6,\n",
        "        \"horizon\":10000,\n",
        "        \"gamma\":1,\n",
        "        \"lam\":0.95,\n",
        "        \"cliprange\": .2,\n",
        "        \"cliprange_value\":.2,\n",
        "        \"vf_coef\":.1,\n",
        "        \"batch_size\": 256,\n",
        "        \"forward_batch_size\": 16,\n",
        "        \"ppo_epochs\": 4,\n",
        "    }\n",
        "\n",
        "    def __init__(self, policy_model, ref_model, value_model, **ppo_params):\n",
        "        \"\"\"\n",
        "        Initialize PPOTrainer.\n",
        "        Args:\n",
        "            model (torch.model): Hugging Face transformer model with value head\n",
        "            ref_model (torch.model): Hugging Face transformer reference model used for KL penalty\n",
        "            ppo_params (dict or None): PPO parameters for training. Can include following keys:\n",
        "                'lr' (float): Adam learning rate, default: 1.41e-5\n",
        "                'batch_size' (int): Number of samples per optimisation step, default: 256\n",
        "                'forward_batch_size' (int): Number of samples forward passed through model at a time, default: 16\n",
        "                'ppo_epochs' (int): Number of optimisation epochs per batch of samples, default: 4\n",
        "                'gamma' (float)): Gamma parameter for advantage calculation, default: 1.\n",
        "                'lam' (float): Lambda parameter for advantage calcualation, default: 0.95\n",
        "                'cliprange_value' (float): Range for clipping values in loss calculation, default: 0.2\n",
        "                'cliprange' (float): Range for clipping in PPO policy gradient loss, default: 0.2\n",
        "                'vf_coef' (float): Scaling factor for value loss, default: 0.1\n",
        "                'adap_kl_ctrl' (bool): Use adaptive KL control, otherwise linear, default: True\n",
        "                'init_kl_coef' (float): Initial KL penalty coefficient (used for adaptive and linear control), default: 0.2\n",
        "                'target' (float): Target KL value for adaptive KL control, default: 6.0\n",
        "                'horizon' (float): Horizon for adaptive KL control, default: 10000\n",
        "        \"\"\"\n",
        "        self.ppo_params = self.default_params\n",
        "        self.ppo_params.update(ppo_params)\n",
        "\n",
        "        self.ref_model = ref_model\n",
        "        self.policy_model = policy_model\n",
        "        self.value_model = value_model\n",
        "        self.policy_optimizer = Adam(policy_model.parameters(), lr=self.ppo_params['lr'])\n",
        "        self.value_optimizer = Adam(value_model.parameters(), lr=self.ppo_params['lr'])\n",
        "\n",
        "        if self.ppo_params['adap_kl_ctrl']:\n",
        "            self.kl_ctl = AdaptiveKLController(self.ppo_params['init_kl_coef'],\n",
        "                                               self.ppo_params['target'],\n",
        "                                               self.ppo_params['horizon'])\n",
        "        else:\n",
        "            self.kl_ctl = FixedKLController(self.ppo_params['init_kl_coef'])\n",
        "\n",
        "\n",
        "    def step(self, query, response, scores):\n",
        "        \"\"\"\n",
        "        Run a PPO optimisation step.\n",
        "        args:\n",
        "            query (torch.tensor): tensor containing the encoded queries, shape [batch_size, query_length]\n",
        "            response (torch.tensor): tensor containing the encoded responses, shape [batch_size, response_length]\n",
        "            scores (torch.tensor): tensor containing the scores, shape [batch_size]\n",
        "        returns:\n",
        "            train_stats (dict): a summary of the training statistics\n",
        "        \"\"\"\n",
        "\n",
        "        bs = self.ppo_params['batch_size']\n",
        "        timing = dict()\n",
        "        t0 = time.time()\n",
        "\n",
        "        gen_len = response.shape[1]\n",
        "        model_input = torch.cat((query, response), axis=1)\n",
        "\n",
        "        t = time.time()\n",
        "        logprobs, ref_logprobs, values = self.batched_forward_pass(model_input, gen_len)\n",
        "        timing['time/ppo/forward_pass'] = time.time()-t\n",
        "\n",
        "        t = time.time()\n",
        "        rewards, non_score_reward, kl_coef = self.compute_rewards(scores, logprobs, ref_logprobs)\n",
        "        timing['time/ppo/compute_rewards'] = time.time()-t\n",
        "\n",
        "        t = time.time()\n",
        "        all_stats = []\n",
        "        idxs = list(range(bs))\n",
        "        for _ in range(self.ppo_params['ppo_epochs']):\n",
        "            random.shuffle(idxs)\n",
        "            for i in range(bs):\n",
        "                idx = idxs[i]\n",
        "                train_stats = self.train_minibatch(logprobs[idx:idx+1], values[idx:idx+1],\n",
        "                                                   rewards[idx:idx+1],\n",
        "                                                   response[idx:idx+1], model_input[idx:idx+1])\n",
        "                all_stats.append(train_stats)\n",
        "        timing['time/ppo/optimize_step'] = time.time()-t\n",
        "\n",
        "        t = time.time()\n",
        "        train_stats = stack_dicts(all_stats)\n",
        "\n",
        "        # reshape advantages/ratios such that they are not averaged.\n",
        "        train_stats['policy/advantages'] = torch.flatten(train_stats['policy/advantages']).unsqueeze(0)\n",
        "        train_stats['policy/ratio'] = torch.flatten(train_stats['policy/ratio']).unsqueeze(0)\n",
        "\n",
        "        stats = self.record_step_stats(scores=scores, logprobs=logprobs, ref_logprobs=ref_logprobs,\n",
        "                                       non_score_reward=non_score_reward, train_stats=train_stats,\n",
        "                                       kl_coef=kl_coef)\n",
        "        stats = stats_to_np(stats)\n",
        "        timing['time/ppo/calc_stats'] = time.time()-t\n",
        "\n",
        "        self.kl_ctl.update(stats['objective/kl'], self.ppo_params['batch_size'])\n",
        "\n",
        "        timing['time/ppo/total'] = time.time()-t0\n",
        "        stats.update(timing)\n",
        "        return stats\n",
        "\n",
        "    def batched_forward_pass(self, model_input, gen_len):\n",
        "        \"\"\"Calculate model outputs in multiple batches.\"\"\"\n",
        "        bs = self.ppo_params['batch_size']\n",
        "        fbs = self.ppo_params['forward_batch_size']\n",
        "        logprobs = []\n",
        "        ref_logprobs = []\n",
        "        values = []\n",
        "\n",
        "        for i in range(int(self.ppo_params['batch_size']/fbs)):\n",
        "            m_input = model_input[i*fbs:(i+1)*fbs]\n",
        "            logits, _, _ = self.policy_model(m_input)\n",
        "            _, _, v = self.value_model(m_input)\n",
        "            ref_logits, _, _ = self.ref_model(m_input)\n",
        "\n",
        "            values.append(v[:, -gen_len-1:-1].detach())\n",
        "            logprobs.append(logprobs_from_logits(logits[:,:-1,:], m_input[:,1:])[:, -gen_len:].detach())\n",
        "            ref_logprobs.append(logprobs_from_logits(ref_logits[:,:-1,:], m_input[:,1:])[:, -gen_len:].detach())\n",
        "\n",
        "        return torch.cat(logprobs), torch.cat(ref_logprobs), torch.cat(values)\n",
        "\n",
        "    def train_minibatch(self, logprobs, values, rewards, response, model_input):\n",
        "        \"\"\"Train one PPO minibatch\"\"\"\n",
        "        loss_p, train_stats  = self.loss_policy(logprobs, values, rewards, response, model_input)\n",
        "        loss_v = self.loss_value(values, rewards, response, model_input)\n",
        "        self.policy_optimizer.zero_grad()\n",
        "        self.value_optimizer.zero_grad()\n",
        "        loss_p.backward()\n",
        "        loss_v.backward()\n",
        "        self.policy_optimizer.step()\n",
        "        self.value_optimizer.step()\n",
        "        return train_stats\n",
        "\n",
        "    def compute_rewards(self, scores, logprobs, ref_logprobs):\n",
        "        \"\"\"Compute per token rewards from scores and KL-penalty.\"\"\"\n",
        "        kl = logprobs - ref_logprobs\n",
        "        non_score_reward = -self.kl_ctl.value * kl\n",
        "        rewards = non_score_reward.clone().detach()\n",
        "        rewards[:, -1] += scores\n",
        "        return rewards, non_score_reward, self.kl_ctl.value\n",
        "\n",
        "    def loss_value(self, values, rewards, response, model_input):\n",
        "        \"\"\"Calculate value loss\"\"\"\n",
        "        lastgaelam = 0\n",
        "        advantages_reversed = []\n",
        "        gen_len = response.shape[1]\n",
        "\n",
        "        for t in reversed(range(gen_len)):\n",
        "            nextvalues = values[:, t + 1] if t < gen_len - 1 else 0.0\n",
        "            delta = rewards[:, t] + self.ppo_params['gamma'] * nextvalues - values[:, t]\n",
        "            lastgaelam = delta + self.ppo_params['gamma'] * self.ppo_params['lam'] * lastgaelam\n",
        "            advantages_reversed.append(lastgaelam)\n",
        "        advantages = torch.stack(advantages_reversed[::-1]).transpose(0, 1)\n",
        "\n",
        "        returns = advantages + values\n",
        "        advantages = whiten(advantages)\n",
        "        advantages = advantages.detach()\n",
        "\n",
        "        logits, _, _ = self.policy_model(model_input)\n",
        "        _, _, vpred = self.value_model(model_input)\n",
        "\n",
        "        logprob = logprobs_from_logits(logits[:,:-1,:], model_input[:, 1:])\n",
        "\n",
        "        #only the generation part of the values/logprobs is needed\n",
        "        logprob, vpred = logprob[:, -gen_len:], vpred[:,-gen_len-1:-1]\n",
        "\n",
        "        vpredclipped = clip_by_value(vpred,\n",
        "                                     values - self.ppo_params[\"cliprange_value\"],\n",
        "                                     values + self.ppo_params[\"cliprange_value\"])\n",
        "\n",
        "        vf_losses1 = (vpred - returns)**2\n",
        "        vf_losses2 = (vpredclipped - returns)**2\n",
        "        vf_loss = .5 * torch.mean(torch.max(vf_losses1, vf_losses2))\n",
        "\n",
        "        return self.ppo_params['vf_coef'] * vf_loss\n",
        "\n",
        "    def loss_policy(self, old_logprobs, values, rewards, response, model_input):\n",
        "        \"\"\"Calculate policy loss.\"\"\"\n",
        "        lastgaelam = 0\n",
        "        advantages_reversed = []\n",
        "        gen_len = response.shape[1]\n",
        "\n",
        "        for t in reversed(range(gen_len)):\n",
        "            nextvalues = values[:, t + 1] if t < gen_len - 1 else 0.0\n",
        "            delta = rewards[:, t] + self.ppo_params['gamma'] * nextvalues - values[:, t]\n",
        "            lastgaelam = delta + self.ppo_params['gamma'] * self.ppo_params['lam'] * lastgaelam\n",
        "            advantages_reversed.append(lastgaelam)\n",
        "        advantages = torch.stack(advantages_reversed[::-1]).transpose(0, 1)\n",
        "\n",
        "        returns = advantages + values\n",
        "        advantages = whiten(advantages)\n",
        "        advantages = advantages.detach()\n",
        "\n",
        "        logits, _, _ = self.policy_model(model_input)\n",
        "        _, _, vpred = self.value_model(model_input)\n",
        "\n",
        "        logprob = logprobs_from_logits(logits[:,:-1,:], model_input[:, 1:])\n",
        "\n",
        "        #only the generation part of the values/logprobs is needed\n",
        "        logprob, vpred = logprob[:, -gen_len:], vpred[:,-gen_len-1:-1]\n",
        "\n",
        "        vpredclipped = clip_by_value(vpred,\n",
        "                                     values - self.ppo_params[\"cliprange_value\"],\n",
        "                                     values + self.ppo_params[\"cliprange_value\"])\n",
        "\n",
        "        vf_losses1 = (vpred - returns)**2\n",
        "        vf_losses2 = (vpredclipped - returns)**2\n",
        "        vf_loss = .5 * torch.mean(torch.max(vf_losses1, vf_losses2))\n",
        "        vf_clipfrac =  torch.mean(torch.gt(vf_losses2, vf_losses1).double())\n",
        "\n",
        "        ratio = torch.exp(logprob - old_logprobs)\n",
        "\n",
        "        pg_losses = -advantages * ratio\n",
        "        pg_losses2 = -advantages * torch.clamp(ratio,\n",
        "                                               1.0 - self.ppo_params['cliprange'],\n",
        "                                               1.0 + self.ppo_params['cliprange'])\n",
        "\n",
        "        pg_loss = torch.mean(torch.max(pg_losses, pg_losses2))\n",
        "        pg_clipfrac = torch.mean(torch.gt(pg_losses2, pg_losses).double())\n",
        "\n",
        "        entropy = torch.mean(entropy_from_logits(logits))\n",
        "        approxkl = .5 * torch.mean((logprob - old_logprobs)**2)\n",
        "        policykl = torch.mean(logprob - old_logprobs)\n",
        "        return_mean, return_var = torch.mean(returns), torch.var(returns)\n",
        "        value_mean, value_var = torch.mean(values), torch.var(values)\n",
        "\n",
        "        stats = dict(\n",
        "            loss=dict(policy=pg_loss, value=vf_loss),\n",
        "            policy=dict(entropy=entropy, approxkl=approxkl,policykl=policykl, clipfrac=pg_clipfrac,\n",
        "                        advantages=advantages, advantages_mean=torch.mean(advantages), ratio=ratio),\n",
        "            returns=dict(mean=return_mean, var=return_var),\n",
        "            val=dict(vpred=torch.mean(vpred), error=torch.mean((vpred - returns) ** 2),\n",
        "                     clipfrac=vf_clipfrac, mean=value_mean, var=value_var),\n",
        "        )\n",
        "        return pg_loss, flatten_dict(stats)\n",
        "\n",
        "\n",
        "    def record_step_stats(self, kl_coef, **data):\n",
        "        \"\"\"Record training step statistics.\"\"\"\n",
        "        kl = data['logprobs'] - data['ref_logprobs']\n",
        "        mean_kl = torch.mean(torch.sum(kl, axis=-1))\n",
        "        mean_entropy = torch.mean(torch.sum(-data['logprobs'], axis=1))\n",
        "        mean_non_score_reward =torch.mean(torch.sum(data['non_score_reward'], axis=1))\n",
        "        stats = {\n",
        "            'objective/kl': mean_kl,\n",
        "            'objective/kl_dist': kl,\n",
        "            'objective/logprobs': data['logprobs'],\n",
        "            'objective/ref_logprobs': data['ref_logprobs'],\n",
        "            'objective/kl_coef': kl_coef,\n",
        "            'objective/entropy': mean_entropy,\n",
        "            'ppo/mean_non_score_reward': mean_non_score_reward,\n",
        "        }\n",
        "\n",
        "        for k, v in data['train_stats'].items():\n",
        "            stats[f'ppo/{k}'] = torch.mean(v, axis=0)\n",
        "        stats['ppo/val/var_explained'] = 1 - stats['ppo/val/error'] / stats['ppo/returns/var']\n",
        "        return stats"
      ],
      "metadata": {
        "id": "dIla8m6KY2Px"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K6OuopKOcwc"
      },
      "source": [
        "### Train Reward Model on Writing Prompt Response Comparisons"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_response_dataset = load_dataset(\"rewardsignal/reddit_writing_prompts\", data_files=\"prompt_responses_full.csv\", split='train[:80%]')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNRynAtbZHsY",
        "outputId": "14ef1aa5-0739-4cfb-fbe6-e2760ece324b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using custom data configuration rewardsignal--reddit_writing_prompts-dd5d2a64487ab606\n",
            "Reusing dataset csv (/root/.cache/huggingface/datasets/csv/rewardsignal--reddit_writing_prompts-dd5d2a64487ab606/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_response_dataset.features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6PzHCnYdm99",
        "outputId": "1faff948-3842-48e4-dcd7-e6c502fd6c52"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Unnamed: 0': Value(dtype='int64', id=None),\n",
              " 'num_responses': Value(dtype='float64', id=None),\n",
              " 'prompt': Value(dtype='string', id=None),\n",
              " 'prompt_created_utc': Value(dtype='float64', id=None),\n",
              " 'prompt_id': Value(dtype='string', id=None),\n",
              " 'prompt_score': Value(dtype='float64', id=None),\n",
              " 'response': Value(dtype='string', id=None),\n",
              " 'response_children': Value(dtype='string', id=None),\n",
              " 'response_created_utc': Value(dtype='float64', id=None),\n",
              " 'response_id': Value(dtype='string', id=None),\n",
              " 'response_rank': Value(dtype='float64', id=None),\n",
              " 'response_score': Value(dtype='float64', id=None),\n",
              " 'score_bin': Value(dtype='float64', id=None)}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer_name = input()\n",
        "# tokenizer_name = 'distilgpt2'\n",
        "# tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n",
        "# prompt_prefix = \"Writing Prompt: \"\n",
        "# response_prefix = \"Response: \"\n",
        "\n",
        "# def preprocess_text_function(examples):\n",
        "#   examples[\"prompt\"] = [prompt.replace('[WP] ', prompt_prefix) for prompt in examples[\"prompt\"]]\n",
        "#   examples[\"response\"] = [response_prefix + response for response in examples[\"response\"]]\n",
        "#   return tokenizer(examples['prompt'], examples['response'], truncation=True)\n",
        "\n",
        "# tokenized_reward_dataset = prompt_response_dataset.map(preprocess_text_function, batched=True)"
      ],
      "metadata": {
        "id": "Nn2gZrhWbH1z"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_reward_dataset = tokenized_reward_dataset.remove_columns(['Unnamed: 0', 'prompt_id', 'prompt', 'prompt_score', 'prompt_created_utc', 'response_id', 'response', 'response_score', 'response_created_utc', 'num_responses', 'response_children', 'score_bin']\n",
        "# )\n",
        "# tokenized_reward_dataset = tokenized_reward_dataset.rename_column('response_rank', 'labels')\n",
        "# def preprocess_labels_function(examples):\n",
        "#   examples['labels'] = 1 if (examples['labels'] == 0) else 0\n",
        "#   return examples\n",
        "# tokenized_reward_dataset = tokenized_reward_dataset.map(preprocess_labels_function)\n",
        "# tokenized_reward_dataset = tokenized_reward_dataset.cast_column('labels', ClassLabel(num_classes=2, names=['best', 'not-best'], names_file=None, id=None))\n",
        "# tokenized_reward_dataset.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "XfNAYT3jpSKW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reward_train_dataset = tokenized_reward_dataset.shuffle(seed=42).select(range(3*len(tokenized_reward_dataset)//4))\n",
        "# reward_eval_dataset = tokenized_reward_dataset.shuffle(seed=42).select(range(3*len(tokenized_reward_dataset)//4, len(tokenized_reward_dataset)))"
      ],
      "metadata": {
        "id": "XmSXC91IpUZ1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # model_name = input()\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "# reward_model_name = tokenizer_name\n",
        "# reward_model = AutoModelForSequenceClassification.from_pretrained(reward_model_name, num_labels=2)\n",
        "# reward_model.config.pad_token_id = reward_model.config.eos_token_id\n",
        "\n",
        "# train_dataloader = DataLoader(\n",
        "#     reward_train_dataset, shuffle=True, batch_size=4, collate_fn=data_collator\n",
        "# )\n",
        "# eval_dataloader = DataLoader(\n",
        "#     reward_eval_dataset, batch_size=4, collate_fn=data_collator\n",
        "# )\n",
        "\n",
        "# optimizer = AdamW(reward_model.parameters(), lr=3e-5)\n",
        "# accelerator = Accelerator()\n",
        "# train_dataloader, eval_dataloader, reward_model, optimizer = accelerator.prepare(train_dataloader, eval_dataloader, reward_model, optimizer)\n",
        "# num_epochs = 1\n",
        "# num_training_steps = num_epochs * len(train_dataloader)\n",
        "# lr_scheduler = get_scheduler(\n",
        "#     \"linear\",\n",
        "#     optimizer=optimizer,\n",
        "#     num_warmup_steps=0,\n",
        "#     num_training_steps=num_training_steps,\n",
        "# )\n",
        "\n",
        "# progress_bar = tqdm(range(num_training_steps))"
      ],
      "metadata": {
        "id": "6kan7BQ5qo1_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "!apt install git-lfs\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774,
          "referenced_widgets": [
            "7384175267a4465f98b75109c25e739a",
            "03f6cd1c25794927ad43f58540b9c5c1",
            "eb4edfea8a3548239d7253fe8f29195d",
            "ac94a9b0ea004899819b31ed50ab5e99",
            "641f8c298ded499ca7f38cb8beb1cb01",
            "3c44a20ab29a4c9b9e98216808a50fd9",
            "c96b5711da3747fe87b700cc422e4a69",
            "4eb53fd6949740d393367a74faa44196",
            "cc103ae6ef2149bbaa30561bfdb3efba",
            "8d41ad6eb9214379a735c0790f2cdcda",
            "0e2a54db19c14b3dbf7256f814d8f6ab",
            "0c5052950b6c42df8464bbf15236d3b9",
            "0a0a43868b3445d59f1f65cc34474477",
            "2a22beb094154b4091e6c1f8a8e87963",
            "f18c4c8507d7450988c5c2ae6eb6f44c",
            "1e21a4d723ae4075bae050ba06c70339",
            "7049bcd4f63f4a65ace981a5c8b05804"
          ]
        },
        "id": "b3_FsD6uE8sl",
        "outputId": "b54db863-041f-431d-aad5-435640419e64"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.1.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (21.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface_hub) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface_hub) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (2.10)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (2.3.4-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7384175267a4465f98b75109c25e739a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5SCteoZXOcwd"
      },
      "outputs": [],
      "source": [
        "# reward_model.train()\n",
        "# for epoch in range(num_epochs):\n",
        "#     for batch in train_dataloader:\n",
        "#         outputs = reward_model(**batch)\n",
        "#         loss = outputs.loss\n",
        "#         accelerator.backward(loss)\n",
        "#         optimizer.step()\n",
        "#         lr_scheduler.step()\n",
        "#         optimizer.zero_grad()\n",
        "#         progress_bar.update(1)\n",
        "# reward_model.push_to_hub(tokenizer_name + \"_reward_model_02\", use_temp_dir=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# len(eval_dataloader)"
      ],
      "metadata": {
        "id": "r8mpwVk6qoDQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uffB8CqiOcwg"
      },
      "outputs": [],
      "source": [
        "# metric = load_metric(\"accuracy\")\n",
        "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "# reward_model.to(device)\n",
        "# reward_model.eval()\n",
        "# count = 0\n",
        "# for batch in eval_dataloader:\n",
        "#     count += 1\n",
        "#     batch = {k: v.to(device) for k, v in batch.items()}\n",
        "#     with torch.no_grad():\n",
        "#         outputs = reward_model(**batch)\n",
        "\n",
        "#     logits = outputs.logits\n",
        "#     predictions = torch.argmax(logits, dim=-1)\n",
        "#     metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "#     if (count % 1000 == 0):\n",
        "#       print(count)\n",
        "\n",
        "# metric.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "untba8DWOcwg"
      },
      "source": [
        "### Fine-tune LM on responding to writing prompts "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "CBrg9zihOcwg",
        "outputId": "b8ca7641-c4bd-4b48-fa89-094e4126f91f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/rewardsignal--reddit_writing_prompts-dd5d2a64487ab606/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-5bc08506140d8b27.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/rewardsignal--reddit_writing_prompts-dd5d2a64487ab606/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-08bf676bb657ea28.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/rewardsignal--reddit_writing_prompts-dd5d2a64487ab606/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-a13ff98572a05dfb.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/rewardsignal--reddit_writing_prompts-dd5d2a64487ab606/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-12fa3a11c1a663cb.arrow\n"
          ]
        }
      ],
      "source": [
        "# tokenizer_name = input()\n",
        "tokenizer_name = 'distilgpt2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n",
        "prompt_prefix = \"Writing Prompt: \"\n",
        "response_prefix = \"Response: \"\n",
        "\n",
        "def preprocess_text_function(examples):\n",
        "  examples[\"prompt\"] = [prompt.replace('[WP] ', prompt_prefix) for prompt in examples[\"prompt\"]]\n",
        "  examples[\"response\"] = [response_prefix + response for response in examples[\"response\"]]\n",
        "  return tokenizer(examples['prompt'], examples['response'], truncation=True)\n",
        "\n",
        "tokenized_prompt_response_dataset = prompt_response_dataset.map(preprocess_text_function, batched=True, remove_columns=['Unnamed: 0', 'prompt_id', 'prompt', 'prompt_score', 'prompt_created_utc', 'response_id', 'response', 'response_score', 'response_created_utc', 'num_responses', 'response_children', 'score_bin', 'response_rank'], num_proc=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 512\n",
        "def group_texts(examples):\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "        # customize this part to your needs.\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "    # Split by chunks of max_len.\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "tokenized_prompt_response_dataset = tokenized_prompt_response_dataset.map(group_texts, batched=True, batch_size = 1000, num_proc = 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzwJMXgYbrNC",
        "outputId": "f01098aa-6339-4427-a8ed-b830289b7152"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/rewardsignal--reddit_writing_prompts-dd5d2a64487ab606/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-c95c481401b11fc4.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/rewardsignal--reddit_writing_prompts-dd5d2a64487ab606/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-c5840e979d00732e.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/rewardsignal--reddit_writing_prompts-dd5d2a64487ab606/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-33fcc7f14548838d.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/rewardsignal--reddit_writing_prompts-dd5d2a64487ab606/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-5af14d4e66eb4003.arrow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenized_prompt_response_dataset[1][\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "132gx_ZrdB4Y",
        "outputId": "27222090-388d-4adf-c0a4-80b07cfd368b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'So why did we let that dragon inside, exactly? I asked.\\n\\nCompany policy, based on our past experiences, he said. The last woman whose emotional support dragon was denied entry, came back with her financial support lawyer. He blew out a smoke ring. Believe me when I say it is cheaper that way, and by a lot.Writing Prompt: \"Ma\\'am you can\\'t bring your emotional support dragon inside the restaurant.\"Response: \"Well, why the hell not?!\"\\n\\nThe host flicked her eyes up to meet the unaffected gaze of the giant blue beast, and then back down to the woman who wore the bracelet which assigned it as hers.\\n\\n\"Ma\\'am, this is a small restaurant, he won\\'t fit inside,\" the host explained patiently, a bit astounded by the idea that this woman believed her enormous, full grown dragon would fit.\\n\\nThe lady huffed, then stammered, \"Well...what about outside seating.\"\\n\\nThe host surveyed the sidewalk area shaded by an awning that flapped about in the slight breeze. The tables were full but she spotted a family of four with their Manticore curled up beside them, the owner of whom was signing the receipt.\\n\\n\"Uhhh...it\\'ll be a 10 minute wait?\" the host tried.\\n\\nThe woman seemed reluctantly appeased by this and leaned against one of the awning poles to wait.\\n\\nThe family left, Manticore in tow, and the host snatched the receipt and rushed inside to get a busser on the table as soon as possible. As she squeezed past the commotion at table 5, where a mother and her many children, each with their own personal Pixie, was trying to control the havoc that the Pixies were wreaking, the manager caught the host\\'s attention.\\n\\nIn a conspiratorial voice, the manager said, \"Is that a dragon I see out there?\" The host nodded. The manager sighed and closed their eyes, pinching the bridge of their nose, as if a migraine had suddenly overtaken them. \"Did you already promise her a table?\" Another nod. \"It\\'s okay, it\\'s not your fault. You\\'re new, someone should have told you. We have a strict \\'no dragons\\' policy after the fiasco about a decade ago when someone\\'s fledgling yawned and set the awning on fire.\"\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_prompt_response_dataset.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "NiBYxbJWd4o6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# supervised_train_dataset = tokenized_prompt_response_dataset.shuffle(seed=42).select(range(3*len(tokenized_prompt_response_dataset)//4))\n",
        "# supervised_eval_dataset = tokenized_prompt_response_dataset.shuffle(seed=42).select(range(3*len(tokenized_prompt_response_dataset)//4, len(tokenized_prompt_response_dataset)))"
      ],
      "metadata": {
        "id": "kskJLMlYeAp9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_name = input()\n",
        "# from transformers import AutoModelForCausalLM\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "# supervised_model_name = tokenizer_name\n",
        "# supervised_model = AutoModelForCausalLM.from_pretrained(supervised_model_name, num_labels=2)\n",
        "\n",
        "# train_dataloader = DataLoader(\n",
        "#     supervised_train_dataset, shuffle=True, batch_size=4, collate_fn=data_collator\n",
        "# )\n",
        "# eval_dataloader = DataLoader(\n",
        "#     supervised_eval_dataset, batch_size=4, collate_fn=data_collator\n",
        "# )\n",
        "\n",
        "# optimizer = AdamW(supervised_model.parameters(), lr=3e-5)\n",
        "# accelerator = Accelerator()\n",
        "# train_dataloader, eval_dataloader, supervised_model, optimizer = accelerator.prepare(train_dataloader, eval_dataloader, supervised_model, optimizer)\n",
        "# num_epochs = 1\n",
        "# num_training_steps = num_epochs * len(train_dataloader)\n",
        "# lr_scheduler = get_scheduler(\n",
        "#     \"linear\",\n",
        "#     optimizer=optimizer,\n",
        "#     num_warmup_steps=0,\n",
        "#     num_training_steps=num_training_steps,\n",
        "# )\n",
        "\n",
        "# progress_bar = tqdm(range(num_training_steps))"
      ],
      "metadata": {
        "id": "LHhf85zzeVqA"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# supervised_model.train()\n",
        "# for epoch in range(num_epochs):\n",
        "#     for batch in train_dataloader:\n",
        "#         outputs = supervised_model(**batch)\n",
        "#         loss = outputs.loss\n",
        "#         accelerator.backward(loss)\n",
        "#         optimizer.step()\n",
        "#         lr_scheduler.step()\n",
        "#         optimizer.zero_grad()\n",
        "#         progress_bar.update(1)\n",
        "# supervised_model.push_to_hub(tokenizer_name + \"_supervised_model_01\", use_temp_dir=True)"
      ],
      "metadata": {
        "id": "qS_N1ez8e5a4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "# saved_model = AutoModelForCausalLM.from_pretrained(\"anshr/distilgpt2_supervised_model_01\")\n",
        "# saved_model.to(device)\n",
        "# count = 0\n",
        "# for batch in eval_dataloader:\n",
        "#     count += 1\n",
        "#     batch = {k: v.to(device) for k, v in batch.items()}\n",
        "#     with torch.no_grad():\n",
        "#         outputs = saved_model.generate(**batch, max_length=512, min_length = 200)\n",
        "#         print(tokenizer.batch_decode(outputs, max_length = 512))\n",
        "#     if count == 10: break\n",
        "\n"
      ],
      "metadata": {
        "id": "fNChaDJ0y9wQ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H1xo6-KOcwi"
      },
      "source": [
        "## Load models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miR36jNYOcwj"
      },
      "source": [
        "### Load Reward Model\n",
        "We load a DistilGPT2 classifier fine-tuned on the writing prompt dataset to predict whether a response is the best response (as judged by karma) or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "uqqAwnpSOcwj"
      },
      "outputs": [],
      "source": [
        "reward_model = AutoModelForSequenceClassification.from_pretrained(\"anshr/distilgpt2_reward_model_02\")\n",
        "reward_model_tokenizer = AutoTokenizer.from_pretrained(\"anshr/distilgpt2_reward_model_02\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atxMVcRyOcwk"
      },
      "source": [
        "The model outputs are the logits for the not-best and best classes. We will use the logits for the best class as a reward signal for the language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YZQBaQdOcwl"
      },
      "source": [
        "### Load pre-trained DistilGPT2 language models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO6Uohs5Ocwl"
      },
      "source": [
        "We load the DistilGPT2 model with a value head and the tokenizer. We load the model twice; the first model is optimized while the second model serves as a reference to calculate the KL-divergence from the starting point. This serves as an additional reward signal in the PPO training to make sure the optimized model does not deviate too much from the original language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "STmOA8z6Ocwl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5185007-b95c-42ea-8659-d70293a49e2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of LMHeadWithValueModel were not initialized from the model checkpoint at anshr/distilgpt2_supervised_model_01 and are newly initialized: ['v_head.summary.weight', 'v_head.summary.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of LMHeadWithValueModel were not initialized from the model checkpoint at anshr/distilgpt2_supervised_model_01 and are newly initialized: ['v_head.summary.weight', 'v_head.summary.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# policy_model = LMHeadWithValueModel.from_pretrained(\"anshr/distilgpt2_supervised_model_01\")\n",
        "policy_model_ref = LMHeadWithValueModel.from_pretrained(\"anshr/distilgpt2_supervised_model_01\")\n",
        "value_model = LMHeadWithValueModel.from_pretrained(\"anshr/distilgpt2_supervised_model_01\")\n",
        "policy_tokenizer = AutoTokenizer.from_pretrained(\"anshr/distilgpt2_supervised_model_01\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ujpd45sOcwm"
      },
      "source": [
        "### Move models to GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb5blwklOcwm"
      },
      "source": [
        "If `cuda` is available move the computations to the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "6A0aeLENOcwm"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "B-69JJ3FOcwm"
      },
      "outputs": [],
      "source": [
        "_ = reward_model.to(device)\n",
        "# _ = policy_model.to(device)\n",
        "_ = policy_model_ref.to(device)\n",
        "_ = value_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"steps\": 25600,\n",
        "    \"batch_size\": 16,\n",
        "    \"forward_batch_size\": 4,\n",
        "    \"ppo_epochs\": 4,   \n",
        "    \"txt_in_len\": 20,\n",
        "    \"txt_out_len\": 100,\n",
        "    \"lr\": 1.41e-5,\n",
        "    \"init_kl_coef\":0.2,\n",
        "    \"target\": 6,\n",
        "    \"horizon\":10000,\n",
        "    \"gamma\":1,\n",
        "    \"lam\":0.95,\n",
        "    \"cliprange\": .2,\n",
        "    \"cliprange_value\":.2,\n",
        "    \"vf_coef\":.1, \n",
        "}"
      ],
      "metadata": {
        "id": "PJB34Tg_SF7c"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_reward_batch_from_txt(text_list, tokenizer, device):\n",
        "  # tokenize\n",
        "    tensors = [tokenizer.encode(txt, return_tensors=\"pt\").to(device) for txt in text_list]\n",
        "    \n",
        "    # find max length to pad to\n",
        "    max_len = max([t.size()[1] for t in tensors])\n",
        "    \n",
        "    # get padded tensors and attention masks\n",
        "    padded_tensors = []\n",
        "    attention_masks = []\n",
        "    for tensor in tensors:\n",
        "        attention_mask = torch.ones(tensor.size(), device=device)\n",
        "        padded_tensors.append(pad_to_size(tensor, max_len, padding=0))\n",
        "        attention_masks.append(pad_to_size(attention_mask, max_len, padding=0))\n",
        "    \n",
        "    # stack all tensors\n",
        "    padded_tensors = torch.cat(padded_tensors)\n",
        "    attention_masks = torch.cat(attention_masks)  \n",
        "    \n",
        "    return padded_tensors, attention_masks"
      ],
      "metadata": {
        "id": "a-RKOUIDimuy"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io9OtDXIOcwn"
      },
      "source": [
        "## Optimize model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAYIpvs7Ocwn"
      },
      "source": [
        "**Steps**\n",
        "\n",
        "The training loop consists of the following steps:\n",
        "1. Get a batch of queries\n",
        "2. Get the query responses from the policy\n",
        "3. Join query and responses and tokenize for BERT analysis\n",
        "4. Get sentiments for query/responses from BERT\n",
        "5. Optimize policy with PPO using the (query, response, reward) triplet\n",
        "6. Log all the training statistics\n",
        "\n",
        "**Forward batching**\n",
        "\n",
        "Since the models can be fairly big and we want to rollout large PPO batches this can lead to out-of-memory errors when doing the forward passes for text generation and sentiment analysis. We introduce the parameter `forward_batch_size` to split the forward passes into smaller batches. Although this hurts performance a little this is neglectible compared to the computations of the backward passes when optimizing the model. The same parameter is used in the `PPOTrainer` when doing forward passes. The `batch_size` should multiple of `forward_batch_size`.\n",
        "\n",
        "**Training time**\n",
        "\n",
        "This step takes **~2h** on a P6000 GPU with the above specified settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "cQgypKdaOcwn"
      },
      "outputs": [],
      "source": [
        "# ppo_trainer = PPOTrainer(policy_model, policy_model_ref, value_model, **config)\n",
        "# fbs = config['forward_batch_size']\n",
        "\n",
        "# for epoch in tqdm(range(int(np.ceil(config[\"steps\"]/config['batch_size'])))):\n",
        "#     torch.cuda.empty_cache()\n",
        "#     logs = dict()\n",
        "#     game_data = dict()\n",
        "#     timing = dict()\n",
        "#     t0 = time.time()\n",
        "    \n",
        "#     #### get a batch from the dataset\n",
        "#     df_batch = tokenized_prompt_response_dataset.shuffle().select(range(config[\"batch_size\"]))\n",
        "#     game_data['query'] = [policy_tokenizer.decode(_) for _ in df_batch['input_ids'][:, :config[\"txt_in_len\"]].to(device)]\n",
        "#     query_tensors = df_batch['input_ids'][:, :config[\"txt_in_len\"]].to(device)\n",
        "    \n",
        "#     #### get response from gpt2\n",
        "#     t = time.time()\n",
        "#     total_length = config['txt_in_len']+config['txt_out_len']\n",
        "#     response_tensors = []\n",
        "#     for i in range(int(config['batch_size']/fbs)):\n",
        "#         response  = respond_to_batch(policy_model, query_tensors[i*fbs:(i+1)*fbs],\n",
        "#                                      txt_len=config['txt_out_len'])\n",
        "#         response_tensors.append(response)\n",
        "#     response_tensors = torch.cat(response_tensors)\n",
        "#     game_data['response'] = [policy_tokenizer.decode(response_tensors[i, :]) for i in range(config['batch_size'])]\n",
        "#     timing['time/get_response'] = time.time()-t\n",
        "\n",
        "#     #### tokenize text for sentiment analysis\n",
        "#     t = time.time()\n",
        "#     texts = [q + r for q,r in zip(game_data['query'], game_data['response'])]\n",
        "#     reward_inputs, attention_masks = build_reward_batch_from_txt(texts, reward_model_tokenizer, device)    \n",
        "#     timing['time/build_input_sentiment'] = time.time()-t\n",
        "\n",
        "#     #### get sentiment score\n",
        "#     t = time.time()\n",
        "#     rewards = []\n",
        "#     for i in range(int(config['batch_size']/fbs)):\n",
        "#         res = reward_model.forward(input_ids=reward_inputs[i*fbs:(i+1)*fbs],\n",
        "#                                       attention_mask=attention_masks[i*fbs:(i+1)*fbs])[0][:, 1].detach()\n",
        "#         rewards.append(res)\n",
        "#     rewards = torch.cat(rewards)\n",
        "#     timing['time/get_sentiment_preds'] = time.time()-t\n",
        "    \n",
        "#     #### Run PPO training \n",
        "#     t = time.time()\n",
        "#     stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
        "#     timing['time/optimization'] = time.time()-t\n",
        "     \n",
        "#     #### Log everything\n",
        "#     timing['time/epoch'] = time.time()-t0\n",
        "#     table_rows = [list(r) for r in zip(game_data['query'], game_data['response'], rewards.cpu().tolist())]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cuda.empty_cache()\n",
        "# policy_model.push_to_hub(tokenizer_name + \"_trained_policy_model_01\", use_temp_dir=True)"
      ],
      "metadata": {
        "id": "xk4raM2coghh"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLoPS8sjOcwo"
      },
      "source": [
        "## Model inspection\n",
        "Let's inspect some examples from the IMDB dataset. We can use `gpt2_model_ref` to compare the tuned model `gpt2_model` against the model before optimisation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy_model = AutoModel.from_pretrained(\"anshr/distilgpt2_trained_policy_model_01\")\n",
        "_ = policy_model.to(device)"
      ],
      "metadata": {
        "id": "KRk9xppAq4Nq",
        "outputId": "e82053fc-29bc-4cd4-9971-4d8db93a17db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154,
          "referenced_widgets": [
            "1b9424b4a1f14fe9a7e83e91a85afaca",
            "97bc18aa57494d729483941af6499720",
            "0c245cd5fe4f423ebf7d2ffa96c1e388",
            "d43543eb275d4e88b4e1d7b32080a392",
            "ce72e04d22164cacbc8b4634fff67280",
            "9f30f01b2d8146b3b9120e1b9d878532",
            "53bf60ce739f4c5881fb8285b4790c9a",
            "58bc4632dc9445f486d015bad8e548d0",
            "a8d97943729b4bc295da876b7c441d3f",
            "efcdce69b0104921a49456c120106e8a",
            "19397b6bf5984a458d17dc73da17e64b",
            "db2980508bde4a378f4c69b78baa4e3c",
            "74b83058fcd648269d867b0008442062",
            "91bd70809d654a719ed69fd4b8058d55",
            "49bd45a5c2914fbeb1d89bca0c25ded2",
            "213b034f1401458da56f79a482edf78b",
            "65d9c0c9e29b44f4af6af7c2ed672178",
            "9c312dc6be03463e8d60589e115d9148",
            "02702f95e989466eada303c37d9ae35c",
            "c4d1677f4f644fcbb63f5cd34112924e",
            "a55e72353a9d4cb8828f346a93d56a54",
            "1963a49754b2466a9b9aeff0ff0fbda9"
          ]
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b9424b4a1f14fe9a7e83e91a85afaca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/319M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db2980508bde4a378f4c69b78baa4e3c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at anshr/distilgpt2_trained_policy_model_01 were not used when initializing GPT2Model: ['lm_head.weight', 'v_head.summary.weight', 'v_head.summary.bias']\n",
            "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "zKHoyWaqOcwo",
        "outputId": "a4a465b6-35d5-4367-bbde-cfd16e211830",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               query  \\\n",
              "0   and began laughing loudly, laughter reverbera...   \n",
              "1   start to see flashes that dont look like any...   \n",
              "2  red. It was so stupid of me to agree to this, ...   \n",
              "3   *win*.\\n\\nThe Cyberdemon stared blankly at th...   \n",
              "\n",
              "                                   response (before)  \\\n",
              "0  fact attempt at finding the right squeaker she...   \n",
              "1   she refused law. No matter how one was in cha...   \n",
              "2  .\\n\\nIts been nearly two days since Id forgo...   \n",
              "3  . He was so fast.\\n\\n-\\n\\nEnd of comments/s/co...   \n",
              "\n",
              "                                    response (after)  rewards (before)  \\\n",
              "0  > bec I they cons is se bqu backEageE/ want...         -1.799845   \n",
              "1   cons with I~ulE I unorsome thatiaockont f...         -2.124275   \n",
              "2  ess cons[ Iine^ I) I1 Iz Iver I sh ch I IK I...         -1.789432   \n",
              "3  ookictation neE' afterUage have cons [ chage...         -1.634552   \n",
              "\n",
              "   rewards (after)  \n",
              "0        -2.146531  \n",
              "1        -2.294234  \n",
              "2        -2.326010  \n",
              "3        -1.855445  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fc48afed-ce74-4393-893a-77f6683b81b0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>query</th>\n",
              "      <th>response (before)</th>\n",
              "      <th>response (after)</th>\n",
              "      <th>rewards (before)</th>\n",
              "      <th>rewards (after)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>and began laughing loudly, laughter reverbera...</td>\n",
              "      <td>fact attempt at finding the right squeaker she...</td>\n",
              "      <td>&gt; bec I they cons is se bqu backEageE/ want...</td>\n",
              "      <td>-1.799845</td>\n",
              "      <td>-2.146531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>start to see flashes that dont look like any...</td>\n",
              "      <td>she refused law. No matter how one was in cha...</td>\n",
              "      <td>cons with I~ulE I unorsome thatiaockont f...</td>\n",
              "      <td>-2.124275</td>\n",
              "      <td>-2.294234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>red. It was so stupid of me to agree to this, ...</td>\n",
              "      <td>.\\n\\nIts been nearly two days since Id forgo...</td>\n",
              "      <td>ess cons[ Iine^ I) I1 Iz Iver I sh ch I IK I...</td>\n",
              "      <td>-1.789432</td>\n",
              "      <td>-2.326010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>*win*.\\n\\nThe Cyberdemon stared blankly at th...</td>\n",
              "      <td>. He was so fast.\\n\\n-\\n\\nEnd of comments/s/co...</td>\n",
              "      <td>ookictation neE' afterUage have cons [ chage...</td>\n",
              "      <td>-1.634552</td>\n",
              "      <td>-1.855445</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc48afed-ce74-4393-893a-77f6683b81b0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fc48afed-ce74-4393-893a-77f6683b81b0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fc48afed-ce74-4393-893a-77f6683b81b0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "#### get a batch from the dataset\n",
        "bs = 4\n",
        "game_data = dict()\n",
        "df_batch = tokenized_prompt_response_dataset.shuffle().select(range(bs))\n",
        "game_data['query'] = [policy_tokenizer.decode(_) for _ in df_batch['input_ids']]\n",
        "query_tensors = df_batch['input_ids'].to(device)\n",
        "\n",
        "#### get response from gpt2 and gpt2_ref\n",
        "total_length = config['txt_in_len']+config['txt_out_len']\n",
        "response_tensors_ref  = respond_to_batch(policy_model_ref, query_tensors, txt_len=config['txt_out_len'])\n",
        "game_data['response (before)'] = [policy_tokenizer.decode(response_tensors_ref[i, :]) for i in range(bs)]\n",
        "\n",
        "response_tensors  = respond_to_batch(policy_model, query_tensors, txt_len=config['txt_out_len'])\n",
        "game_data['response (after)'] = [policy_tokenizer.decode(response_tensors[i, :]) for i in range(bs)]\n",
        "\n",
        "#### sentiment analysis of query/response pairs before/after\n",
        "texts = [q + r for q,r in zip(game_data['query'], game_data['response (before)'])]\n",
        "reward_inputs, attention_masks = build_reward_batch_from_txt(texts, reward_model_tokenizer, device)    \n",
        "rewards = reward_model.forward(input_ids=reward_inputs, attention_mask=attention_masks)[0][:, 1].detach()\n",
        "game_data['rewards (before)'] = rewards.cpu().numpy()\n",
        "\n",
        "texts = [q + r for q,r in zip(game_data['query'], game_data['response (after)'])]\n",
        "reward_inputs, attention_masks = build_reward_batch_from_txt(texts, reward_model_tokenizer, device)    \n",
        "rewards = reward_model.forward(input_ids=reward_inputs, attention_mask=attention_masks)[0][:, 1].detach()\n",
        "game_data['rewards (after)'] = rewards.cpu().numpy()\n",
        "\n",
        "# store results in a dataframe\n",
        "df_results = pd.DataFrame(game_data)\n",
        "df_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk4tHlPKOcwp"
      },
      "source": [
        "Looking at the reward mean/median of the generated sequences we observe a significant difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40oKYlbeOcwp",
        "outputId": "8b738b3d-bf37-44b3-bcfd-7cac55d2d9dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "rewards (before)    0.211196\n",
              "rewards (after)     3.068743\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "median:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "rewards (before)    0.296938\n",
              "rewards (after)     3.997496\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print('mean:')\n",
        "display(df_results.mean())\n",
        "print()\n",
        "print('median:')\n",
        "display(df_results.median())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQNK6C5DOcwp"
      },
      "source": [
        "## Save model\n",
        "Finally, we save the model to disk for later usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqUfAyzjOcwp",
        "outputId": "ee5fcb46-9953-4659-a131-d5e9c8d356ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('gpt2-imdb-pos/vocab.json',\n",
              " 'gpt2-imdb-pos/merges.txt',\n",
              " 'gpt2-imdb-pos/special_tokens_map.json',\n",
              " 'gpt2-imdb-pos/added_tokens.json')"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.makedirs('gpt2-imdb-pos')\n",
        "gpt2_model.save_pretrained('gpt2-imdb-pos')\n",
        "gpt2_tokenizer.save_pretrained('gpt2-imdb-pos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toahL9oeOcwp"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "name": "03-writing-prompts-rlhf.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "background_execution": "on"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7384175267a4465f98b75109c25e739a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_03f6cd1c25794927ad43f58540b9c5c1",
              "IPY_MODEL_eb4edfea8a3548239d7253fe8f29195d",
              "IPY_MODEL_ac94a9b0ea004899819b31ed50ab5e99",
              "IPY_MODEL_641f8c298ded499ca7f38cb8beb1cb01",
              "IPY_MODEL_3c44a20ab29a4c9b9e98216808a50fd9"
            ],
            "layout": "IPY_MODEL_c96b5711da3747fe87b700cc422e4a69"
          }
        },
        "03f6cd1c25794927ad43f58540b9c5c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4eb53fd6949740d393367a74faa44196",
            "placeholder": "",
            "style": "IPY_MODEL_cc103ae6ef2149bbaa30561bfdb3efba",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "eb4edfea8a3548239d7253fe8f29195d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_8d41ad6eb9214379a735c0790f2cdcda",
            "placeholder": "",
            "style": "IPY_MODEL_0e2a54db19c14b3dbf7256f814d8f6ab",
            "value": ""
          }
        },
        "ac94a9b0ea004899819b31ed50ab5e99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_0c5052950b6c42df8464bbf15236d3b9",
            "style": "IPY_MODEL_0a0a43868b3445d59f1f65cc34474477",
            "tooltip": ""
          }
        },
        "641f8c298ded499ca7f38cb8beb1cb01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a22beb094154b4091e6c1f8a8e87963",
            "placeholder": "",
            "style": "IPY_MODEL_f18c4c8507d7450988c5c2ae6eb6f44c",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. <br> <i>Logging in with your username and password is deprecated and\nwon't be possible anymore in the near future. You can still use them for now by\nclicking below.</i> </center>"
          }
        },
        "3c44a20ab29a4c9b9e98216808a50fd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Use password",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_1e21a4d723ae4075bae050ba06c70339",
            "style": "IPY_MODEL_7049bcd4f63f4a65ace981a5c8b05804",
            "tooltip": ""
          }
        },
        "c96b5711da3747fe87b700cc422e4a69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "4eb53fd6949740d393367a74faa44196": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc103ae6ef2149bbaa30561bfdb3efba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d41ad6eb9214379a735c0790f2cdcda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e2a54db19c14b3dbf7256f814d8f6ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c5052950b6c42df8464bbf15236d3b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a0a43868b3445d59f1f65cc34474477": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "2a22beb094154b4091e6c1f8a8e87963": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f18c4c8507d7450988c5c2ae6eb6f44c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e21a4d723ae4075bae050ba06c70339": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7049bcd4f63f4a65ace981a5c8b05804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "1b9424b4a1f14fe9a7e83e91a85afaca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97bc18aa57494d729483941af6499720",
              "IPY_MODEL_0c245cd5fe4f423ebf7d2ffa96c1e388",
              "IPY_MODEL_d43543eb275d4e88b4e1d7b32080a392"
            ],
            "layout": "IPY_MODEL_ce72e04d22164cacbc8b4634fff67280"
          }
        },
        "97bc18aa57494d729483941af6499720": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f30f01b2d8146b3b9120e1b9d878532",
            "placeholder": "",
            "style": "IPY_MODEL_53bf60ce739f4c5881fb8285b4790c9a",
            "value": "Downloading: 100%"
          }
        },
        "0c245cd5fe4f423ebf7d2ffa96c1e388": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58bc4632dc9445f486d015bad8e548d0",
            "max": 1042,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a8d97943729b4bc295da876b7c441d3f",
            "value": 1042
          }
        },
        "d43543eb275d4e88b4e1d7b32080a392": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efcdce69b0104921a49456c120106e8a",
            "placeholder": "",
            "style": "IPY_MODEL_19397b6bf5984a458d17dc73da17e64b",
            "value": " 1.02k/1.02k [00:00&lt;00:00, 43.9kB/s]"
          }
        },
        "ce72e04d22164cacbc8b4634fff67280": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f30f01b2d8146b3b9120e1b9d878532": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53bf60ce739f4c5881fb8285b4790c9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58bc4632dc9445f486d015bad8e548d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8d97943729b4bc295da876b7c441d3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "efcdce69b0104921a49456c120106e8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19397b6bf5984a458d17dc73da17e64b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db2980508bde4a378f4c69b78baa4e3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_74b83058fcd648269d867b0008442062",
              "IPY_MODEL_91bd70809d654a719ed69fd4b8058d55",
              "IPY_MODEL_49bd45a5c2914fbeb1d89bca0c25ded2"
            ],
            "layout": "IPY_MODEL_213b034f1401458da56f79a482edf78b"
          }
        },
        "74b83058fcd648269d867b0008442062": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65d9c0c9e29b44f4af6af7c2ed672178",
            "placeholder": "",
            "style": "IPY_MODEL_9c312dc6be03463e8d60589e115d9148",
            "value": "Downloading: 100%"
          }
        },
        "91bd70809d654a719ed69fd4b8058d55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02702f95e989466eada303c37d9ae35c",
            "max": 333972887,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c4d1677f4f644fcbb63f5cd34112924e",
            "value": 333972887
          }
        },
        "49bd45a5c2914fbeb1d89bca0c25ded2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a55e72353a9d4cb8828f346a93d56a54",
            "placeholder": "",
            "style": "IPY_MODEL_1963a49754b2466a9b9aeff0ff0fbda9",
            "value": " 319M/319M [00:07&lt;00:00, 55.7MB/s]"
          }
        },
        "213b034f1401458da56f79a482edf78b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65d9c0c9e29b44f4af6af7c2ed672178": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c312dc6be03463e8d60589e115d9148": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02702f95e989466eada303c37d9ae35c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4d1677f4f644fcbb63f5cd34112924e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a55e72353a9d4cb8828f346a93d56a54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1963a49754b2466a9b9aeff0ff0fbda9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}