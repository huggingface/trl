train_split: train[:1000]
eval_split: valid[:100]
model_name: EleutherAI/pythia-1b-deduped
log_with: "wandb"
learning_rate: 1e-5
lr_scheduler_type: cosine
num_warmup_steps: 100
weight_decay: 0.05
load_in_8bit: False
use_peft: False
fp16: True
bf16: False
streaming: False
gradient_checkpointing: False
gradient_accumulation_steps: 1
per_device_train_batch_size: 16
per_device_eval_batch_size: 8
train_completions: False
num_train_epochs: 3
# optimizer_type: adafactor
