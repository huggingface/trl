model_name: /home/toolkit/huggingface/tldr_sft_pythia410m_fp32_trainall_3epochs
reward_adapter_name: /home/toolkit/huggingface/tldr_gptrm1b_pythia410m_fp16_1epoch
gold_model_name: /home/toolkit/huggingface/tldr_gptrm_sft_pythia1b_fp16_2epochs
steps: 10000
load_in_8bit: False
bf16: False
fp16: True
learning_rate: 1e-5
lora_all_linear: True
lora_r: 8
lora_alpha: 32
lora_dropout: 0.
gradient_accumulation_steps: 4
mini_batch_size: 8
batch_size: 32
eval_steps: 10
input_ids_input: False
strip_prompt: True
save_steps: 100
gold_eval_greedy: True
