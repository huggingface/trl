model_name: EleutherAI/pythia-1b-deduped
load_in_8bit: False
bf16: False
fp16: True
use_lora: False
per_device_train_batch_size: 4
per_device_eval_batch_size: 2
gradient_accumulation_steps: 8
gradient_checkpointing: False
learning_rate: 1e-6
optimizer_type: adamw_torch
lr_scheduler_type: linear
num_train_epochs: 3
seq_length: 560
eval_steps: 250
