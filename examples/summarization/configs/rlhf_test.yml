train_split: train[:1000]
model_name: /home/toolkit/huggingface/tldr_sft_pythia410m_fp32_trainall_3epochs
reward_adapter_name: /home/toolkit/huggingface/tldr_gptrm1b_pythia410m_fp16_1epoch
steps: 1000
load_in_8bit: False
bf16: False
fp16: True
learning_rate: 1e-5
lora_all_linear: True
lora_r: 8
lora_alpha: 32
lora_dropout: 0.
mini_batch_size: 1
batch_size: 8
