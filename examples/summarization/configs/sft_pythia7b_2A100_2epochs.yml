model_name: "EleutherAI/pythia-6.9b-deduped"
log_with: "wandb"
learning_rate: 1e-5
lr_scheduler_type: cosine
num_warmup_steps: 100
weight_decay: 0.05
load_in_8bit: True
use_peft: True
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
bf16: True
per_device_train_batch_size: 8
per_device_eval_batch_size: 4
gradient_accumulation_steps: 4
num_train_epochs: 2
max_steps: -1
gradient_checkpointing: False
streaming: False
train_completions: True
resume_from_checkpoint: /home/toolkit/trl/results/900b151b9f77665d86d94056a001f1be/code/results/checkpoint-3000
