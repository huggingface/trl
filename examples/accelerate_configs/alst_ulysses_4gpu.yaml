# ALST/Ulysses Sequence Parallelism with 2D Parallelism (DP + CP) for 4 GPUs
#
# This configuration enables 2D parallelism:
# - Context Parallelism (cp_size=2): Sequences split across 2 GPUs
# - Data Parallelism (dp_shard_size=2): Model/optimizer sharded across 2 GPUs
# - Total: 4 GPUs (2 Ã— 2)
#
# Set parallelism_config in your training script:
#   parallelism_config = ParallelismConfig(
#       cp_backend="deepspeed",
#       cp_size=2,
#       dp_shard_size=2,  # Calculated as: num_gpus // cp_size
#       cp_handler=DeepSpeedContextParallelConfig(...)
#   )

compute_environment: LOCAL_MACHINE
debug: false
deepspeed_config:
  zero_stage: 3
  seq_parallel_communication_data_type: bf16
  offload_optimizer_device: none
  offload_param_device: none
  zero3_init_flag: false
  zero3_save_16bit_model: false
distributed_type: DEEPSPEED
downcast_bf16: 'no'
machine_rank: 0
main_training_function: main
mixed_precision: bf16
num_machines: 1
num_processes: 4  # Total number of GPUs
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
parallelism_config:
  parallelism_config_dp_replicate_size: 1
  parallelism_config_dp_shard_size: 2  # Enables 2D parallelism with CP
  parallelism_config_tp_size: 1
  parallelism_config_cp_size: 2  # Context parallel size
  parallelism_config_cp_backend: deepspeed
  parallelism_config_cp_seq_length_is_variable: true
  parallelism_config_cp_attn_implementation: flash_attention_2
