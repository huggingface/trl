{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35919\n",
      "[2025-02-20 06:06:47] server_args=ServerArgs(model_path='Qwen/Qwen2.5-0.5B-Instruct', tokenizer_path='Qwen/Qwen2.5-0.5B-Instruct', tokenizer_mode='auto', load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, quantization=None, context_length=None, device='cuda', served_model_name='Qwen/Qwen2.5-0.5B-Instruct', chat_template=None, is_embedding=False, revision=None, skip_tokenizer_init=False, host='0.0.0.0', port=35919, mem_fraction_static=0.88, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, cpu_offload_gb=0, prefill_only_one_req=False, tp_size=1, stream_interval=1, stream_output=False, random_seed=728043385, constrained_json_whitespace_pattern=None, watchdog_timeout=300, download_dir=None, base_gpu_id=0, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key=None, file_storage_pth='sglang_storage', enable_cache_report=False, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_draft_model_path=None, speculative_algorithm=None, speculative_num_steps=5, speculative_num_draft_tokens=64, speculative_eagle_topk=8, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, return_hidden_states=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, enable_flashinfer_mla=False)\n",
      "[2025-02-20 06:06:56 TP0] Init torch distributed begin.\n",
      "[2025-02-20 06:06:56 TP0] Load weight begin. avail mem=78.84 GB\n",
      "[2025-02-20 06:06:57 TP0] Using model weights format ['*.safetensors']\n",
      "[2025-02-20 06:06:57 TP0] No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.02it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.01it/s]\n",
      "\n",
      "[2025-02-20 06:06:57 TP0] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=77.79 GB\n",
      "[2025-02-20 06:06:57 TP0] KV Cache is allocated. K size: 34.16 GB, V size: 34.16 GB.\n",
      "[2025-02-20 06:06:57 TP0] Memory pool end. avail mem=8.94 GB\n",
      "[2025-02-20 06:06:57 TP0] Capture cuda graph begin. This can take up to several minutes.\n",
      "100%|██████████| 23/23 [00:06<00:00,  3.62it/s]\n",
      "[2025-02-20 06:07:04 TP0] Capture cuda graph end. Time elapsed: 6.36 s\n",
      "[2025-02-20 06:07:04 TP0] max_total_num_tokens=5970520, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4097, context_len=32768\n",
      "[2025-02-20 06:07:04] INFO:     Started server process [2508578]\n",
      "[2025-02-20 06:07:04] INFO:     Waiting for application startup.\n",
      "[2025-02-20 06:07:04] INFO:     Application startup complete.\n",
      "[2025-02-20 06:07:04] INFO:     Uvicorn running on http://0.0.0.0:35919 (Press CTRL+C to quit)\n",
      "[2025-02-20 06:07:05] INFO:     127.0.0.1:43292 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "[2025-02-20 06:07:05] INFO:     127.0.0.1:43296 - \"GET /get_model_info HTTP/1.1\" 200 OK\n",
      "[2025-02-20 06:07:05 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-02-20 06:07:06] INFO:     127.0.0.1:43308 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-02-20 06:07:06] The server is fired up and ready to roll!\n",
      "\n",
      "\n",
      "                    NOTE: Typically, the server runs in a separate terminal.\n",
      "                    In this notebook, we run the server and notebook code together, so their outputs are combined.\n",
      "                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.\n",
      "                    We are running those notebooks in a CI parallel environment, so the throughput is not representative of the actual performance.\n",
      "                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-20 06:12:29 TP0] Prefill batch. #new-seq: 3, #new-token: 36, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-02-20 06:12:29 TP0] Prefill batch. #new-seq: 6, #new-token: 72, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 3, #queue-req: 0\n",
      "[2025-02-20 06:12:29] INFO:     127.0.0.1:58684 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-02-20 06:12:29 TP0] Decode batch. #running-req: 0, #token: 0, token usage: 0.00, gen throughput (token/s): 0.91, #queue-req: 0\n",
      "[2025-02-20 06:12:31 TP0] Scheduler hit an exception: Traceback (most recent call last):\n",
      "  File \"/home/misc/jinpan/.local/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 1825, in run_scheduler_process\n",
      "    scheduler.event_loop_overlap()\n",
      "  File \"/home/misc/jinpan/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/misc/jinpan/.local/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 494, in event_loop_overlap\n",
      "    self.process_input_requests(recv_reqs)\n",
      "  File \"/home/misc/jinpan/.local/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 580, in process_input_requests\n",
      "    output = self._request_dispatcher(recv_req)\n",
      "  File \"/home/misc/jinpan/.local/lib/python3.10/site-packages/sglang/utils.py\", line 440, in __call__\n",
      "    return fn(obj)\n",
      "  File \"/home/misc/jinpan/.local/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 1689, in update_weights_from_distributed\n",
      "    success, message = self.tp_worker.update_weights_from_distributed(recv_req)\n",
      "  File \"/home/misc/jinpan/.local/lib/python3.10/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 226, in update_weights_from_distributed\n",
      "    success, message = self.worker.update_weights_from_distributed(recv_req)\n",
      "  File \"/home/misc/jinpan/.local/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 201, in update_weights_from_distributed\n",
      "    success, message = self.model_runner.update_weights_from_distributed(\n",
      "  File \"/home/misc/jinpan/.local/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 493, in update_weights_from_distributed\n",
      "    dtype if isinstance(dtype, torch.dtype) else getattr(torch, dtype)\n",
      "  File \"/home/misc/jinpan/.local/lib/python3.10/site-packages/torch/__init__.py\", line 2562, in __getattr__\n",
      "    raise AttributeError(f\"module '{__name__}' has no attribute '{name}'\")\n",
      "AttributeError: module 'torch' has no attribute 'torch.float32'\n",
      "\n",
      "[2025-02-20 06:12:31] Received sigquit from a child proces. It usually means the child failed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from sglang.utils import launch_server_cmd\n",
    "from sglang.utils import wait_for_server, terminate_process\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "\n",
    "server_process, port = launch_server_cmd(\n",
    "    \"python -m sglang.launch_server --model-path Qwen/Qwen2.5-0.5B-Instruct --port 32232 --host 0.0.0.0\"\n",
    ")\n",
    "\n",
    "print(port)\n",
    "\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminate_process(server_process)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
