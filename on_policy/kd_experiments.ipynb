{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92a98cfe-8caa-4476-81fa-f1adc9ca897a",
   "metadata": {},
   "source": [
    "## Knowledge Distillation\n",
    "\n",
    "Knowledge distillation (KD) is widely used for compressing a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. Auto-regressive sequence models, such as language models (LMs), have shown impressive capabilities in numerous tasks, where the key to this success is often scaling the amount of training data as well as the number of model parameters. However, scaling parameter count comes at a cost, and the deployment of such models is limited by either their inference cost or memory footprint. Thus, a crucial goal for practical use of large capable models is to compress them by reducing their parameter count, while retaining as much as possible of their performance.\n",
    "\n",
    "One of the prevalent techniques for compressing models is knowledge distillation (Hinton et al., 2015). Distillation is the process of training a model– the student– to replicate the knowledge of another model– the teacher– on a specific set of tasks. Typically, the student has fewer parameters than the teacher and as such, distillation can improve task-specific performance while maintaining lower inference cost and memory footprint than the teacher.\n",
    "\n",
    "The large model could be an ensemble of separately trained models or a single large model trained with a very strong regularizer such as dropout. Once the large model has been trained, we can then use a different kind of training, which we call \"distillation\" to transfer the knowledge from the large model to a small model that is more suitable for deployment. Strong dropout helps ensure that the model learns robust and generalizable features rather than overfitting the training data.\n",
    "\n",
    "However, current KD methods[1] for auto-regressive sequence models suffer from distribution mismatch between output sequences seen during training and those generated by student during inference. \n",
    "\n",
    "> Why this train-inference mismatch occurs:\n",
    "\n",
    "- Auto-regressive nature of language models: These models generate text one token at a time, using previously generated tokens as context for the next prediction.\n",
    "\n",
    "- Training process: During training, the student model is typically exposed to complete, correct sequences from the training data or teacher-generated sequences. The model learns to predict the next token given the correct previous tokens.\n",
    "\n",
    "- Inference (generation) process: At inference time, the model generates text from scratch or continues from a given prompt. As it generates, it uses its own previous outputs as context for subsequent tokens.\n",
    "\n",
    "- The mismatch: During training, the model always sees \"correct\" or \"expert-generated\" contexts. During inference, the model sees its own generated context, which may contain errors or be less optimal than the training contexts. As generation progresses, these small differences can compound, leading to increasingly divergent contexts.\n",
    "\n",
    "- Consequences: The partial sequences encountered during inference can be quite different from those seen in training. The model may not have been trained to handle or recover from its own errors or less-than-optimal generations.\n",
    "\n",
    "\n",
    "To address this issue, the authors [On-Policy Distillation paper] introduce Generalized Knowledge Distillation (GKD). \n",
    "\n",
    "> Instead of solely relying on a fixed set of output sequences, GKD trains the student on its self-generated output sequences by leveraging feedback from the teacher. \n",
    "\n",
    "> GKD offers flexibility to employ alternative loss functions between the student and teacher, which may be useful when the student lacks the expressivity to mimic the teacher's distribution. \n",
    "\n",
    "\n",
    "![Alt text](gkd_algo.png)\n",
    "\n",
    "Teacher model: `Qwen2-7B-Instruct` \n",
    "\n",
    "Student model: `Qwen2-0.5B-Instruct`\n",
    "\n",
    "\n",
    "**High level tasks:**\n",
    "\n",
    "1. SFT student model on Teacher completions dataset. \n",
    "\n",
    "2. Use the SFT model to generate the output sequences on the fly with temperature of 1 to encourage diversity in generated sequences.\n",
    "\n",
    "3. Obtain token level feedback from teacher's logits and leverage the GKDTrainer from Kashif's branch in TRL where we choose the divergence to optimize between teacher and student distributions\n",
    "\n",
    "[1] Current KD methods for auto-regressive sequence models require, generating a fixed set of output sequences from the teacher model (Supervised KD) or a fixed dataset of sequences that the teacher can label by assigning token-level probabilities. \n",
    "\n",
    "References: \n",
    "\n",
    "- https://arxiv.org/abs/1503.02531 \n",
    "- https://arxiv.org/pdf/2306.13649\n",
    "- https://arxiv.org/abs/2106.05237\n",
    "- https://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html \n",
    "\n",
    "\n",
    "Plan is to start with something a bit smaller (to validate it works before scaling up):\n",
    "\n",
    "- Distill Qwen2-7B-Instruct to Qwen2-0.5B\n",
    "- Use LMSYS prompts as the source of generating student / teacher completions\n",
    "- `GKDTrainer` branch from Kashif R.: https://github.com/huggingface/trl/pull/1814\n",
    "- Dataset: https://huggingface.co/datasets/andito/chatbot_arena_completions \n",
    "- PR: https://github.com/huggingface/llm-swarm/pull/31/commits/f50230ca5a0cc880e6aab88127bb2dedae0368c7 \n",
    "- PPO Trainer: https://huggingface.co/docs/trl/ppo_trainer \n",
    "- GKD Trainer example script: https://github.com/kashif/trl/blob/gkd-trainer/examples/scripts/gkd.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "699eb716-4b8e-45fe-a3be-4900e793fe45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.1 MB 5.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow\n",
      "  Downloading pyarrow-16.1.0-cp39-cp39-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 40.8 MB 121.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.23.5-py3-none-any.whl (402 kB)\n",
      "\u001b[K     |████████████████████████████████| 402 kB 108.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[K     |████████████████████████████████| 547 kB 123.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.42.4-py3-none-any.whl (9.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.3 MB 102.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading torch-2.3.1-cp39-cp39-manylinux1_x86_64.whl (779.1 MB)\n",
      "\u001b[K     |██████████▍                     | 253.3 MB 185.8 MB/s eta 0:00:03"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 779.1 MB 2.5 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.18.1-cp39-cp39-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.0 MB 60.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting trl\n",
      "  Downloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "\u001b[K     |████████████████████████████████| 245 kB 69.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading accelerate-0.32.1-py3-none-any.whl (314 kB)\n",
      "\u001b[K     |████████████████████████████████| 314 kB 90.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard\n",
      "  Downloading tensorboard-2.17.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.5 MB 65.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tzdata>=2022.7\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[K     |████████████████████████████████| 345 kB 57.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.22.4\n",
      "  Downloading numpy-2.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.3 MB 71.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[K     |████████████████████████████████| 505 kB 59.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /home/user/miniconda/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/user/miniconda/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.16.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/user/miniconda/lib/python3.9/site-packages (from huggingface_hub->-r requirements.txt (line 3)) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/user/miniconda/lib/python3.9/site-packages (from huggingface_hub->-r requirements.txt (line 3)) (4.12.2)\n",
      "Requirement already satisfied: requests in /home/user/miniconda/lib/python3.9/site-packages (from huggingface_hub->-r requirements.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/user/miniconda/lib/python3.9/site-packages (from huggingface_hub->-r requirements.txt (line 3)) (4.61.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/user/miniconda/lib/python3.9/site-packages (from huggingface_hub->-r requirements.txt (line 3)) (6.0.1)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[K     |████████████████████████████████| 177 kB 59.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 114.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
      "\u001b[K     |████████████████████████████████| 193 kB 67.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[K     |████████████████████████████████| 116 kB 95.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec[http]<=2024.5.0,>=2023.1.0\n",
      "  Downloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "\u001b[K     |████████████████████████████████| 316 kB 61.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 65.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow-hotfix\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Collecting tqdm>=4.42.1\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 23.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /home/user/miniconda/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (23.2.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (304 kB)\n",
      "\u001b[K     |████████████████████████████████| 304 kB 116.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240 kB)\n",
      "\u001b[K     |████████████████████████████████| 240 kB 114.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 125.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/user/miniconda/lib/python3.9/site-packages (from requests->huggingface_hub->-r requirements.txt (line 3)) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/miniconda/lib/python3.9/site-packages (from requests->huggingface_hub->-r requirements.txt (line 3)) (2.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/user/miniconda/lib/python3.9/site-packages (from requests->huggingface_hub->-r requirements.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user/miniconda/lib/python3.9/site-packages (from requests->huggingface_hub->-r requirements.txt (line 3)) (1.26.6)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 70.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.20,>=0.19\n",
      "  Downloading tokenizers-0.19.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.6 MB 123.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2024.5.15-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (774 kB)\n",
      "\u001b[K     |████████████████████████████████| 774 kB 28.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.22.4\n",
      "  Downloading numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 18.2 MB 70.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 196.0 MB 137 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[K     |████████████████████████████████| 823 kB 45.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 124.2 MB 25.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting triton==2.3.1\n",
      "  Downloading triton-2.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 168.1 MB 69 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /home/user/miniconda/lib/python3.9/site-packages (from torch->-r requirements.txt (line 6)) (3.1.4)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 56.5 MB 16.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 22.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.7 MB 24.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.1 MB 15.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 731.7 MB 41 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 176.2 MB 18.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[K     |████████████████████████████████| 99 kB 23.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sympy\n",
      "  Downloading sympy-1.13.0-py3-none-any.whl (6.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.2 MB 29.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 121.6 MB 35.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 410.6 MB 17 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12\n",
      "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.3 MB 48.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading pillow-10.4.0-cp39-cp39-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.5 MB 65.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tyro>=0.5.11\n",
      "  Downloading tyro-0.8.5-py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 66.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting shtab>=1.5.6\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Collecting docstring-parser>=0.16\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Collecting eval-type-backport>=0.1.3\n",
      "  Downloading eval_type_backport-0.2.0-py3-none-any.whl (5.9 kB)\n",
      "Collecting rich>=11.1.0\n",
      "  Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[K     |████████████████████████████████| 240 kB 58.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/user/miniconda/lib/python3.9/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl->-r requirements.txt (line 8)) (2.18.0)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[K     |████████████████████████████████| 87 kB 32.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: psutil in /home/user/miniconda/lib/python3.9/site-packages (from accelerate->-r requirements.txt (line 9)) (6.0.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 66.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 67.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.4\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 41.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.48.2\n",
      "  Downloading grpcio-1.64.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 47.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "\u001b[K     |████████████████████████████████| 227 kB 42.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf!=4.24.0,<5.0.0,>=3.19.6\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[K     |████████████████████████████████| 294 kB 43.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /home/user/miniconda/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 10)) (52.0.0.post20210125)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/user/miniconda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 10)) (8.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/user/miniconda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 10)) (3.19.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/user/miniconda/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 10)) (2.1.5)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[K     |████████████████████████████████| 536 kB 39.8 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, multidict, frozenlist, yarl, tqdm, nvidia-cusparse-cu12, nvidia-cublas-cu12, mpmath, mdurl, fsspec, filelock, async-timeout, aiosignal, tzdata, triton, sympy, pytz, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusolver-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, numpy, networkx, markdown-it-py, huggingface-hub, dill, aiohttp, xxhash, torch, tokenizers, shtab, safetensors, rich, regex, pyarrow-hotfix, pyarrow, pandas, multiprocess, eval-type-backport, docstring-parser, werkzeug, tyro, transformers, tensorboard-data-server, protobuf, pillow, markdown, grpcio, datasets, accelerate, absl-py, trl, torchvision, tensorboard\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.61.2\n",
      "    Uninstalling tqdm-4.61.2:\n",
      "      Successfully uninstalled tqdm-4.61.2\n",
      "Successfully installed absl-py-2.1.0 accelerate-0.32.1 aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.20.0 dill-0.3.8 docstring-parser-0.16 eval-type-backport-0.2.0 filelock-3.15.4 frozenlist-1.4.1 fsspec-2024.5.0 grpcio-1.64.1 huggingface-hub-0.23.5 markdown-3.6 markdown-it-py-3.0.0 mdurl-0.1.2 mpmath-1.3.0 multidict-6.0.5 multiprocess-0.70.16 networkx-3.2.1 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 pandas-2.2.2 pillow-10.4.0 protobuf-4.25.3 pyarrow-16.1.0 pyarrow-hotfix-0.6 pytz-2024.1 regex-2024.5.15 rich-13.7.1 safetensors-0.4.3 shtab-1.7.1 sympy-1.13.0 tensorboard-2.17.0 tensorboard-data-server-0.7.2 tokenizers-0.19.1 torch-2.3.1 torchvision-0.18.1 tqdm-4.66.4 transformers-4.42.4 triton-2.3.1 trl-0.9.6 tyro-0.8.5 tzdata-2024.1 werkzeug-3.0.3 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "files = !ls\n",
    "\n",
    "if 'requirements.txt' in files:\n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf686fea-4848-461b-9391-3fca157e6b49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install pyarrow\n",
    "# !pip install huggingface_hub\n",
    "# !pip install datasets\n",
    "# !pip install transformers\n",
    "# !pip install torch\n",
    "# !pip install torchvision\n",
    "# !pip install trl\n",
    "# !pip install accelerate -U\n",
    "#!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d21c968-bfb2-41ae-9171-cf5fc9272dd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/kd_exps/trl/on_policy'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3672a065-d80b-42b7-bcc1-1d1d8217f549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "import accelerate\n",
    "import gc\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from huggingface_hub import HfFolder, Repository, create_repo, login\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca9806c9-6a6e-4853-9844-177dcd80a2ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Number of available GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA Available: {cuda_available}\")\n",
    "\n",
    "# Get the number of available GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of available GPUs: {num_gpus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "065793d5-3909-4122-a17a-b59e3eeaf4aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_gpu_memory():\n",
    "    \"\"\"\n",
    "    Function to print current GPU memory usage\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Current GPU memory available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        print(f\"Current GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        print(f\"Current GPU memory cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc20381e-c46f-4e8e-9413-51005f1e3474",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory available: 42.30 GB\n",
      "Current GPU memory allocated: 0.00 GB\n",
      "Current GPU memory cached: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2451a6cf-5216-40ff-b9d2-e1f406a734dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Read Qwen2-7B-Instruct completions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "788ff9c8-fc43-4277-a933-663ab738c54f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = load_dataset(\"andito/chatbot_arena_completions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4373b5a2-a669-4ea3-9e3b-edbb4a9e1ca4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question_id', 'messages'],\n",
       "        num_rows: 32980\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bac25c3b-a390-4ab6-8ea1-e021817ce917",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_id': ['58210e39b3fd4441a2bd4a518bb44c2d'],\n",
       " 'messages': [[{'content': 'What is the difference between OpenCL and CUDA?',\n",
       "    'role': 'user'},\n",
       "   {'content': \"CUDA (Compute Unified Device Architecture) and OpenCL (Open Computing Language) are two popular GPU computing platforms developed by different companies.\\n\\nCUDA is a parallel computing platform and application programming interface (API) model created by NVIDIA. It was specifically developed for NVIDIA GPU hardware and is based on NVIDIA's architecture, making it highly optimized for specific NVIDIA GPUs. CUDA allows developers to directly access the processing power of the GPU and develop parallel-accelerated applications that can run at essentially the same speed as NVIDIA processors. It has a proprietary nature with exclusive use of NVIDIA GPUs and adherence to NVIDIA's hardware and software standards.\\n\\nOpenCL, on the other hand, is a platform-independent (not tied to a specific hardware vendor) open standard for parallel programming that was designed by the Khronos Group. The primary goal of OpenCL is to provide a universal framework for programming GPUs, CPUs, and other processing units. OpenCL supports a wide variety of hardware from different manufacturers and seeks to maximize performance by being compatible with different hardware architectures, such as CPUs, GPUs, and specialized DSPs. OpenCL provides a similar set of features to CUDA but with the advantage of supporting multiple architectures.\\n\\nThus, the difference between them mainly lies in their architecture compatibility, optimization, and the hardware they target:\\n\\n1. **Architecture Compatibility:**\\n   - CUDA is optimized primarily for Nvidia GPUs, while OpenCL can run on a wide range of devices, including GPUs from Nvidia, AMD, and Intel, as well as CPUs and other specialized hardware.\\n\\n2. **Optimization and Performance:**\\n   - CUDA provides more direct access to GPU resources for parallel programming, typically resulting in finer control and potentially higher performance for Nvidia GPUs. \\n   - OpenCL, while being more versatile, might have a slightly steeper learning curve and more overhead for programming across different hardware types.\\n\\n3. **Standardization and Portability:**\\n   - CUDA is closed-source and not portable between devices from different vendors.\\n   - OpenCL is an open standard, making it portable between different hardware architectures and potentially easier to adapt to future technology advancements, though it may require more effort to optimize for each target platform.\\n\\nIn summary, CUDA is tightly integrated with Nvidia hardware, offering high performance for applications whose requirements align well with Nvidia's architecture. On the other hand, OpenCL is a portable and open standard that works across different vendors, providing broader compatibility at the cost of potentially less optimized performance.\",\n",
       "    'role': 'assistant'}]]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['train'][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc6e0dcf-c897-4cb7-b417-47586282bf33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030321406913280776"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split test / eval set\n",
    "test_size = 1000 / len(df['train'])\n",
    "test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b41480f-a820-440d-8848-394069df6147",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question_id', 'messages'],\n",
       "        num_rows: 31980\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question_id', 'messages'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the dataset\n",
    "split_dataset = df['train'].train_test_split(test_size=test_size, seed=42)\n",
    "split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb1ca018-4e26-4678-a533-c738c689d767",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question_id', 'messages'],\n",
       "        num_rows: 31980\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question_id', 'messages'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new DatasetDict with both splits\n",
    "df_v1 = DatasetDict({\n",
    "                     'train': split_dataset['train'],\n",
    "                     'test': split_dataset['test']\n",
    "                   })\n",
    "\n",
    "df_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf41f47e-b0c1-4433-bb5b-9d98b197d5cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.1 SFT student model on Teacher completions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9fab844-a232-4cb8-9b37-72fc472d8ea8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1978f635-1d27-4132-90c3-a8c5686a48d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save to disk\n",
    "#tokenizer.save_pretrained(os.getcwd() + \"/model\")\n",
    "#model.save_pretrained(os.getcwd() + \"/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e37a59cb-fc53-47fa-bcb4-46033c6cb2c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load model from disk\n",
    "tokenizer = AutoTokenizer.from_pretrained(os.getcwd() + \"/model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(os.getcwd() + \"/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7abe6512-4b65-4007-9aa7-e52c4c6ea5b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eos_token': '<|im_end|>',\n",
       " 'pad_token': '<|endoftext|>',\n",
       " 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# special tokens\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0533f297-0d84-4709-bfef-0e8d31e134dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6422c0-4eaf-4005-a74f-b1d74951ef50",
   "metadata": {},
   "source": [
    "Let's add special tokens and format the conversation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f983ec29-27b5-4d7d-abbe-531751fef3e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To Pandas\n",
    "pd_dict = {}\n",
    "\n",
    "for split, data in df_v1.items():\n",
    "    \n",
    "    pd_dict[split] = df_v1[split].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bf5b968-6957-4a99-882d-0f3e2e058825",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Train DF shape: (31980, 2)'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Train DF shape: {pd_dict['train'].shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "441ca5f7-f305-4316-a718-96c604c01d5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Test DF shape: (1000, 2)'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Test DF shape: {pd_dict['test'].shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0baa959-8ee6-4728-b2f0-1df9b7ef65be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to format the conversation dataset\n",
    "def format_conversation(sample):\n",
    "    \n",
    "    #print('sample', sample)\n",
    "    \n",
    "    text = \"\"\n",
    "    for message in sample:  # Assuming single conversation per example\n",
    "        \n",
    "        if message['role'] == 'user':\n",
    "            text = f\"<|im_start|>### User: {message['content']}<|im_end|>\"\n",
    "        \n",
    "        elif message['role'] == 'assistant':\n",
    "            text += f\"<|im_start|>### Assistant: {message['content']}\"\n",
    "            return text + tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21fda73e-2bb4-4a28-a883-4c45197a6553",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply format function to the dataset\n",
    "pd_dict['train']['texts'] = pd_dict['train'].apply(lambda x: format_conversation(x['messages']), axis=1)\n",
    "pd_dict['test']['texts'] = pd_dict['test'].apply(lambda x: format_conversation(x['messages']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6c95209-8ff7-4205-8b1a-fe4798cd5d7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Max Length: 11796'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max Sequence Length\n",
    "max_length_train = max(len(text) for text in pd_dict['train']['texts'])\n",
    "max_length_test = max(len(text) for text in pd_dict['test']['texts'])\n",
    "\n",
    "max_length = max(max_length_train, max_length_test) \n",
    "\n",
    "f\"Max Length: {max_length}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f022d04-9550-4a93-84dd-278e6596d1a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To HF Dataset\n",
    "hf_dict = {}\n",
    "\n",
    "for split, df in pd_dict.items():\n",
    "            \n",
    "    hf_df = Dataset.from_pandas(df)\n",
    "        \n",
    "    hf_dict[split] = hf_df\n",
    "    \n",
    "hf_dataset = DatasetDict(hf_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bd48a6-34e6-4bbb-bd71-e77c48e8837f",
   "metadata": {},
   "source": [
    "#### 1.2 SFTTrainer\n",
    "\n",
    "**Loss function: standard cross-entropy loss**[1]\n",
    "\n",
    "This is the primary loss function used for language model training. It measures the difference between the predicted probability distribution of tokens and the actual distribution (Completions from Teacher model). \n",
    "\n",
    "> The loss is calculated token-by-token across the entire sequence.\n",
    "\n",
    "> It's then averaged over all non-ignored tokens in the batch.\n",
    "\n",
    "> The model predicts the next token given all the previous tokens, thus it is called an autoregressive model.\n",
    "\n",
    "\n",
    "Loss for a single token prediction: `L = -Σ(y_i * log(p_i))`\n",
    "\n",
    "Where:\n",
    "\n",
    "    - `y_i` is the true probability of class i (usually 1 for the correct class, 0 for others)\n",
    "    - `p_i` is the predicted probability of class i\n",
    "\n",
    "\n",
    "[1] https://discuss.huggingface.co/t/fine-tune-with-sfttrainer/67311 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28bea7f2-d6ba-4c65-9ff6-dc39350f350c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory available: 42.30 GB\n",
      "Current GPU memory allocated: 0.00 GB\n",
      "Current GPU memory cached: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "299d83e1-4a46-49ea-b9cc-9f709adc6441",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul 12 01:57:08 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:20:1D.0 Off |                    0 |\n",
      "| N/A   38C    P0              57W / 400W |      7MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8547ab69-367e-4ce7-95b3-adc21138b56c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638a9342f8ee41098609194d7eda0173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/31980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbde8b41a8c7461dbd52e36086b9f293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Trainer\n",
    "\n",
    "instruction_template = \"### User:\"\n",
    "response_template = \"### Assistant:\"\n",
    "collator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template, response_template=response_template, tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "                        max_seq_length=max_length,\n",
    "                        per_device_train_batch_size=5,\n",
    "                        per_device_eval_batch_size=5,\n",
    "                        gradient_accumulation_steps=2,\n",
    "                        learning_rate=0.0001,\n",
    "                        weight_decay=0.01, # L2 regularization\n",
    "                        lr_scheduler_type=\"cosine\",\n",
    "                        warmup_ratio=0.1, # warm up for first 10% of steps\n",
    "                        num_train_epochs=3,\n",
    "                        logging_dir=\"./logs\",\n",
    "                        report_to=[\"tensorboard\"],\n",
    "                        logging_steps=500, \n",
    "                        eval_strategy=\"steps\",\n",
    "                        eval_steps=500,  # Evaluate steps\n",
    "                        save_strategy=\"steps\",\n",
    "                        save_steps=5000, # save checkpoint\n",
    "                        output_dir = \"./results\",\n",
    "                        dataset_text_field=\"texts\",\n",
    "                        #fp16=True,  # Enable mixed precision training\n",
    "                        max_grad_norm=1.0,\n",
    "                        gradient_checkpointing=True,  # Enable gradient checkpointing\n",
    "                        optim=\"adamw_torch_fused\", \n",
    "                        load_best_model_at_end=True,\n",
    "                        metric_for_best_model=\"eval_loss\"\n",
    "                      )\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "                        model,\n",
    "                        train_dataset=hf_dataset['train'],\n",
    "                        eval_dataset=hf_dataset['test'],\n",
    "                        args=sft_config,\n",
    "                        data_collator=collator,\n",
    "                        tokenizer=tokenizer\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "209f672f-95f0-4f8b-af62-e144173045c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/user/miniconda/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6862' max='9594' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6862/9594 3:42:51 < 1:28:45, 0.51 it/s, Epoch 2.15/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.137800</td>\n",
       "      <td>2.364868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.502900</td>\n",
       "      <td>2.746630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.719200</td>\n",
       "      <td>2.816531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.740700</td>\n",
       "      <td>2.780465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.687900</td>\n",
       "      <td>2.758202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.669100</td>\n",
       "      <td>2.735217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.310100</td>\n",
       "      <td>2.736810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.072900</td>\n",
       "      <td>2.696918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.062000</td>\n",
       "      <td>2.663558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.043100</td>\n",
       "      <td>2.602335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.991400</td>\n",
       "      <td>2.566806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.972700</td>\n",
       "      <td>2.524784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.797900</td>\n",
       "      <td>2.701897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "/home/user/miniconda/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85e079ac-697e-4048-ac3f-db10745d514d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 00:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test results: {'eval_loss': 2.602334976196289, 'eval_runtime': 49.8678, 'eval_samples_per_second': 20.053, 'eval_steps_per_second': 4.011, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set after training\n",
    "test_results = trainer.evaluate()\n",
    "print(\"Final test results:\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9888948-a43d-4b6f-aa3d-a30c0e7c5d0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "trainer.save_model(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "77d3d4e6-f15b-4e4e-9f72-cd0d21643818",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Push to HF Hub / Repo\n",
    "\n",
    "# Load the SFT model and tokenizer from disk\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# Set the organization and repository name\n",
    "org_name = \"Distillation-Hugs\"  # Replace with your organization name\n",
    "repo_name = \"kd_exps\"  # Replace with your desired model name\n",
    "\n",
    "# Create a new repository\n",
    "#repo_url = create_repo(repo_id=\"Distillation-Hugs/kd_exps\", repo_type=\"model\", private=False)\n",
    "\n",
    "# Clone the empty repository\n",
    "#repo = Repository(local_dir=\"./hf_model_repo\", clone_from=repo_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3706e8e9-81dc-40bf-b6c4-a3df556bce7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b00ec41c0c43f3abe7f1b2562154a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cdd26a0db654224bfe7b9ef1ad28719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Distillation-Hugs/kd_exps/commit/932dd2ebadace38124e0b520a45c0eecf47f1adb', commit_message='Upload Qwen2ForCausalLM', commit_description='', oid='932dd2ebadace38124e0b520a45c0eecf47f1adb', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.push_to_hub(repo_id=f\"{org_name}/{repo_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f11b043-5e30-405e-a1d2-660dd1e22e6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Distillation-Hugs/kd_exps/commit/e179a88345ef2ff04dfe6303afb131ebddbc2d51', commit_message='Upload tokenizer', commit_description='', oid='e179a88345ef2ff04dfe6303afb131ebddbc2d51', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer.push_to_hub(repo_id=f\"{org_name}/{repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e11edb7-99f8-4f3b-afe6-5836c408f50d",
   "metadata": {},
   "source": [
    "### GKD \n",
    "\n",
    "- GKD trains the student on its self-generated sequences that are on-policy, instead of a fixed set of outputs sequences, using teacher probabilities as expert labels on these sequences.\n",
    "\n",
    "**Key distinctions between On-Policy KD vs GDK**\n",
    "\n",
    "> Data source:\n",
    "\n",
    "- On-policy KD uses only student-generated data.\n",
    "- GKD uses a mixture of fixed dataset and student-generated data.\n",
    "\n",
    "> Flexibility:\n",
    "\n",
    "- On-policy KD is more focused on addressing the train-inference mismatch. \n",
    "    - During training, the model always sees \"correct\" or \"expert-generated\" contexts. During inference, the model sees its own generated context, which may contain errors or be less optimal than the training contexts. As generation progresses, these small differences can compound, leading to increasingly divergent contexts.        \n",
    "- GKD is a more general framework that can encompass both supervised and on-policy approaches.\n",
    "\n",
    "> Hyperparameters:\n",
    "\n",
    "- GKD introduces λ to control the balance between fixed and student-generated data.\n",
    "- GKD allows for experimentation with different divergence measures.\n",
    "\n",
    "> Generalizability:\n",
    "\n",
    "- On-policy KD can be seen as a specific instance of GKD (when λ = 1 and using forward KL divergence).\n",
    "- GKD can represent a spectrum of approaches, from fully supervised (λ = 0) to fully on-policy (λ = 1).\n",
    "\n",
    "**Problem setup:**\n",
    "\n",
    "- We are given two auto-regressive sequence models of different capacity, where `pS` and `pT` refers to the student and teacher respectively. \n",
    "\n",
    "- We assume that the student has learnable parameters `θ` and `pSθ` (student model) is differentiable w.r.t `θ`. This is important because the slope (gradient) changes at different points along the curve. \n",
    "\n",
    "- We have generated a dataset by sampling sequences from the teacher. \n",
    "\n",
    "- Divergence `D` is defined as the discrepancy between token-level distributions of `pT` and `pS`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69da1e5e-37b5-4e94-804d-d8088452e28a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/lib/python39.zip\n",
      "/home/user/miniconda/lib/python3.9\n",
      "/home/user/miniconda/lib/python3.9/lib-dynload\n",
      "\n",
      "/home/user/miniconda/lib/python3.9/site-packages\n",
      "/trl_local/trl/trainer/\n",
      "/trl/trainer/\n",
      "/data/kd_exps\n",
      "/data/kd_exps\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Get the directory of the current script\n",
    "#current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# Add the directory containing your .py file to sys.path\n",
    "#sys.path.append(os.path.join(os.getcwd(), '/trl/trainer/'))\n",
    "\n",
    "for path in sys.path:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca76525c-908c-4693-a93e-cbbf7cfae5f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/kd_exps/trl/on_policy'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d189be54-129f-4f40-bdae-867739572368",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/lib/python39.zip\n",
      "/home/user/miniconda/lib/python3.9\n",
      "/home/user/miniconda/lib/python3.9/lib-dynload\n",
      "\n",
      "/home/user/miniconda/lib/python3.9/site-packages\n",
      "/trl_local/trl/trainer/\n",
      "/trl/trainer/\n",
      "/data/kd_exps\n",
      "/data/kd_exps\n",
      "/data/kd_exps\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "for path in sys.path:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb4511e2-5bf8-4b85-ab1e-77eb4c0cbe36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'trl.trainer.gkd_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgkd_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GKDConfig\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgkd_trainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GKDTrainer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelConfig\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'trl.trainer.gkd_config'"
     ]
    }
   ],
   "source": [
    "from trl.trainer.gkd_config import GKDConfig\n",
    "from trl.trainer.gkd_trainer import GKDTrainer\n",
    "from trl.trainer.model_config import ModelConfig\n",
    "from trl.trainer.callbacks import RichProgressCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85ba7ac2-bed4-47a8-9bcb-5bed44536d54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'GKDConfig' from 'trl.trainer' (/home/user/miniconda/lib/python3.9/site-packages/trl/trainer/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     GKDConfig,\n\u001b[1;32m      3\u001b[0m     GKDTrainer,\n\u001b[1;32m      4\u001b[0m     ModelConfig,\n\u001b[1;32m      5\u001b[0m     RichProgressCallback\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'GKDConfig' from 'trl.trainer' (/home/user/miniconda/lib/python3.9/site-packages/trl/trainer/__init__.py)"
     ]
    }
   ],
   "source": [
    "from trl.trl.trainer import (\n",
    "    GKDConfig,\n",
    "    GKDTrainer,\n",
    "    ModelConfig,\n",
    "    RichProgressCallback\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
