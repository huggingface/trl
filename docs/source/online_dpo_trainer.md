# Online DPO Trainer

TRL supports training LLMs with online DPO ([Guo et al., 2024](https://huggingface.co/papers/2402.04792)) with a reward model (RM). The idea of online DPO is to generate completions based on prompts and either have an RM or a LLM judge to rank the responses. Then the model is updated with the ranked responses using the DPO loss.

While [Guo et al. (2024)](https://huggingface.co/papers/2402.04792) used a LLM judge, in this implementation we just used a RM.


## Get started

The basic API looks as follows:

```python
from datasets import Dataset
from trl import OnlineDPOConfig, OnlineDPOTrainer
from transformers import (
    AutoModelForCausalLM,
    AutoModelForSequenceClassification,
    AutoTokenizer,
)
NUM_DUMMY_SAMPLES = 100
tokenizer = AutoTokenizer.from_pretrained("HuggingFaceTB/SmolLM-135M-Instruct")
tok.add_special_tokens({"pad_token": "[PAD]"})
# The model to optimise
model = AutoModelForCausalLM.from_pretrained("HuggingFaceTB/SmolLM-135M-Instruct")
ref_model = AutoModelForCausalLM.from_pretrained("HuggingFaceTB/SmolLM-135M-Instruct")
# The model to score completions with. In practice, you will need a fine-tuned reward model. See Reward Bench for some good ones: https://huggingface.co/spaces/allenai/reward-bench
reward_model = AutoModelForSequenceClassification.from_pretrained("HuggingFaceTB/SmolLM-135M-Instruct", num_labels=1)
train_dataset = Dataset.from_dict(
    {"input_ids": [tok.encode("Q: Hi how are you? A:")] * NUM_DUMMY_SAMPLES})
eval_dataset = Dataset.from_dict(
    {"input_ids": [tok.encode("Q: What do you like to eat A:")] * NUM_DUMMY_SAMPLES})
trainer = OnlineDPOTrainer(
    OnlineDPOConfig(
        output_dir="online-dpo-model",
    ),
    model=model,
    ref_model=ref_model,
    reward_model=reward_model,
    tokenizer=tok,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)
trainer.train()
```


To just run the online DPO script to make sure the trainer can run, you can run the following command to train an online DPO model with a dummy reward model.

```bash
python examples/scripts/online_dpo.py \
    --dataset_name trl-internal-testing/tldr-preference-sft-trl-style \
    --learning_rate 3e-6 \
    --output_dir models/minimal/online_dpo \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 64 \
    --total_episodes 30000 \
    --model_name_or_path EleutherAI/pythia-14m \
    --sft_model_path EleutherAI/pythia-14m \
    --reward_model_path EleutherAI/pythia-14m \
    --non_eos_penalty \
    --stop_token eos \
    --response_length 53 \
    --sanity_check
```


## Explanation of the logged metrics

The logged metrics are as follows. Here is an example [tracked run at Weights and Biases](https://wandb.ai/huggingface/trl/runs/dd2o3g35)

* `eps`: Tracks the number of episodes per second.
* `objective/kl`: The mean Kullback-Leibler (KL) divergence between the current model and reference model.
* `objective/entropy`: The mean entropy of the model, indicating the randomness of the actions chosen by the model.
* `objective/non_score_reward`: The mean reward from non-score-related sources, basically `beta * kl.sum(1)`, where `beta` is the KL penalty coefficient and `kl` is the per-token KL divergence.
* `objective/rlhf_reward`: The mean RLHF reward, which is `score - non_score_reward`.
* `objective/scores`: The mean scores returned by the reward model / environment.
* `objective/scores_margin`: The mean score margin (according to the external reward model) between the chosen and rejected completions.
* `rewards/accuracies`: The accuracies of the online DPO's implicit reward model.
* `rewards/chosen`: The mean reward (according to online DPO's implicit reward model)of the chosen completions.
* `rewards/rejected`: The mean reward (according to online DPO's implicit reward model) of the rejected completions.
* `rewards/margins`: The mean reward margin (according to online DPO's implicit reward model) between the chosen and rejected completions.
* `logps/chosen`: The mean log probabilities of the chosen completions.
* `logps/rejected`: The mean log probabilities of the rejected completions.
* `val/num_eos_tokens`: The number of end-of-sequence (EOS) tokens generated, which can indicate the number of complete responses.
* `lr`: lr: The current learning rate used by the optimizer.
* `episode`: episode: The current global step or episode count in the training process.


## Cookbook

* Debugging TIP: `objective/rlhf_reward`: this is the ultimate objective of the RLHF training. If training works as intended, this metric should keep going up.
* Memory TIP: If you are running out of memory, you can try to reduce the `--per_device_train_batch_size` or increase the `--gradient_accumulation_steps` to reduce the memory footprint.
* Memory TIP: If you have multiple GPUs, you can also run training with DeepSpeed stage 3 to reduce the memory footprint `accelerate launch --config_file examples/accelerate_configs/deepspeed_zero3.yaml`.
* Usage TIP: We recommend to use the "EOS trick" via `--non_eos_penalty --stop_token eos`, which replaces the score of completions that do not end with an EOS token with a static scalar penalty `--penalty_reward_value`. This can help the model learn to generate more coherent completions.


## What is my model doing exactly?

To help you understand what your model is doing, we periodically log some sample completions from the model. Here is an example of a completion. In an example [tracked run at Weights and Biases](https://wandb.ai/huggingface/trl/runs/dd2o3g35), it looks like the following, allowing you to see the model's response at different stages of training. By default we generate `--num_sample_generations 10` during training, but you can customize the number of generations.

![](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/ppov2_completions.gif)


In the logs the sampled generations look like 

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ query                           â”ƒ model response                  â”ƒ score    â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”©
â”‚  SUBREDDIT: r/AskReddit         â”‚  I'm in love with a friend, and â”‚ 3.921875 â”‚
â”‚                                 â”‚ I don't know how to get rid of  â”‚          â”‚
â”‚ TITLE: How do you get someone   â”‚ those feelings. I'm             â”‚          â”‚
â”‚ out of your head?               â”‚ desperate.<|endoftext|>[PAD][Pâ€¦ â”‚          â”‚
â”‚                                 â”‚                                 â”‚          â”‚
â”‚ POST: Hi,                       â”‚                                 â”‚          â”‚
â”‚ I'm 22, and I have been with my â”‚                                 â”‚          â”‚
â”‚ girlfriend for 5 years now. We  â”‚                                 â”‚          â”‚
â”‚ recently moved together. We've  â”‚                                 â”‚          â”‚
â”‚ always loved each other         â”‚                                 â”‚          â”‚
â”‚ intensely.                      â”‚                                 â”‚          â”‚
â”‚                                 â”‚                                 â”‚          â”‚
â”‚ Problem, I recently started to  â”‚                                 â”‚          â”‚
â”‚ have feelings for an other      â”‚                                 â”‚          â”‚
â”‚ person (a friend). This person  â”‚                                 â”‚          â”‚
â”‚ has had a boyfriend for now 3   â”‚                                 â”‚          â”‚
â”‚ years, and has absolutely no    â”‚                                 â”‚          â”‚
â”‚ ideas. Those feelings were so   â”‚                                 â”‚          â”‚
â”‚ strong, it was hard to hide     â”‚                                 â”‚          â”‚
â”‚ them. After 2 months of me      â”‚                                 â”‚          â”‚
â”‚ being distant and really sad,   â”‚                                 â”‚          â”‚
â”‚ my girlfriend forced me to say  â”‚                                 â”‚          â”‚
â”‚ what was bothering me. I'm not  â”‚                                 â”‚          â”‚
â”‚ a good liar, and now she knows. â”‚                                 â”‚          â”‚
â”‚                                 â”‚                                 â”‚          â”‚
â”‚ We decided to give us a week    â”‚                                 â”‚          â”‚
â”‚ alone, I went to my parents.    â”‚                                 â”‚          â”‚
â”‚                                 â”‚                                 â”‚          â”‚
â”‚ Now, I'm completely lost. I     â”‚                                 â”‚          â”‚
â”‚ keep on thinking about this     â”‚                                 â”‚          â”‚
â”‚ person, and I hate that. I      â”‚                                 â”‚          â”‚
â”‚ would like for those feelings   â”‚                                 â”‚          â”‚
â”‚ to go away, to leave me alone.  â”‚                                 â”‚          â”‚
â”‚ But I can't.                    â”‚                                 â”‚          â”‚
â”‚                                 â”‚                                 â”‚          â”‚
â”‚ What do I do? It's been 3       â”‚                                 â”‚          â”‚
â”‚ months now, and I'm just        â”‚                                 â”‚          â”‚
â”‚ desperate.                      â”‚                                 â”‚          â”‚
â”‚                                 â”‚                                 â”‚          â”‚
â”‚ TL;DR:                          â”‚                                 â”‚          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  SUBREDDIT: r/pettyrevenge      â”‚  My mom woke me up with a loud  â”‚ 6.84375  â”‚
â”‚                                 â”‚ TV. I blasted Gangnam Style on  â”‚          â”‚
â”‚ TITLE: So, my mom woke me up    â”‚ repeat, with the bass cranked   â”‚          â”‚
â”‚ with a loud TV.                 â”‚ up as high as it could          â”‚          â”‚
â”‚                                 â”‚ go.<|endoftext|>[PAD][PAD][PADâ€¦ â”‚          â”‚
â”‚ POST: She was in her living     â”‚                                 â”‚          â”‚
â”‚ room, watching TV. This was at  â”‚                                 â”‚          â”‚
â”‚ about 8:30 in the morning, and  â”‚                                 â”‚          â”‚
â”‚ she was exercising. She turned  â”‚                                 â”‚          â”‚
â”‚ the TV up extra loud to hear it â”‚                                 â”‚          â”‚
â”‚ over her excercycle, and woke   â”‚                                 â”‚          â”‚
â”‚ me up. I went in there asking   â”‚                                 â”‚          â”‚
â”‚ for her to turn it down. She    â”‚                                 â”‚          â”‚
â”‚ said she didn't have to; I      â”‚                                 â”‚          â”‚
â”‚ explained that I always used    â”‚                                 â”‚          â”‚
â”‚ headphones so she didn't have   â”‚                                 â”‚          â”‚
â”‚ to deal with my noise and that  â”‚                                 â”‚          â”‚
â”‚ she should give me a little     â”‚                                 â”‚          â”‚
â”‚ more respect, given that I paid â”‚                                 â”‚          â”‚
â”‚ rent at the time.               â”‚                                 â”‚          â”‚
â”‚                                 â”‚                                 â”‚          â”‚
â”‚ She disagreed. I went back to   â”‚                                 â”‚          â”‚
â”‚ my room, rather pissed off at   â”‚                                 â”‚          â”‚
â”‚ the lack of equality. I had no  â”‚                                 â”‚          â”‚
â”‚ lock on my door; but I had a    â”‚                                 â”‚          â”‚
â”‚ dresser right next to it, so I  â”‚                                 â”‚          â”‚
â”‚ pulled one of the drawers out   â”‚                                 â”‚          â”‚
â”‚ enough so that it caused the    â”‚                                 â”‚          â”‚
â”‚ door to not be openable. Then,  â”‚                                 â”‚          â”‚
â”‚ I turned my speakers up really  â”‚                                 â”‚          â”‚
â”‚ loud and blasted Gangnam Style  â”‚                                 â”‚          â”‚
â”‚ on repeat, with the bass        â”‚                                 â”‚          â”‚
â”‚ cranked up as high as it could  â”‚                                 â”‚          â”‚
â”‚ go.                             â”‚                                 â”‚          â”‚
â”‚                                 â”‚                                 â”‚          â”‚
â”‚ If you hate Gangnam Style for   â”‚                                 â”‚          â”‚
â”‚ being overplayed, you will see  â”‚                                 â”‚          â”‚
â”‚ why I chose that particular     â”‚                                 â”‚          â”‚
â”‚ song. I personally don't mind   â”‚                                 â”‚          â”‚
â”‚ it. But here's the thing about  â”‚                                 â”‚          â”‚
â”‚ my bass; it vibrates the walls, â”‚                                 â”‚          â”‚
â”‚ making one hell of a lot of     â”‚                                 â”‚          â”‚
â”‚ noise. Needless to say, my mom  â”‚                                 â”‚          â”‚
â”‚ was not pleased and shut off    â”‚                                 â”‚          â”‚
â”‚ the internet. But it was oh so  â”‚                                 â”‚          â”‚
â”‚ worth it.                       â”‚                                 â”‚          â”‚
â”‚                                 â”‚                                 â”‚          â”‚
â”‚ TL;DR:                          â”‚                                 â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Implementation details

Many online implementation details are borrowed from the PPOv2Trainer, which is itself based on the [The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization](https://huggingface.co/papers/2403.17031). Here are some additional implementation details:

1. When we turn on the EOS trick (i.e., replacing the score of completions that do not end with an EOS token with a scalar penalty score like `-1`) via `--non_eos_penalty --stop_token eos`, it's possible that the chosen and rejected completions have the same score. In this case, we will naively select the completion with the lower index and the chosen completion.

## Benchmark experiments

To validate the online DPO implementation works, we ran experiments on the 1B and 6.9B models. Here are the commands we used to run the experiments. We take the SFT / RM models directly from [The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization](https://huggingface.co/papers/2403.17031).


```
# 1B Online DPO experiment
accelerate launch --config_file examples/accelerate_configs/deepspeed_zero2.yaml \
    examples/scripts/online_dpo.py \
    --dataset_name trl-internal-testing/tldr-preference-sft-trl-style \
    --learning_rate 3e-6 \
    --output_dir models/minimal/online_dpo_tldr \
    --per_device_train_batch_size 16 \
    --gradient_accumulation_steps 4 \
    --local_rollout_forward_batch_size 32 \
    --num_epochs 1 \
    --num_mini_batches 1 \
    --total_episodes 1000000 \
    --model_name_or_path cleanrl/EleutherAI_pythia-1b-deduped__sft__tldr  \
    --sft_model_path cleanrl/EleutherAI_pythia-1b-deduped__sft__tldr \
    --reward_model_path cleanrl/EleutherAI_pythia-1b-deduped__reward__tldr \
    --save_strategy no \
    --non_eos_penalty \
    --stop_token eos \
    --beta 0.1 \
    --response_length 53 \
    --push_to_hub

# 6.9B Online DPO experiment
accelerate launch --config_file examples/accelerate_configs/deepspeed_zero3.yaml \
    examples/scripts/online_dpo.py \
    --dataset_name trl-internal-testing/tldr-preference-sft-trl-style \
    --learning_rate 3e-6 \
    --output_dir models/minimal/online_dpo_tldr_6.9b \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 16 \
    --local_rollout_forward_batch_size 8 \
    --num_epochs 1 \
    --num_mini_batches 1 \
    --total_episodes 1000000 \
    --model_name_or_path EleutherAI/pythia-6.9b-deduped \
    --sft_model_path cleanrl/EleutherAI_pythia-6.9b-deduped__sft__tldr \
    --reward_model_path cleanrl/EleutherAI_pythia-6.9b-deduped__reward__tldr \
    --save_strategy no \
    --non_eos_penalty \
    --stop_token eos \
    --beta 0.1 \
    --response_length 53 \
    --push_to_hub
```

Checkpoints and experiment tracking are available at:

- [ğŸ¤— Model checkpoint](https://huggingface.co/vwxyzjn/ppo_tldr)
- [ğŸ Tracked experiment](https://wandb.ai/huggingface/trl/runs/dd2o3g35)


To evaluate, we use [vLLM](https://github.com/vllm-project/vllm) to load the checkpoints and GPT-4o mini as a judge model to evaluate the generated TL;DR against the reference TL;DR.
For more information on how to use judges, see [Judges](judges).

```bash
$ python examples/scripts/evals/judge_tldr.py --model_name_or_path cleanrl/EleutherAI_pythia-1b-deduped__sft__tldr --judge_model gpt-4o-mini --num_examples 1000
Model win rate: 33.00%
python examples/scripts/evals/judge_tldr.py --model_name_or_path cleanrl/EleutherAI_pythia-6.9b-deduped__sft__tldr --judge_model gpt-4o-mini --num_examples 1000
Model win rate: 41.50%
python examples/scripts/evals/judge_tldr.py --model_name_or_path vwxyzjn/online_dpo_tldr --judge_model gpt-4o-mini --num_examples 1000
Model win rate: 62.60%
python examples/scripts/evals/judge_tldr.py --model_name_or_path vwxyzjn/online_dpo_tldr_6.9b --judge_model gpt-4o-mini --num_examples 1000
Model win rate: 74.20%
```

We can then plot the RLHF scaling chart.

```python
import matplotlib.pyplot as plt

data = {
    "SFT": [[1e9, 6.9e9], [0.33, 0.415]],
    "Online DPO": [[1e9, 6.9e9], [0.626, 0.742]],
}
for model, (x, y) in data.items():
    plt.scatter(x, y, label=model)

plt.axhline(y=0.5, color="black", linestyle="-.", label="Human reference summary")
plt.title("RLHF scaling by model size")
plt.xlabel("Model size")
plt.ylabel("Win rate against reference summaries\n(according to GPT-4o mini)")
plt.xscale("log")
plt.xlim(5e8, 1.2e10)
plt.xticks([1e9, 1e10], ["1B", "10B"])
plt.legend()
plt.grid(True, which="both", ls="--", c="0.7")
plt.tight_layout()
plt.savefig("plot.png")
```


![](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/online_dpo_scaling.png)

The online DPO checkpoint gets increasingly more win rate as we scale up the model sizes. This is a good sign that the online DPO implementation is working as intended.
