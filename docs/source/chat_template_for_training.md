# Modifying Chat Templates for Assistant-Only Training

When fine-tuning language models with [`SFTTrainer`], you often want to compute the loss **only on tokens generated by the assistant**, while ignoring user, system, and tool response messages. This guide explains how to modify a model's chat template to support this functionality using the `{% generation %}` and `{% endgeneration %}` Jinja tags.

## How It Works

The `assistant_only_loss=True` option in [`SFTConfig`] enables training on assistant responses only. However, this feature requires the model's chat template to explicitly mark which tokens belong to the assistant's output using special Jinja template tags.

When `assistant_only_loss=True` is set, TRL calls `tokenizer.apply_chat_template()` with `return_assistant_tokens_mask=True`. The tokenizer uses `{% generation %}` tags in the Jinja template to identify which tokens belong to the assistant's response.

```
User message       → NOT trained on (masked with -100)
System message     → NOT trained on (masked with -100)
Assistant message  → TRAINED on (included in loss)
```

> [!WARNING]
> Most model chat templates do not include these tags by default. If your model's template lacks these tags, `assistant_only_loss=True` will fail and there will be no mask.

## Modifying a Chat Template

### Step 1: Locate the Chat Template

Chat templates are stored in the tokenizer. You can read and write them as follows:

```python
# Export template to a file
open("template.jinja", "w").write(tokenizer.chat_template)
```

### Step 2: Add `{% generation %}` Tags

Wrap the assistant's response content with `{% generation %}` and `{% endgeneration %}` tags.

#### Before (Original Template)

```jinja
{%- if message.role == "assistant" %}
    {{- '<|im_start|>assistant\n' + message.content + '<|im_end|>\n' }}
{%- endif %}
```

#### After (Modified Template)

```jinja
{%- if message.role == "assistant" %}
    {{- '<|im_start|>assistant\n' }}
    {% generation %}
    {{- message.content + '<|im_end|>' }}
    {% endgeneration %}
    {{- '\n' }}
{%- endif %}
```
>[!TIP]
>If your model supports tool calling, ensure the `{% generation %}` tags also wrap tool calls since these are generated by the assistant, but **not** tool responses:

### Step 3: Apply the Modified Template

There are several ways to use your modified template:

```python
# Option 1: Pass the template path to SFTConfig
config = SFTConfig(
    chat_template_path="path/to/your/template.jinja",
    assistant_only_loss=True,
)

# Option 2: Modify the tokenizer directly
tokenizer.chat_template = open("template.jinja").read()
```

## Troubleshooting

### Known Incompatibilities

- `assistant_only_loss=True` with VLMs: Not currently supported
