# Credits

This work is heavily influenced by the repo [here](https://github.com/kvablack/ddpo-pytorch) and the associated [paper](https://arxiv.org/abs/2305.13301).

# Getting started with Stable Diffusion finetuning with reinforcement learning


The machinery for finetuning of Stable Diffusion models with reinforcement learning makes heavy use of HuggingFace's `diffusers`
library. A reason for  stating this is that getting started requires a bit of familiarity with the `diffusers` library concepts, mainly two of them - pipelines and schedulers.
Right out of the box (`diffusers` library), there isn't a `Pipeline` nor a `Scheduler` instance that is suitable for finetuning with reinforcement learning. Some adjustments need to made. 

There is an interface that is provided by this library that is required to be implemented to be used with the `DDPOTrainer`, which is the main machinery for fine-tuning Stable Diffusion with reinforcement learning. 
There is a default implementation of this interface that you can use out of the box. Assuming the default implementation is sufficient and/or to get things moving, refer to the training example alongside this guide. 

While there is an explicit class interface for the pipeline, the scheduler is constrained by a method signature associated with the pipeline class. The trainer will attempt to get the scheduler
via this pipeline associated method rather than directly. This was the most minimal way forward and the only constraint is that a particular required output type.

For a more detailed look into the interface and the default scheduler and pipeline implementation, go [here](https://github.com/lvwerra/trl/tree/main/trl/models/modelling_sd_base.py)

Also in addition, there is the expectation of providing a reward function and a prompt function. The reward function is used to evaluate the generated images  and the prompt function is used to generate the prompts that are used to generate the images.

# Getting started with `examples/scripts/stable_diffusion_tuning.py`

The `stable_diffusion_tuning.py` script is a working example of finetuning a Stable Diffusion model with reinforcement learning. The script should be a good starting point for getting started.
Almost every configuration parameter has a default and only one thing is required of the user. Have the model weights similar or exactly that of the one found here https://github.com/christophschuhmann/improved-aesthetic-predictor uploaded
to the HuggingFace model hub. The hub id is then to be used as the `hf_hub_aesthetic_model_id` commandline parameter. 

```batch
python stable_diffusion_tuning.py --hf_hub_aesthetic_model_id myusername/ddpo-aesthetic-scorer
```

The script uses a small subset of parameters to configure the trainer. And all of these are configurable via the commandline.
It should be noted (in general) that because the trainer uses `accelerate` as a core component, some parameters are those of accelerate's.
The commandline flags that are associated with the example's scripts parameters are listed below.

|----|----|-----|
|parameter|description| default |
|----|----|-----|
|`--hf_hub_aesthetic_model_id`|The HuggingFace model hub id of the aesthetic scorer model|None|
|`--pretrained_model`|The string id of the pretrained Stable Diffusion model|`"runwayml/stable-diffusion-v1-5"`|
|`--pretrained_revision`|The revision of the pretrained Stable Diffusion model|`"main"`|
|`--num_epochs`|The number of epochs to train for|`200`|
|`--train_batch_size`|The batch size to use for training|`8`|
|`--sample_batch_size`|The batch size to use for sampling|`8`|
|`--gradient_accumulation_steps`|The number of accelerator based gradient accumulation steps to use|`1`|
|`--sample_num_steps`| The number of steps to sample for|`50`|
|`--sample_num_batches_per_epoch`|The number of batches to sample per epoch|`4`|
|`--log_with`|The logger to use. Either `wandb` or `tensorboard`|`wandb`|
|`--per_prompt_stat_tracking`|Whether to track stats per prompt. If false, advantages will be calculated using the mean and std of the entire batch as opposed to tracking per prompt|`True`|
|`--per_prompt_stat_tracking_buffer_size`|The size of the buffer to use for tracking stats per prompt|`32`|
|`--tracker_project_name`|The name of the project for use on the tracking platform (wandb/tensorboard/etc) |`"stable_diffusion_training"`|
| `--logging_dir`|The directory to use for logging|`"logs"`|
| `--project_dir`|The directory to use for saving the model|`"save"`|
| `--automatic_checkpoint_naming`|Whether to automatically name model checkpoints|`True`|
| `--total_limit`| Number of checkpoints to keep before overwriting old ones|`5`|
|----|----|-----|

The following are things to keep in mind (The code checks this for you as well) in general while configuring the trainer (beyond the use case of using the example script)

- The configurable sample batch size should be greater than or equal to the configurable training batch size
- The configurable sample batch size must be divisible by the configurable train batch size
- The configurable sample batch size must be divisible by both the configurable gradient accumulation steps and the configurable accelerator processes count

# Setting up the image logging hook function

Expect the function to be given a list of lists of the form
```python
[[image, prompt, prompt_metadata, rewards], ...]

```
and `image`, `prompt`, `prompt_metadata`, `rewards` are batched.
The last list in the lists of lists represents the last sample batch. You are likely to want to log this one
While you are free to log however you want the use of `wandb` or `tensorboard` is recommended.

Example code for logging sampled images with `wandb` is given below.

```python
# for logging these images to wandb

def image_outputs_hook(image_data, global_step, accelerate_logger):
    # extract the last one
    result = {}
    images, prompts, _, rewards = image_data[-1]
    for i, image in enumerate(images):
        pil = Image.fromarray(
            (image.cpu().numpy().transpose(1, 2, 0) * 255).astype(np.uint8)
        )
        pil = pil.resize((256, 256))
        result[f"{prompts[i]:.25} | {rewards[i]:.2f}"] = [pil]
    accelerate_logger.log_images(
        result,
        step=global_step,
    )

```