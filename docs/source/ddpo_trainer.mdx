# Credits

This work is heavily influenced by the repo [here](https://github.com/kvablack/ddpo-pytorch) and the associated [paper](https://arxiv.org/abs/2305.13301).

# Getting started with Stable Diffusion finetuning with reinforcement learning

The machinery for finetuning of Stable Diffusion models with reinforcement learning makes heavy use of HuggingFace's `diffusers`
library. A reason for  stating this is that getting started requires a bit of familiarity with the `diffusers` library concepts, mainly two of them.
`Pipeline` and `Scheduler`. 
Right out of the box (`diffusers` library), there isn't a `Pipeline` nor a `Scheduler` instance that is suitable for finetuning with reinforcement learning. Some adjustments need to made. 

There is an interface that is provided by this library that is required to be implemented to be used with the `DDPOTrainer`, which is the main machinery for fine-tuning Stable Diffusion with reinforcement learning. 
There is a default implementation of this interface that you can use out of the box. Assuming the default implementation is sufficient and/or to get things moving, refer to the training example alongside this guide. 

While there is an explicit class interface for the pipeline, the scheduler is constrained by a method signature associated with the pipeline class. The trainer will attempt to get the scheduler
via this pipeline associated method rather than directly. This was the most minimal way forward and the only constraint is that a particular required output type.

For a more detailed look into the interface and the default scheduler and pipeline implementation, go [here](https://github.com/lvwerra/trl/models/modelling_sd_base.py)

Also in addition, there is the expectation of providing a reward function and a prompt function. The reward function is used to evaluate the generated images  and the prompt function is used to generate the prompts that are used to generate the images.

# Setting up the image logging hook function

Expect the function to be given a list of lists of the form
```python
[[image, prompt, prompt_metadata, rewards], ...]

```
and `image`, `prompt`, `prompt_metadata`, `rewards` are batched.
The last list in the lists of lists represents the last sample batch. You are likely to want to log this one
While you are free to log however you want the use of `wandb` or `tensorboard` is recommended.

Example code for logging sampled images with `wandb` is given below.

```python
# for logging these images to wandb

def image_outputs_hook(image_data, global_step, accelerate_logger):
    # extract the last one
    result = {}
    images, prompts, _, rewards = image_data[-1]
    for i, image in enumerate(images):
        pil = Image.fromarray(
            (image.cpu().numpy().transpose(1, 2, 0) * 255).astype(np.uint8)
        )
        pil = pil.resize((256, 256))
        result[f"{prompts[i]:.25} | {rewards[i]:.2f}"] = [pil]
    accelerate_logger.log_images(
        result,
        step=global_step,
    )

```

`DDPOConfig` is the config object type. Most of the configuration params have defaults. The following are things to keep in mind (The code checks this for you as well)

- The configurable sample batch size should be greater than or equal to self.config.train_batch_size
- The configurable sample batch size must be divisible by the configurable train batch size
- The configurable sample batch size must be divisible by both the configurable gradient accumulation steps and the configurable accelerator processes count

For a working example please refer to `stable_diffusion_tuning.py` in the `examples/scripts` directory.
