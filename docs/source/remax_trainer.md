# ReMax Trainer

[![](https://img.shields.io/badge/All_models-ReMax-blue)](https://huggingface.co/models?other=remax,trl)

## Overview

TRL introduces the ReMax Trainer, an advanced tool for training language models, inspired by the ICML 2024 paper [ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models](https://arxiv.org/abs/2310.10505). ReMax provides a structured approach to language model post-training using reinforcement learning. In comparison to PPO, ReMax is easier to implement, more memory-efficient, and faster, with fewer hyper-parameters to tune.

The abstract from the paper is the following:

> Reinforcement Learning from Human Feedback (RLHF) is key to aligning Large Language Models (LLMs), typically paired with the Proximal Policy Optimization (PPO) algorithm. While PPO is a powerful method designed for general reinforcement learning tasks, it is overly sophisticated for LLMs, leading to laborious hyper-parameter tuning and significant computation burdens. To make RLHF efficient, we present ReMax, which leverages 3 properties of RLHF: fast simulation, deterministic transitions, and trajectory-level rewards. These properties are not exploited in PPO, making it less suitable for RLHF. Building on the renowned REINFORCE algorithm, ReMax does not require training an additional value model as in PPO and is further enhanced with a new variance reduction technique. ReMax offers several benefits over PPO: it is simpler to implement, eliminates more than 4 hyper-parameters in PPO, reduces GPU memory usage, and shortens training time. ReMax can save about 46% GPU memory than PPO when training a 7B model and enables training on A800-80GB GPUs without the memory-saving offloading technique needed by PPO. Applying ReMax to a Mistral-7B model resulted in a 94.78% win rate on the AlpacaEval leaderboard and a 7.739 score on MT-bench, setting a new SOTA for open-source 7B models. These results show the effectiveness of ReMax while addressing the limitations of PPO in LLMs.

This post-training method was contributed by [Ziniu Li](liziniu1997@gmail.com).

## Quick start

This example demonstrates how to train a model using the ReMax method. We train a [Qwen 0.5B Instruct model](https://huggingface.co/Qwen/Qwen2-0.5B-Instruct) with the prompts from the [TLDR dataset](https://huggingface.co/datasets/trl-lib/tldr) (completion column is ignored!). You can view the data in the dataset here:

<iframe
  src="https://huggingface.co/datasets/trl-lib/tldr/embed/viewer/default/train?row=0"
  frameborder="0"
  width="100%"
  height="560px"
></iframe>

Below is the script to train the model.

```python
# train_remax.py
from datasets import load_dataset
from trl import ReMaxConfig, ReMaxTrainer

dataset = load_dataset("trl-lib/tldr", split="train")

# Define the reward function, which rewards completions that are close to 20 characters
def reward_len(completions, **kwargs):
    return [-abs(20 - len(completion)) for completion in completions]

training_args = ReMaxConfig(output_dir="Qwen2-0.5B-ReMax", logging_steps=10)
trainer = ReMaxTrainer(
    model="Qwen/Qwen2-0.5B-Instruct",
    reward_funcs=reward_len,
    args=training_args,
    train_dataset=dataset,
)
trainer.train()
```

Execute the script using the following command:

```bash
accelerate launch train_remax.py
```

Distributed across 8 GPUs, the training takes approximately 1 day.

![](https://huggingface.co/datasets/trl-lib/documentation-images/resolve/main/remax_curves.png)

## Looking deeper into the ReMax method

ReMax is an online reinforcement learning algorithm that iteratively improves by learning from data generated by the model during training. For each training iteration, ReMax follows three steps: First, it generates completions for a batch of prompts. Then, it computes a reward for each completion. Finally, it updates the model parameters to maximize these rewards. A key innovation of ReMax is its use of a deterministic greedy baseline for estimating advantages and gradients. This baseline provides a theoretically-grounded reference point for estimating the advantage and gradient.

To understand how ReMax works, it can be broken down into four main steps: **Generating completions**, **computing the advantage**, and **computing the loss**.

### Generating completions

ReMax generates completions for a batch of prompts, using the model to generate a set of completions for each prompt.

### Computing the advantage

For each sequence $(x^{i}, a^{i}_1, \ldots, a^{i}_T)$, we calculate the reward $r^{i}(x^{i}, a^{i}_1, \ldots, a^{i}_T)$ using a reward model. To align with the comparative nature of these models—often trained on datasets that compare different outputs for the same question—we compute the advantage to reflect these relative comparisons. The advantage is normalized as follows:

$$\hat{A}^{i} = r^{i}(x^{i}, a^{i}_1, \ldots, a^{i}_T) - b^{i}$$

where

$$
b^{i} = \text{reward}(x^{i}, \bar{a}^{i}_1, \bar{a}^{i}_2, \ldots, \bar{a}^{i}_T)
$$

Here, $\bar{a}_1, \bar{a}_2, \ldots, \bar{a}_T$ are the completions generated by the model through greedy decoding of the prompt $x$.

This methodology is what gives the approach its name: **ReMax**.


### Computing the loss

As described in the paper, the gradient of ReMax is influenced by a weighted likelihood ratio of the completions, with the weights being the previously mentioned advantage. Essentially, if the advantage is positive, the gradient aims to increase the likelihood of the completion; if negative, it aims to decrease the likelihood.

$$
\mathcal{L}_{\text{ReMax}}(\theta) = -\frac{1}{N} \sum_{i=1}^N \frac{1}{T} \sum_{t=1}^{T} \hat{A}^{i}  \log \pi_\theta(a^{i}_t \mid x^{i}, a^{i}_{< t})
$$

Additionally, techniques such as KL regularization, importance sampling, and clipping can be applied to ReMax for off-policy training.


## Logged metrics

The ReMax Trainer logs the following metrics:

- `completion_length`: The average completion length.
- `reward/{reward_func_name}`: The reward computed by each reward function.
- `reward`: The average reward.
- `reward_std` : The average standard deviation within reward groups.
- `greedy_reward`: The average reward of the greedy baseline.
- `greedy_reward_std`: The average standard deviation within greedy reward groups.
- `kl` : The average KL divergence between the model and the reference model calculated on completions.

## Customization

## Speed up training with vLLM-powered generation  

Generation is often the main bottleneck that makes training slow with online methods. To accelerate generation, you can use [vLLM](https://github.com/vllm-project/vllm), a library that enables fast generation. To enable it, pass `use_vllm=True` in the training arguments.  

```python
from trl import ReMaxConfig

training_args = ReMaxConfig(..., use_vllm=True)
```  

For more information, see [Speeding up training with vLLM](speeding_up_training#vllm-for-fast-generation-in-online-methods).  

### Using a custom reward function

The [`ReMaxTrainer`] supports using custom reward functions instead of dense reward models. To ensure compatibility, your reward function must satisfy the following requirements:

1. **Input arguments**:
   - The function must accept the following as keyword arguments:
     - `prompts` (contains the prompts),
     - `completions` (contains the generated completions),
     - All columns names (but `prompt`) that the dataset may have. For example, if the dataset contains a column named `ground_truth`, the function will be called with `ground_truth` as a keyword argument.

     The easiest way to comply with this requirement is to use `**kwargs` in the function signature.
   - Depending on the dataset format, the input will vary:
     - For [standard format](dataset_formats#standard), `prompts` and `completions` will be lists of strings.
     - For [conversational format](dataset_formats#conversational), `prompts` and `completions` will be lists of message dictionaries.

2. **Return value**: The function must return a list of floats. Each float represents the reward corresponding to a single completion.

#### Example 1: Reward longer completions

Below is an example of a reward function for a standard format that rewards longer completions:

```python
def reward_func(completions, **kwargs):
    """Reward function that gives higher scores to longer completions."""
    return [float(len(completion)) for completion in completions]
```

You can test it as follows:

```python
>>> prompts = ["The sky is", "The sun is"]
>>> completions = [" blue.", " in the sky."]
>>> print(reward_func(prompts=prompts, completions=completions))
[6.0, 12.0]
```

#### Example 2: Reward completions with specific format

Below is an example of a reward function that checks if the completion has a specific format. This example is inspired by the _format reward_ function used in the paper [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://huggingface.co/papers/2501.12948).
It is designed for conversational format, where prompts and completions consist of structured messages.

```python
import re

def format_reward_func(completions, **kwargs):
    """Reward function that checks if the completion has a specific format."""
    pattern = r"^<think>.*?</think><answer>.*?</answer>$"
    completion_contents = [completion[0]["content"] for completion in completions]
    matches = [re.match(pattern, content) for content in completion_contents]
    return [1.0 if match else 0.0 for match in matches]
```

You can test this function as follows:

```python
>>> prompts = [
...     [{"role": "assistant", "content": "What is the result of (1 + 2) * 4?"}],
...     [{"role": "assistant", "content": "What is the result of (3 + 1) * 2?"}],
... ]
>>> completions = [
...     [{"role": "assistant", "content": "<think>The sum of 1 and 2 is 3, which we multiply by 4 to get 12.</think><answer>(1 + 2) * 4 = 12</answer>"}],
...     [{"role": "assistant", "content": "The sum of 3 and 1 is 4, which we multiply by 2 to get 8. So (3 + 1) * 2 = 8."}],
... ]
>>> format_reward_func(prompts=prompts, completions=completions)
[1.0, 0.0]
```

#### Example 3: Reward completions based on a reference

Below is an example of a reward function that checks if the completion is correct. This example is inspired by the _accuracy reward_ function used in the paper [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://huggingface.co/papers/2501.12948).
This example is designed for [standard format](dataset_formats#standard), where the dataset contains a column named `ground_truth`.

```python
from math_verify import parse, verify

def reward_func(completions, ground_truth, **kwargs):
    completion_answers = [parse(completion[0]["content"]) for completion in completions]
    ground_truth_answers = [parse(truth) for truth in ground_truth]
    return [1.0 if verify(answer, ground_truth) else 0.0 for answer, ground_truth in zip(completion_answers, ground_truth_answers)]
```

You can test this function as follows:

```python
>>> prompts = ["Problem: Solve the equation $2x + 3 = 7$. Solution:", "Problem: Solve the equation $3x - 5 = 10$."]
>>> completions = [r" The solution is \boxed{2}.", r" The solution is \boxed{6}."]
>>> ground_truth = ["2", "5"]
>>> reward_func(prompts=prompts, completions=completions, ground_truth=ground_truth)
[1.0, 0.0]
```

#### Passing the reward function to the trainer

To use your custom reward function, pass it to the [`ReMaxTrainer`] as follows:

```python
from trl import ReMaxTrainer

trainer = ReMaxTrainer(
    reward_funcs=reward_func,
    ...,
)
```

If you have multiple reward functions, you can pass them as a list:

```python
from trl import ReMaxTrainer

trainer = ReMaxTrainer(
    reward_funcs=[reward_func1, reward_func2],
    ...,
)
```
and the reward will be computed as the sum of the rewards from each function, or the weighted sum if `reward_weights` is provided in the config.

Note that [`ReMaxTrainer`] supports multiple reward functions of different types. See the parameters documentation for more details.

## ReMaxTrainer

[[autodoc]] ReMaxTrainer

## ReMaxConfig

[[autodoc]] ReMaxConfig
