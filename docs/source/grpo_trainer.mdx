# GRPO Trainer

[Group Relative Policy Optimization](https://arxiv.org/pdf/2402.03300) (GRPO) is a variant of Proximal Policy Optimization (PPO) introduced for reinforcement learning in language models. GRPO eliminates the need for a separate value function model by using group-based reward estimation. For each question, it samples multiple outputs from the policy model and calculates advantages relative to the group's average reward. This approach reduces computational resources compared to PPO while still allowing for effective policy updates. GRPO incorporates a KL-divergence term to regularize the policy updates relative to a reference model. The gradient updates in GRPO depend on both the relative advantages within each group and the KL-divergence from the reference model. This method aims to improve mathematical reasoning capabilities in language models more efficiently than standard PPO.
