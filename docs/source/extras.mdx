# Extras: Alternative ways to get better model output besides RL based fine-tuning 

Within the extras module is the `best-of-n` sampler class that serves as an alternative method of generating better model output.
As to how it fares against the RL based fine-tuning, please look in the `examples` directory for a comparison example

## Usage

To get started, instantiate an instance of the class with a model, a length sampler, a tokenizer and a callable serves as a generalized version of a reward pipeline.

```python

from transformers import pipeline, AutoTokenizer
from trl import AutoModelForCausalLMWithValueHead
from trl.core import LengthSampler
from trl.extras import BestOfNSampler

ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(ref_model_name)
reward_pipe = pipeline("sentiment-analysis", model=reward_model, device=device)
tokenizer = AutoTokenizer.from_pretrained(ref_model_name)
tokenizer.pad_token = tokenizer.eos_token


# callable that takes a list of raw text and returns a goal specific best text
def reward_pipeline(list_of_strings):
  scores = torch.tensor(
                [output["score"] for output in reward_pipe(list_of_strings)]
  )
  return list_of_strings[torch.argmax(scores)]


best_of_n = BestOfNSampler(model, tokenizer, reward_pipeline, length_sampler=output_length_sampler)


```

And assuming you have a list/tensor of tokenized queries, you can generate better output by calling the `generate` method

```python

best_of_n.generate(query_tensors, device=device, **gen_kwargs)

```


