# SimPO Trainer

[Simple Preference Optimization](https://arxiv.org/abs/2405.14734) (SimPO) by Yu Meng, Mengzhou Xia, and Danqi Chen proposes a simpler and more effective preference optimization algorithm than DPO without using a reference model. The key designs in SimPO are (1) using length-normalized log likelihood as the implicit reward, and (2) incorporating a target reward margin in the Bradley-Terry ranking objective.

The official code can be found [here](https://github.com/princeton-nlp/SimPO).

## Expected dataset format

The SimPO trainer expects a format identical to the [DPO trainer](https://huggingface.co/docs/trl/trainer#trl.DPOTrainer), which should include three entries. These entries should be named as follows:

- `prompt`
- `chosen`
- `rejected`

for example:

```py
simpo_dataset_dict = {
    "prompt": [
        "hello",
        "how are you",
        "What is your name?",
        "What is your name?",
        "Which is the best programming language?",
        "Which is the best programming language?",
        "Which is the best programming language?",
    ],
    "chosen": [
        "hi nice to meet you",
        "I am fine",
        "My name is Mary",
        "My name is Mary",
        "Python",
        "Python",
        "Java",
    ],
    "rejected": [
        "leave me alone",
        "I am not fine",
        "Whats it to you?",
        "I dont have a name",
        "Javascript",
        "C++",
        "C++",
    ],
}
```
where the `prompt` contains the context inputs, `chosen` contains the corresponding chosen responses and `rejected` contains the corresponding negative (rejected) responses. Note that a prompt can have multiple responses and this is reflected in the entries being repeated in the dictionary's value arrays.

## Expected model format
The SimPO trainer expects a model of `AutoModelForCausalLM`, compared to PPO that expects `AutoModelForCausalLMWithValueHead` for the value function.

## Using the `SimPOTrainer`
For a detailed example have a look at the `examples/scripts/simpo.py` script. At a high level we need to initialize the `SimPOTrainer` with a `model` we wish to train. **Note that SimPOTrainer eliminates the need to use the reference model, simplifying the optimization process.**

```py
simpo_config = SimPOConfig(
    beta=2.5, # beta
    gamma_beta_ratio=0.25, # ratio between target reward margin (gamma) and beta
)

simpo_trainer = SimPOTrainer(
    model,
    args=simpo_config,
    train_dataset=train_dataset,
    tokenizer=tokenizer,
)
```
After this, one can then call:

```py
simpo_trainer.train()
```

## Logging

While training and evaluating we record the following reward metrics:

* `rewards/chosen`: the mean log probabilities of the policy model for the chosen responses scaled by beta
* `rewards/rejected`: the mean log probabilities of the policy model for the rejected responses scaled by beta
* `rewards/accuracies`: mean of how often the chosen rewards are > than the corresponding rejected rewards
* `rewards/margins`: the mean difference between the chosen and corresponding rejected rewards
 
## SimPOTrainer

[[autodoc]] SimPOTrainer

## SimPOConfig

[[autodoc]] SimPOConfig
