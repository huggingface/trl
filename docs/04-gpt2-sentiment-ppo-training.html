---

title: Tune GPT2 to generate positive reviews

keywords: fastai
sidebar: home_sidebar

summary: "Optimise GPT2 to produce positive IMDB movie reviews using a BERT sentiment classifier as a reward function."
description: "Optimise GPT2 to produce positive IMDB movie reviews using a BERT sentiment classifier as a reward function."
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/04-gpt2-sentiment-ppo-training.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div style="text-align: center">
{% include image.html max-width="600" file="/trl/images/gpt2_bert_training.png" %}
<p style="text-align: center;"> <b>Figure:</b> Experiment setup to tune GPT2. The yellow arrows are outside the scope of this notebook, but the trained models are available through Hugging Face. </p>
</div><p>In this notebook we fine-tune GPT2 (small) to generate positive movie reviews based on the IMDB dataset. The model gets the start of a real review and is tasked to produce positive continuations. To reward positive continuations we use a BERT classifier to analyse the sentiment of the produced sentences and use the classifier's outputs as rewards signals for PPO training.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Setup-experiment">Setup experiment<a class="anchor-link" href="#Setup-experiment"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Import-dependencies">Import dependencies<a class="anchor-link" href="#Import-dependencies"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">load_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 2
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">wandb</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">tqdm</span><span class="o">.</span><span class="n">pandas</span><span class="p">()</span>

<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">pipeline</span>

<span class="kn">from</span> <span class="nn">trl.gpt2</span> <span class="kn">import</span> <span class="n">GPT2HeadWithValueModel</span><span class="p">,</span> <span class="n">respond_to_batch</span>
<span class="kn">from</span> <span class="nn">trl.ppo</span> <span class="kn">import</span> <span class="n">PPOTrainer</span>
<span class="kn">from</span> <span class="nn">trl.core</span> <span class="kn">import</span> <span class="n">build_bert_batch_from_txt</span><span class="p">,</span> <span class="n">listify_batch</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Configuration">Configuration<a class="anchor-link" href="#Configuration"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;model_name&quot;</span><span class="p">:</span> <span class="s2">&quot;lvwerra/gpt2-imdb&quot;</span><span class="p">,</span>
    <span class="s2">&quot;cls_model_name&quot;</span><span class="p">:</span> <span class="s2">&quot;lvwerra/distilbert-imdb&quot;</span><span class="p">,</span>
    <span class="s2">&quot;steps&quot;</span><span class="p">:</span> <span class="mi">20000</span><span class="p">,</span>
    <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span>
    <span class="s2">&quot;forward_batch_size&quot;</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
    <span class="s2">&quot;ppo_epochs&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>   
    <span class="s2">&quot;txt_in_min_len&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s2">&quot;txt_in_max_len&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
    <span class="s2">&quot;txt_out_min_len&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="s2">&quot;txt_out_max_len&quot;</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
    <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">1.41e-5</span><span class="p">,</span>
    <span class="s2">&quot;init_kl_coef&quot;</span><span class="p">:</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="s2">&quot;target&quot;</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
    <span class="s2">&quot;horizon&quot;</span><span class="p">:</span><span class="mi">10000</span><span class="p">,</span>
    <span class="s2">&quot;gamma&quot;</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span>
    <span class="s2">&quot;lam&quot;</span><span class="p">:</span><span class="mf">0.95</span><span class="p">,</span>
    <span class="s2">&quot;cliprange&quot;</span><span class="p">:</span> <span class="mf">.2</span><span class="p">,</span>
    <span class="s2">&quot;cliprange_value&quot;</span><span class="p">:</span><span class="mf">.2</span><span class="p">,</span>
    <span class="s2">&quot;vf_coef&quot;</span><span class="p">:</span><span class="mf">.1</span><span class="p">,</span> 
<span class="p">}</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Forward batching</strong>: Since the models can be fairly big and we want to rollout large PPO batches this can lead to out-of-memory errors when doing the forward passes for text generation and sentiment analysis. We introduce the parameter <code>forward_batch_size</code> to split the forward passes into smaller batches. Although this hurts performance a little this is neglectible compared to the computations of the backward passes when optimizing the model. The same parameter is used in the <a href="/trl/02-ppo#PPOTrainer"><code>PPOTrainer</code></a> when doing forward passes. The <code>batch_size</code> should multiple of <code>forward_batch_size</code>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">pipe_device</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You can see that we load a GPT2 model called <code>gpt2_imdb</code>. This model was additionally fine-tuned on the IMDB dataset for 1 epoch with the huggingface <a href="https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py">script</a> (no special settings). The other parameters are mostly taken from the original paper <a href="https://arxiv.org/pdf/1909.08593.pdf">"Fine-Tuning Language Models from Human Preferences"</a>. This model as well as the BERT model is available in the Huggingface model zoo <a href="https://huggingface.co/models">here</a>. The following code should automatically download the models.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Initialize-W&amp;B-logger">Initialize W&amp;B logger<a class="anchor-link" href="#Initialize-W&amp;B-logger"> </a></h3><p>We use <code>wandb</code>to log all the metrics during training.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;run-42&#39;</span><span class="p">,</span> <span class="n">project</span><span class="o">=</span><span class="s1">&#39;gpt2-test&#39;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre><span class="ansi-blue-intense-fg ansi-bold">wandb</span>: Currently logged in as: <span class="ansi-yellow-fg">lvwerra</span> (use `wandb login --relogin` to force relogin)
<span class="ansi-blue-intense-fg ansi-bold">wandb</span>: wandb version 0.12.16 is available!  To upgrade, please run:
<span class="ansi-blue-intense-fg ansi-bold">wandb</span>:  $ pip install wandb --upgrade
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

                    Syncing run <strong><a href="https://wandb.ai/lvwerra/gpt2-test/runs/30spsy9r" target="_blank">run-42</a></strong> to <a href="https://wandb.ai/lvwerra/gpt2-test" target="_blank">Weights & Biases</a> (<a href="https://docs.wandb.com/integrations/jupyter.html" target="_blank">docs</a>).<br/>

                
</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<button onClick="this.nextSibling.style.display='block';this.style.display='none';">Display W&B run</button><iframe src="https://wandb.ai/lvwerra/gpt2-test/runs/30spsy9r?jupyter=true" style="border:none;width:100%;height:420px;display:none;"></iframe>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Load-data-and-models">Load data and models<a class="anchor-link" href="#Load-data-and-models"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Load-IMDB-dataset">Load IMDB dataset<a class="anchor-link" href="#Load-IMDB-dataset"> </a></h3><p>The IMDB dataset contains 50k movie review annotated with "positive"/"negative" feedback indicating the sentiment.  We load the IMDB dataset into a DataFrame and filter for comments that are at least 500 characters long and take the first 1000 characters of each comment. The first filter we apply to avoid comments that are less than <code>txt_in_len</code> token long and the second to avoid tokenizing way more text than we actually need.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># load imdb with datasets</span>
<span class="n">ds</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;imdb&#39;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">)</span>
<span class="n">ds</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">rename_columns</span><span class="p">({</span><span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="s1">&#39;review&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">:</span> <span class="s1">&#39;sentiment&#39;</span><span class="p">})</span>
<span class="n">ds</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s2">&quot;review&quot;</span><span class="p">])</span><span class="o">&gt;</span><span class="mi">200</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Reusing dataset imdb (/home/leandro/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)
Loading cached processed dataset at /home/leandro/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-ff455473e884c6a3.arrow
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ds</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Dataset({
    features: [&#39;review&#39;, &#39;sentiment&#39;],
    num_rows: 24895
})</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Load-BERT-classifier">Load BERT classifier<a class="anchor-link" href="#Load-BERT-classifier"> </a></h3><p>We load a BERT classifier fine-tuned on the IMDB dataset.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sent_kwargs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;return_all_scores&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="s2">&quot;function_to_apply&quot;</span><span class="p">:</span> <span class="s2">&quot;none&quot;</span><span class="p">,</span>
    <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;forward_batch_size&quot;</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">sentiment_pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;sentiment-analysis&quot;</span><span class="p">,</span><span class="s2">&quot;lvwerra/distilbert-imdb&quot;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">pipe_device</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The model outputs are the logits for the negative and positive class. We will use the logits for positive class as a reward signal for the language model.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;this movie was really bad!!&#39;</span>
<span class="n">sentiment_pipe</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="o">**</span><span class="n">sent_kwargs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[[{&#39;label&#39;: &#39;NEGATIVE&#39;, &#39;score&#39;: 2.335048198699951},
  {&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: -2.726576566696167}]]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;this movie was really good!!&#39;</span>
<span class="n">sentiment_pipe</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="o">**</span><span class="n">sent_kwargs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[[{&#39;label&#39;: &#39;NEGATIVE&#39;, &#39;score&#39;: -2.2947897911071777},
  {&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 2.557039737701416}]]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The resulting reward signal:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Load-pre-trained-GPT2-language-models">Load pre-trained GPT2 language models<a class="anchor-link" href="#Load-pre-trained-GPT2-language-models"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We load the GPT2 model with a value head and the tokenizer. We load the model twice; the first model is optimized while the second model serves as a reference to calculate the KL-divergence from the starting point. This serves as an additional reward signal in the PPO training to make sure the optimized model does not deviate too much from the original language model.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">gpt2_model</span> <span class="o">=</span> <span class="n">GPT2HeadWithValueModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;model_name&#39;</span><span class="p">])</span>
<span class="n">gpt2_model_ref</span> <span class="o">=</span> <span class="n">GPT2HeadWithValueModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;model_name&#39;</span><span class="p">])</span>

<span class="n">gpt2_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;model_name&#39;</span><span class="p">])</span>
<span class="n">gpt2_tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">gpt2_tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Watch-model-with-wandb">Watch model with wandb<a class="anchor-link" href="#Watch-model-with-wandb"> </a></h3><p>This wandb magic logs the gradients and weights of the model during training.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">wandb</span><span class="o">.</span><span class="n">watch</span><span class="p">(</span><span class="n">gpt2_model</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Move-models-to-GPU">Move models to GPU<a class="anchor-link" href="#Move-models-to-GPU"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If <code>cuda</code> is available move the computations to the GPU.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">gpt2_model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">);</span>
<span class="n">gpt2_model_ref</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">);</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Tokenize-IMDB-reviews">Tokenize IMDB reviews<a class="anchor-link" href="#Tokenize-IMDB-reviews"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We want to randomize the query and response length so we introduce a <code>LengthSampler</code> that uniformly samples values from an interval.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">LengthSampler</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">min_value</span><span class="p">,</span> <span class="n">max_value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">values</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">min_value</span><span class="p">,</span> <span class="n">max_value</span><span class="p">))</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
    
<span class="n">input_size</span> <span class="o">=</span> <span class="n">LengthSampler</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;txt_in_min_len&quot;</span><span class="p">],</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;txt_in_max_len&quot;</span><span class="p">])</span>
<span class="n">output_size</span> <span class="o">=</span> <span class="n">LengthSampler</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;txt_out_min_len&quot;</span><span class="p">],</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;txt_out_max_len&quot;</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We pre-tokenize all IMDB in advance to avoid tokenizing twice. In the first step we encode the queries and slice the first <code>input_size()</code> tokens. In a second step we decode these tokens back to text for later display.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">sample</span><span class="p">):</span>
    <span class="n">sample</span><span class="p">[</span><span class="s2">&quot;tokens&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gpt2_tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s2">&quot;review&quot;</span><span class="p">])[:</span><span class="n">input_size</span><span class="p">()]</span>
    <span class="n">sample</span><span class="p">[</span><span class="s2">&quot;query&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gpt2_tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s2">&quot;tokens&quot;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">sample</span>

<span class="n">ds</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Generation-settings">Generation settings<a class="anchor-link" href="#Generation-settings"> </a></h3><p>For the response generation we just use sampling and make sure top-k and nucleus sampling are turned off as well as a minimal length.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">gen_kwargs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;min_length&quot;</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="s2">&quot;top_k&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="s2">&quot;top_p&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="s2">&quot;do_sample&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="s2">&quot;pad_token_id&quot;</span><span class="p">:</span> <span class="n">gpt2_tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
<span class="p">}</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Optimize-model">Optimize model<a class="anchor-link" href="#Optimize-model"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Dataloader">Dataloader<a class="anchor-link" href="#Dataloader"> </a></h3><p>We use a dataloader to return the batches of queries used for each PPO epoch:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">collater</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">dict</span><span class="p">((</span><span class="n">key</span><span class="p">,</span> <span class="p">[</span><span class="n">d</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">data</span><span class="p">])</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">],</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">collater</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Training-loop">Training loop<a class="anchor-link" href="#Training-loop"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The training loop consists of the following main steps:</p>
<ol>
<li>Get the query responses from the policy network (GPT-2)</li>
<li>Get sentiments for query/responses from BERT</li>
<li>Optimize policy with PPO using the (query, response, reward) triplet</li>
</ol>
<p><strong>Training time</strong></p>
<p>This step takes <strong>~2h</strong> on a V100 GPU with the above specified settings.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ppo_trainer</span> <span class="o">=</span> <span class="n">PPOTrainer</span><span class="p">(</span><span class="n">gpt2_model</span><span class="p">,</span> <span class="n">gpt2_model_ref</span><span class="p">,</span> <span class="n">gpt2_tokenizer</span><span class="p">,</span> <span class="o">**</span><span class="n">config</span><span class="p">)</span>

<span class="n">total_ppo_epochs</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;steps&quot;</span><span class="p">]</span><span class="o">/</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]))</span>

<span class="k">for</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">total_ppo_epochs</span><span class="p">),</span> <span class="nb">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">))):</span>
    <span class="n">logs</span><span class="p">,</span> <span class="n">timing</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(),</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">query_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;tokens&quot;</span><span class="p">]]</span>
    
    <span class="c1">#### Get response from gpt2</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">response_tensors</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]):</span>
        <span class="n">gen_len</span> <span class="o">=</span> <span class="n">output_size</span><span class="p">()</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">gpt2_model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">query_tensors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
                                       <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">gen_len</span><span class="p">,</span> <span class="o">**</span><span class="n">gen_kwargs</span><span class="p">)</span>
        <span class="n">response_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()[</span><span class="o">-</span><span class="n">gen_len</span><span class="p">:])</span>
    <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;response&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">gpt2_tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">response_tensors</span><span class="p">]</span>
    <span class="n">timing</span><span class="p">[</span><span class="s1">&#39;time/get_response&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">t</span>

    <span class="c1">#### Compute sentiment score</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">q</span> <span class="o">+</span> <span class="n">r</span> <span class="k">for</span> <span class="n">q</span><span class="p">,</span><span class="n">r</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;query&#39;</span><span class="p">],</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;response&#39;</span><span class="p">])]</span>
    <span class="n">pipe_outputs</span> <span class="o">=</span> <span class="n">sentiment_pipe</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="o">**</span><span class="n">sent_kwargs</span><span class="p">)</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">output</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;score&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">pipe_outputs</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">timing</span><span class="p">[</span><span class="s1">&#39;time/get_sentiment_preds&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">t</span>
    
    <span class="c1">#### Run PPO step </span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">stats</span> <span class="o">=</span> <span class="n">ppo_trainer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">query_tensors</span><span class="p">,</span> <span class="n">response_tensors</span><span class="p">,</span> <span class="n">rewards</span><span class="p">)</span>
    <span class="n">timing</span><span class="p">[</span><span class="s1">&#39;time/optimization&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">t</span>
     
    <span class="c1">#### Log everything</span>
    <span class="n">timing</span><span class="p">[</span><span class="s1">&#39;time/epoch&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">t0</span>
    <span class="n">table_rows</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">r</span><span class="p">)</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;query&#39;</span><span class="p">],</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;response&#39;</span><span class="p">],</span> <span class="n">rewards</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">())]</span>
    <span class="n">logs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;game_log&#39;</span><span class="p">:</span> <span class="n">wandb</span><span class="o">.</span><span class="n">Table</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;query&#39;</span><span class="p">,</span> <span class="s1">&#39;response&#39;</span><span class="p">,</span> <span class="s1">&#39;reward&#39;</span><span class="p">],</span> <span class="n">rows</span><span class="o">=</span><span class="n">table_rows</span><span class="p">)})</span>
    <span class="n">logs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">timing</span><span class="p">)</span>
    <span class="n">logs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">stats</span><span class="p">)</span>
    <span class="n">logs</span><span class="p">[</span><span class="s1">&#39;env/reward_mean&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">logs</span><span class="p">[</span><span class="s1">&#39;env/reward_std&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">logs</span><span class="p">[</span><span class="s1">&#39;env/reward_dist&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">rewards</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">logs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Training-progress">Training progress<a class="anchor-link" href="#Training-progress"> </a></h3><p>If you are tracking the training progress with Weights&amp;Biases you should see a plot similar to the one below. Check out the interactive sample report on wandb.ai: <a href="https://app.wandb.ai/lvwerra/trl-showcase/runs/1jtvxb1m/">link</a>.</p>
<div style="text-align: center">
{% include image.html max-width="800" file="/trl/images/gpt2_tuning_progress.png" %}
<p style="text-align: center;"> <b>Figure:</b> Reward mean and distribution evolution during training. </p>
</div><p>One can observe how the model starts to generate more positive outputs after a few optimisation steps.
{% include note.html content='Investigating the KL-divergence will probably show that at this point the model has not converged to the target KL-divergence, yet. To get there would require longer training or starting with a higher inital coefficient.' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Model-inspection">Model inspection<a class="anchor-link" href="#Model-inspection"> </a></h2><p>Let's inspect some examples from the IMDB dataset. We can use <code>gpt2_model_ref</code> to compare the tuned model <code>gpt2_model</code> against the model before optimisation.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#### get a batch from the dataset</span>
<span class="n">bs</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">game_data</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="n">ds</span><span class="o">.</span><span class="n">set_format</span><span class="p">(</span><span class="s2">&quot;pandas&quot;</span><span class="p">)</span>
<span class="n">df_batch</span> <span class="o">=</span> <span class="n">ds</span><span class="p">[:]</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">bs</span><span class="p">)</span>
<span class="n">game_data</span><span class="p">[</span><span class="s1">&#39;query&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_batch</span><span class="p">[</span><span class="s1">&#39;query&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">query_tensors</span> <span class="o">=</span> <span class="n">df_batch</span><span class="p">[</span><span class="s1">&#39;tokens&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

<span class="n">response_tensors_ref</span><span class="p">,</span> <span class="n">response_tensors</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="c1">#### get response from gpt2 and gpt2_ref</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">bs</span><span class="p">):</span>
    <span class="n">gen_len</span> <span class="o">=</span> <span class="n">output_size</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">gpt2_model_ref</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">query_tensors</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                                     <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">gen_len</span><span class="p">,</span> <span class="o">**</span><span class="n">gen_kwargs</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()[</span><span class="o">-</span><span class="n">gen_len</span><span class="p">:]</span>
    <span class="n">response_tensors_ref</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">gpt2_model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">query_tensors</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                                 <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">gen_len</span><span class="p">,</span> <span class="o">**</span><span class="n">gen_kwargs</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()[</span><span class="o">-</span><span class="n">gen_len</span><span class="p">:]</span>
    <span class="n">response_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

<span class="c1">#### decode responses</span>
<span class="n">game_data</span><span class="p">[</span><span class="s1">&#39;response (before)&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">gpt2_tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">response_tensors_ref</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">bs</span><span class="p">)]</span>
<span class="n">game_data</span><span class="p">[</span><span class="s1">&#39;response (after)&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">gpt2_tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">response_tensors</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">bs</span><span class="p">)]</span>

<span class="c1">#### sentiment analysis of query/response pairs before/after</span>
<span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">q</span> <span class="o">+</span> <span class="n">r</span> <span class="k">for</span> <span class="n">q</span><span class="p">,</span><span class="n">r</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">game_data</span><span class="p">[</span><span class="s1">&#39;query&#39;</span><span class="p">],</span> <span class="n">game_data</span><span class="p">[</span><span class="s1">&#39;response (before)&#39;</span><span class="p">])]</span>
<span class="n">game_data</span><span class="p">[</span><span class="s1">&#39;rewards (before)&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">output</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;score&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">sentiment_pipe</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="o">**</span><span class="n">sent_kwargs</span><span class="p">)]</span>

<span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">q</span> <span class="o">+</span> <span class="n">r</span> <span class="k">for</span> <span class="n">q</span><span class="p">,</span><span class="n">r</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">game_data</span><span class="p">[</span><span class="s1">&#39;query&#39;</span><span class="p">],</span> <span class="n">game_data</span><span class="p">[</span><span class="s1">&#39;response (after)&#39;</span><span class="p">])]</span>
<span class="n">game_data</span><span class="p">[</span><span class="s1">&#39;rewards (after)&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">output</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;score&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">sentiment_pipe</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="o">**</span><span class="n">sent_kwargs</span><span class="p">)]</span>

<span class="c1"># store results in a dataframe</span>
<span class="n">df_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">game_data</span><span class="p">)</span>
<span class="n">df_results</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/home/leandro/miniconda3/envs/trl/lib/python3.9/site-packages/transformers/pipelines/base.py:1075: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
  warnings.warn(
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>query</th>
      <th>response (before)</th>
      <th>response (after)</th>
      <th>rewards (before)</th>
      <th>rewards (after)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Oh dear,</td>
      <td>what are I saying?! I fast-forwarded through</td>
      <td>I must say that I are hanging my head on this</td>
      <td>-0.858954</td>
      <td>-1.007609</td>
    </tr>
    <tr>
      <th>1</th>
      <td>I've seen</td>
      <td>it, as well.&lt;br</td>
      <td>three million dialogue throughout, and</td>
      <td>1.996807</td>
      <td>2.240883</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Hi:&lt;br /&gt;&lt;br</td>
      <td>/&gt;This movie is a turkey though when it comes to</td>
      <td>/&gt;I also like that movie. It's so funny</td>
      <td>-0.438191</td>
      <td>2.415630</td>
    </tr>
    <tr>
      <th>3</th>
      <td>I'm a writer</td>
      <td>and I'm not going to be asked to</td>
      <td>, not a screenwriter. I've written</td>
      <td>-0.655991</td>
      <td>-0.724324</td>
    </tr>
    <tr>
      <th>4</th>
      <td>If you</td>
      <td>absolutely love sensitive romance, the plot a...</td>
      <td>are looking at the cinematography, the acting,</td>
      <td>2.221309</td>
      <td>0.148751</td>
    </tr>
    <tr>
      <th>5</th>
      <td>OMG this</td>
      <td>casting cast. Obi cult breezy, this is</td>
      <td>movie was totally wonderful, I it was the ide...</td>
      <td>-1.533139</td>
      <td>2.590190</td>
    </tr>
    <tr>
      <th>6</th>
      <td>It's</td>
      <td>unrealistic; the guy who was supposed to be E...</td>
      <td>a very good film. It reminds us about over</td>
      <td>-2.097017</td>
      <td>2.835831</td>
    </tr>
    <tr>
      <th>7</th>
      <td>There is a really</td>
      <td>awful laptop game!&lt;br /&gt;&lt;br /&gt;I used to</td>
      <td>interesting story that set us the journey. Th...</td>
      <td>-2.341743</td>
      <td>2.282939</td>
    </tr>
    <tr>
      <th>8</th>
      <td>This is</td>
      <td>my favorite part about</td>
      <td>a well thought well</td>
      <td>2.554794</td>
      <td>2.734139</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Wasn't</td>
      <td>Wasn't it clichd?&lt;|endoftext|&gt;</td>
      <td>anyone else interested in this movie? It's a ...</td>
      <td>-1.790802</td>
      <td>2.631960</td>
    </tr>
    <tr>
      <th>10</th>
      <td>This film is another of director Tim</td>
      <td>Burton's masterpieces</td>
      <td>Curry's best bombs</td>
      <td>2.622917</td>
      <td>2.544106</td>
    </tr>
    <tr>
      <th>11</th>
      <td>I thought this movie</td>
      <td>was excellent. I actually laughed 6 times and...</td>
      <td>was perfect, and I believe it's almost overlo...</td>
      <td>2.548022</td>
      <td>2.601913</td>
    </tr>
    <tr>
      <th>12</th>
      <td>This early John Wayne</td>
      <td>films looked like an abandoned police beating</td>
      <td>film is a realistic portrayal of what</td>
      <td>-1.742279</td>
      <td>2.609762</td>
    </tr>
    <tr>
      <th>13</th>
      <td>I was</td>
      <td>given an experience-a big one, almost 25</td>
      <td>very happy with all the reflections and this ...</td>
      <td>2.250709</td>
      <td>2.558540</td>
    </tr>
    <tr>
      <th>14</th>
      <td>Embarrassingly, I</td>
      <td>am more at a strict conformity after getting ...</td>
      <td>had never seen a movie before. There was one ...</td>
      <td>-2.021666</td>
      <td>-1.803383</td>
    </tr>
    <tr>
      <th>15</th>
      <td>I am a fan</td>
      <td>of living on simple islands, and we have visi...</td>
      <td>of many things and learned how to appreciate ...</td>
      <td>1.791297</td>
      <td>2.324461</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Looking at the reward mean/median of the generated sequences we observe a significant difference.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;mean:&#39;</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">df_results</span><span class="p">[[</span><span class="s2">&quot;rewards (before)&quot;</span><span class="p">,</span> <span class="s2">&quot;rewards (after)&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;median:&#39;</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">df_results</span><span class="p">[[</span><span class="s2">&quot;rewards (before)&quot;</span><span class="p">,</span> <span class="s2">&quot;rewards (after)&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">median</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>mean:
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea ">
<pre>rewards (before)    0.156629
rewards (after)     1.686487
dtype: float64</pre>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>
median:
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea ">
<pre>rewards (before)   -0.547091
rewards (after)     2.479868
dtype: float64</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Save-model">Save model<a class="anchor-link" href="#Save-model"> </a></h2><p>Finally, we save the model and push it to the Hugging Face for later usage.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">gpt2_model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2-imdb-pos-v2&#39;</span><span class="p">,</span> <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">gpt2_tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2-imdb-pos-v2&#39;</span><span class="p">,</span> <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/home/leandro/miniconda3/envs/trl/lib/python3.9/site-packages/huggingface_hub/hf_api.py:1001: FutureWarning: `create_repo` now takes `token` as an optional positional argument. Be sure to adapt your code!
  warnings.warn(
Cloning https://huggingface.co/lvwerra/gpt2-imdb-pos-v2 into local empty directory.
remote: Enforcing permissions...        
remote: Allowed refs: all        
To https://huggingface.co/lvwerra/gpt2-imdb-pos-v2
   369b075..28b9865  main -&gt; main

remote: Enforcing permissions...        
remote: Allowed refs: all        
To https://huggingface.co/lvwerra/gpt2-imdb-pos-v2
   28b9865..42792ea  main -&gt; main

</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(&#39;gpt2-imdb-pos-v2/tokenizer_config.json&#39;,
 &#39;gpt2-imdb-pos-v2/special_tokens_map.json&#39;,
 &#39;gpt2-imdb-pos-v2/vocab.json&#39;,
 &#39;gpt2-imdb-pos-v2/merges.txt&#39;,
 &#39;gpt2-imdb-pos-v2/added_tokens.json&#39;,
 &#39;gpt2-imdb-pos-v2/tokenizer.json&#39;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

