# Copyright 2024 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import time
import warnings
from typing import Any, Callable, Dict, List, Optional, Tuple, Union

import torch
import torch.nn.functional as F
# from accelerate.utils import verksam_save_model # Removed as it's not used and causes import error
from datasets import Dataset
from transformers import DataCollatorForLanguageModeling, PreTrainedTokenizer, PreTrainedTokenizerFast, TrainerCallback

from typing import Any, Callable, Dict, List, Optional, Tuple, Union

import torch
import torch.nn.functional as F # Added for potential F.kl_div
# from accelerate.utils import verksam_save_model # May not be needed if super().save_model works
from datasets import Dataset
from transformers import DataCollatorForLanguageModeling, PreTrainedTokenizer, PreTrainedTokenizerFast, TrainerCallback
from transformers.utils import ModelOutput # For type hinting model outputs

# from ..core import logprobs_from_logits # Not found in trl.core
from .ppo_trainer import PPOTrainer
from .repo_config import RePOConfig
from .utils import ReplayBuffer, selective_log_softmax # Import selective_log_softmax


class RePOTrainer(PPOTrainer):
    """
    The RePOTrainer class implements Replay-Enhanced Policy Optimization (RePO)
    by enhancing the `PPOTrainer` with a replay buffer mechanism.

    RePO aims to improve sample efficiency and stability by reusing past experiences
    during policy optimization. It combines on-policy updates (like standard PPO)
    with off-policy updates from data sampled from a replay buffer. Importance
    sampling is used to correct for the distributional mismatch between the data
    in the replay buffer (generated by older policies) and the current policy.

    Args:
        config (`RePOConfig`, *optional*):
            Configuration object for the RePOTrainer. If not provided, a default
            `RePOConfig` will be instantiated. This config inherits from `PPOConfig`
            and adds RePO-specific hyperparameters.
        model (`PreTrainedModelWrapper` or `torch.nn.Module`, *optional*):
            The model to be trained. This should be a Hugging Face Transformers model,
            optionally wrapped (e.g., for PEFT). `PPOTrainer` handles the core model setup.
        ref_model (`PreTrainedModelWrapper` or `torch.nn.Module`, *optional*):
            The reference model used for KL divergence calculation in PPO. If `None`,
            `PPOTrainer` may create a reference model from the initial state of `model`.
        tokenizer (`PreTrainedTokenizerBase`, *optional*):
            The tokenizer used for processing data. Must be provided.
        dataset (`Dataset` or `Dict[str, Dataset]`, *optional*):
            The dataset to be used for training. `PPOTrainer` handles dataset processing.
        data_collator (`Callable`, *optional*):
            The data collator to use. `PPOTrainer` has a default.
        optimizer (`torch.optim.Optimizer`, *optional*):
            A custom optimizer. If `None`, `PPOTrainer` will create a default AdamW optimizer.
        lr_scheduler (`torch.optim.lr_scheduler._LRScheduler`, *optional*):
            A custom learning rate scheduler. If `None`, `PPOTrainer` will create a default
            linear scheduler.
        callbacks (`List[TrainerCallback]`, *optional*):
            A list of custom callbacks to customize the training loop.
    """

    def __init__(
        self,
        config: Optional[RePOConfig] = None,
        model: Optional[Any] = None,
        ref_model: Optional[Any] = None,
        tokenizer: Optional[Union[PreTrainedTokenizer, PreTrainedTokenizerFast]] = None,
        dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,
        data_collator: Optional[Callable] = None,
        optimizer: Optional[torch.optim.Optimizer] = None,
        lr_scheduler: Optional[Any] = None,
        callbacks: Optional[List[TrainerCallback]] = None,
    ):
        if config is None:
            # Ensure RePOConfig is used if no config is provided
            config = RePOConfig()
        elif not isinstance(config, RePOConfig):
            warnings.warn(
                "You passed a PPOConfig instance to RePOTrainer. RePOConfig will be used instead.", UserWarning
            )
            # Attempt to convert if it's a PPOConfig, otherwise, default RePOConfig
            # This assumes RePOConfig can be initialized from PPOConfig's fields if necessary,
            # or that PPOConfig fields are a subset of RePOConfig.
            # For simplicity, we'll just use the RePOConfig default if a non-RePOConfig is passed.
            # A more robust solution would be to take PPOConfig and upgrade it.
            config_dict = config.to_dict()
            config = RePOConfig(**config_dict)


        super().__init__(
            config=config, # Pass the (potentially new) RePOConfig
            model=model,
            ref_model=ref_model,
            tokenizer=tokenizer,
            dataset=dataset,
            data_collator=data_collator,
            optimizer=optimizer, # PPOTrainer handles optimizer creation if None
            lr_scheduler=lr_scheduler, # PPOTrainer handles scheduler creation if None
            callbacks=callbacks,
        )

        # Re-assign config to ensure it's the RePOConfig instance
        self.repo_config = config
        # self.config from super() is now also repo_config due to Python's object handling

        self.replay_buffer = ReplayBuffer(capacity=self.repo_config.replay_buffer_size)

        if self.tokenizer is None: # Should have been set by PPOTrainer
            raise ValueError("Tokenizer must be provided and set in PPOTrainer.")

    # We keep PPOTrainer's batched_forward_pass as it's general enough.
    # If RePO required different forward pass logic for off-policy data,
    # we might need a separate one or to adapt this one.
    # def batched_forward_pass( ... ) -> from PPOTrainer

    def train(
        self,
        resume_from_checkpoint: Optional[Union[str, bool]] = None,
        trial: Union["optuna.Trial", Dict[str, Any]] = None, # Added trial for consistency with Trainer
        **kwargs, # Added kwargs for consistency with Trainer
    ):
        """
        Main training loop for RePOTrainer.
        Currently, this method calls the superclass `PPOTrainer.train()` and relies
        on the overridden `compute_loss` method to implement RePO's specific logic
        for loss calculation and replay buffer interaction.

        Further refinement might involve overriding more parts of the PPO training loop
        if deeper integration for experience collection or step timing is needed.
        """
        # The PPOTrainer's `train` method orchestrates rollouts and learning.
        # Our RePO logic is primarily injected via `compute_loss`.
        # If `compute_loss` is called by `training_step` which gets `ppo_elements`
        # (as it does in PPOTrainer), then adding to replay buffer there is feasible.
        super().train(resume_from_checkpoint=resume_from_checkpoint, trial=trial, **kwargs)


    def compute_loss(
        self,
        model: Optional[torch.nn.Module] = None,
        inputs: Optional[Dict[str, Union[torch.Tensor, Any]]] = None, # These are ppo_elements from PPOTrainer
        return_outputs: bool = False,
    ) -> Union[torch.Tensor, Tuple[torch.Tensor, Dict[str, torch.Tensor]]]:
        """
        Computes the RePO loss, combining on-policy PPO loss with an off-policy update
        from the replay buffer.

        Args:
            model (`torch.nn.Module`, *optional*): The model to compute the loss for.
                If None, `self.model` is used.
            inputs (`Dict[str, Union[torch.Tensor, Any]]`, *optional*):
                The input batch for on-policy PPO calculation. This dictionary is
                expected to contain all elements required by `PPOTrainer.compute_loss`
                (e.g., `query_tensors`, `response_tensors`, `logprobs`, `values`,
                `rewards`, `advantages`, `returns`).
            return_outputs (`bool`, *optional*, defaults to `False`):
                Whether to return all metrics along with the loss.

        Returns:
            `Union[torch.Tensor, Tuple[torch.Tensor, Dict[str, torch.Tensor]]]`:
                If `return_outputs` is `False` (default), returns the total loss tensor.
                If `return_outputs` is `True`, returns a tuple `(loss, metrics_dict)`,
                where `metrics_dict` contains on-policy and off-policy metrics.
        """
        if model is None:
            model = self.model # self.model is the actor-critic model from PPOTrainer

        # 1. Compute standard PPO loss for the on-policy batch
        # This call to super().compute_loss will use PPOTrainer's loss calculation
        # which includes policy loss (clipped surrogate objective) and value loss.
        on_policy_loss, on_policy_metrics = super().compute_loss(model, inputs, return_outputs=True)

        # 2. Store the on-policy experience into the replay buffer
        if self.repo_config.use_replay_buffer:
            # Detach tensors before sending to CPU to avoid holding onto computation graph
            experience_to_add = {
                "query_tensors": inputs["query_tensors"].detach().cpu(),
                "response_tensors": inputs["response_tensors"].detach().cpu(),
                "logprobs": inputs["logprobs"].detach().cpu(),  # These are logprobs from behavior policy (current policy)
                "values": inputs["values"].detach().cpu(),      # Values from behavior policy's value function
                "rewards": inputs["rewards"].detach().cpu(),
                "advantages": inputs["advantages"].detach().cpu(), # Advantages calculated w.r.t. behavior policy
                "returns": inputs["returns"].detach().cpu(),
            }
            # ref_logprobs for KL term are usually computed on-the-fly in PPO if ref_model exists.
            # For off-policy KL, we'll recompute them with self.ref_model on off-policy data.
            self.replay_buffer.add(experience_to_add)
            on_policy_metrics["replay_buffer/size"] = float(len(self.replay_buffer))

        total_loss = on_policy_loss
        all_metrics = on_policy_metrics

        # 3. Off-policy update from replay buffer
        if (
            self.repo_config.use_replay_buffer
            and self.state.global_step >= self.repo_config.replay_warmup_steps
            and len(self.replay_buffer) >= self.repo_config.replay_batch_size
        ):
            off_policy_experiences = self.replay_buffer.sample(self.repo_config.replay_batch_size, device=self.accelerator.device)

            if off_policy_experiences is not None:
                # Collate the list of experience dicts into a single dict of batched tensors
                off_policy_batch = {}
                keys = off_policy_experiences[0].keys()
                for key in keys:
                    try:
                        # Ensure tensors are on the correct device (already handled by sample method if device is passed)
                        off_policy_batch[key] = torch.stack([exp[key] for exp in off_policy_experiences])
                    except Exception as e: # Catch potential errors if tensors are not stackable (e.g. variable lengths not handled by ReplayBuffer)
                        warnings.warn(f"Skipping off-policy update due to error processing batch for key '{key}': {e}")
                        if return_outputs: return total_loss, all_metrics
                        return total_loss

                off_policy_query_tensors = off_policy_batch["query_tensors"]
                off_policy_response_tensors = off_policy_batch["response_tensors"]
                # These are logprobs from the policy that generated the data (behavior policy \pi_k)
                behavior_policy_response_logprobs = off_policy_batch["logprobs"]
                # Advantages computed w.r.t. behavior policy's value function V_{\phi_k}
                off_policy_advantages = off_policy_batch["advantages"]

                # Prepare inputs for current policy's forward pass
                input_ids_off_policy = torch.cat([off_policy_query_tensors, off_policy_response_tensors], dim=1)
                attention_mask_off_policy = input_ids_off_policy.ne(self.tokenizer.pad_token_id).long().to(input_ids_off_policy.device)

                # a. Recompute logprobs with current policy (\pi_\theta)
                # model is self.model (actor_critic) from PPOTrainer
                current_policy_logits, _, _ = model(input_ids_off_policy, attention_mask=attention_mask_off_policy)
                # current_policy_all_logprobs = logprobs_from_logits(current_policy_logits[:, :-1, :], input_ids_off_policy[:, 1:])
                current_policy_all_logprobs = selective_log_softmax(current_policy_logits[:, :-1, :], input_ids_off_policy[:, 1:])

                query_len = off_policy_query_tensors.shape[1]
                current_policy_response_logprobs = current_policy_all_logprobs[:, query_len-1:]

                # b. Importance Sampling weights (rho)
                # rho_t = \pi_\theta(a_t|s_t) / \pi_k(a_t|s_t)
                # log_rho_t = log \pi_\theta(a_t|s_t) - log \pi_k(a_t|s_t)
                # For sequences, we sum log probabilities: log_rho = sum_t(log_pi_theta) - sum_t(log_pi_k)
                log_rho = current_policy_response_logprobs.sum(dim=-1) - behavior_policy_response_logprobs.sum(dim=-1)
                rho = torch.exp(log_rho)
                clipped_rho = torch.clamp(rho, 0.0, self.repo_config.importance_sampling_clip) # Clip to avoid large variance, 0 to avoid negative weights if log_rho is very small.

                all_metrics["off_policy/mean_rho"] = rho.mean().item()
                all_metrics["off_policy/mean_clipped_rho"] = clipped_rho.mean().item()

                # c. Off-policy policy loss component (PPO-like surrogate objective)
                # L_CLIP^{OFF} = E_replay [ min(rho_t * A_t^{\pi_k}, clip(rho_t, 1-eps, 1+eps) * A_t^{\pi_k}) ]
                # The advantages A_t^{\pi_k} are from the replay buffer.
                # PPOTrainer's advantages are per-token. Sum them for trajectory advantage for this objective.
                # Note: The paper's Eq. 7 suggests a simpler E_replay [rho_t * A_t], which is one part of PPO's clipped objective.
                # For a more complete PPO-style off-policy update, we might use the full clipped surrogate.
                # Let's implement the simpler version from RePO paper for now: L_OFF = - E [ rho_t * A_t ]

                # Sum advantages per-token for each trajectory in the off-policy batch
                summed_advantages_off_policy = off_policy_advantages.sum(dim=-1)
                off_policy_objective = clipped_rho * summed_advantages_off_policy
                # Factor of -1 because we are maximizing this objective (gradient ascent)
                # but optimizers minimize loss.
                off_policy_loss_component = -off_policy_objective.mean()

                all_metrics["off_policy/policy_objective_mean"] = off_policy_objective.mean().item()
                total_loss += off_policy_loss_component

                # d. KL Regularization for off-policy data (optional, as per RePO paper)
                if self.repo_config.kl_coef_replay > 0 and self.ref_model is not None:
                    with torch.no_grad(): # ref_model is not trained
                        ref_model_logits, _, _ = self.ref_model(
                            input_ids_off_policy, attention_mask=attention_mask_off_policy
                        )
                    ref_all_logprobs = logprobs_from_logits(ref_model_logits[:, :-1, :], input_ids_off_policy[:, 1:])
                    ref_response_logprobs = ref_all_logprobs[:, query_len-1:]

                    # KL(current_policy || ref_policy) for off-policy data
                    # sum_responses ( exp(current_logprobs) * (current_logprobs - ref_logprobs) )
                    # A common approximation is mean(current_logprobs - ref_logprobs) per sequence
                    kl_div_off_policy_per_sequence = (current_policy_response_logprobs - ref_response_logprobs).sum(dim=-1)
                    kl_div_off_policy = kl_div_off_policy_per_sequence.mean()

                    off_policy_kl_loss = self.repo_config.kl_coef_replay * kl_div_off_policy
                    all_metrics["off_policy/kl_div_mean"] = kl_div_off_policy.item()
                    all_metrics["off_policy/kl_loss"] = off_policy_kl_loss.item()
                    total_loss += off_policy_kl_loss

        all_metrics["loss/total"] = total_loss.item() # Ensure total loss is also logged

        if return_outputs:
            return total_loss, all_metrics
        return total_loss

    # TODO: Override ppo_train_loop if necessary to ensure `inputs` to `compute_loss`
    # are indeed the full ppo_elements, and to better control when replay buffer `add` happens.
    # The current implementation assumes `compute_loss` is called with all necessary PPO elements.
