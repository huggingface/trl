{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install accelerate>=0.26.0"
   ]
  },
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 0.00 Gb\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "VRAM_USED = []\n",
    "VRAM_NOTES = []\n",
    "\n",
    "def add_vram_checkpoint(note: str):\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.memory_allocated()\n",
    "    VRAM_USED.append(torch.cuda.memory_allocated())\n",
    "    VRAM_NOTES.append(note)\n",
    "    log_vram_usage()\n",
    "\n",
    "def log_vram_usage():\n",
    "    for i in range(len(VRAM_USED)):\n",
    "        print(f\"{VRAM_NOTES[i]}: {VRAM_USED[i] / 1000 / 1000 / 1000:.2f} Gb\")\n",
    "\n",
    "add_vram_checkpoint(\"Start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 0.00 Gb\n",
      "Model loaded: 1.00 Gb\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/QWEN2.5-Coder-0.5B-Instruct\",\n",
    "    torch_dtype = torch.float16,\n",
    "    device_map = \"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/QWEN2.5-Coder-0.5B-Instruct\")\n",
    "add_vram_checkpoint(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use a list of prompts\n",
    "\n",
    "prompts = [\n",
    "    \"\"\"We can leverage encode_plus() to output the Token Type IDs for us:\n",
    "\n",
    "# Continuation of the previous script\n",
    "encoded_dict = tokenizer.encode_plus(sequence_a, sequence_b)\n",
    "\n",
    "assert encoded_dict['input_ids'] == [101, 20164, 10932, 2271, 7954, 1110, 1359, 1107, 17520, 102, 2777, 1110, 20164, 10932, 2271, 7954, 1359, 136, 102]\n",
    "assert encoded_dict['token_type_ids'] == [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "T\"\"\",\\\n",
    "    \"\"\"hi, what's up?\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 225])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_inputs = tokenizer(\n",
    "    prompts, return_tensors=\"pt\", padding=True, padding_side=\"left\", add_special_tokens=False\n",
    ").to(model.device)\n",
    "prompt_inputs[\"input_ids\"].shape # Shape = B * Max(prompt_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225\n"
     ]
    }
   ],
   "source": [
    "prompt_length = prompt_inputs[\"input_ids\"].size(1) # 21\n",
    "print(prompt_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "G = 4\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=32,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    num_return_sequences=G,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andy\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\integrations\\sdpa_attention.py:48: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 0.00 Gb\n",
      "Model loaded: 1.00 Gb\n",
      "after generation: 1.00 Gb\n"
     ]
    }
   ],
   "source": [
    "prompt_completion_ids= model.generate(**prompt_inputs, generation_config=generation_config)\n",
    "add_vram_checkpoint(\"after generation\")\n",
    "# 2108MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 257])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_completion_ids.shape # B * G * [p + c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# verify the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_4_prompt_ids = prompt_completion_ids[:4, :prompt_length] # prompt tokens\n",
    "# first_2_prompt_ids = prompt_completion_ids[:2, :prompt_length] # prompt tokens\n",
    "# first_1_prompt_ids = prompt_completion_ids[:1, :prompt_length] # prompt tokens\n",
    "# all_8_prompt_ids = prompt_completion_ids[:, :prompt_length] # prompt tokens\n",
    "\n",
    "# assert torch.equal(first_4_prompt_ids[0, :], first_2_prompt_ids[0, :])\n",
    "# assert torch.equal(first_2_prompt_ids[0, :], first_1_prompt_ids[0, :])\n",
    "# assert torch.equal(first_1_prompt_ids[0, :], all_8_prompt_ids[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# first_4_prompt_completion_ids = prompt_completion_ids[:4, :]\n",
    "# first_2_prompt_completion_ids = prompt_completion_ids[:2, :]\n",
    "# first_1_prompt_completion_ids = prompt_completion_ids[:1, :]\n",
    "# all_8_prompt_completion_ids = prompt_completion_ids[:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_4_prompt_ids_logits = model(first_4_prompt_ids).logits\n",
    "# first_2_prompt_ids_logits = model(first_2_prompt_ids).logits\n",
    "# first_1_prompt_ids_logits = model(first_1_prompt_ids).logits\n",
    "# all_8_prompt_ids_logits = model(all_8_prompt_ids).logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-12.9922, -17.0938, -16.8594,  ..., -14.4922, -14.4922, -14.4922],\n",
       "        [-11.8750, -12.4453, -16.4062,  ..., -21.2656, -21.2812, -21.2812],\n",
       "        [-13.7031, -15.9922, -17.9531,  ..., -20.6875, -20.7031, -20.6875],\n",
       "        ...,\n",
       "        [-16.4844, -17.0625, -16.3281,  ..., -24.0000, -24.0000, -24.0000],\n",
       "        [-12.9141,  -8.5938,  -3.7461,  ..., -20.4688, -20.4688, -20.4688],\n",
       "        [-12.6875, -10.0547,  -5.6836,  ..., -19.1094, -19.1094, -19.1094]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first_1_prompt_ids_logits[0].log_softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-12.9922, -17.0781, -16.8594,  ..., -14.4844, -14.4922, -14.4844],\n",
       "        [-11.8828, -12.4375, -16.4062,  ..., -21.2812, -21.2812, -21.2812],\n",
       "        [-13.7031, -16.0000, -17.9375,  ..., -20.6875, -20.7031, -20.6875],\n",
       "        ...,\n",
       "        [-16.4844, -17.0625, -16.3281,  ..., -24.0000, -24.0000, -24.0000],\n",
       "        [-12.9219,  -8.5938,  -3.7461,  ..., -20.4688, -20.4688, -20.4688],\n",
       "        [-12.6797, -10.0547,  -5.6797,  ..., -19.1094, -19.1094, -19.1094]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first_4_prompt_ids_logits[0].log_softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-12.9922, -17.0938, -16.8594,  ..., -14.4922, -14.4922, -14.4922],\n",
       "        [-11.8750, -12.4453, -16.4062,  ..., -21.2812, -21.2812, -21.2812],\n",
       "        [-13.7031, -15.9922, -17.9375,  ..., -20.7031, -20.7031, -20.7031],\n",
       "        ...,\n",
       "        [-16.4844, -17.0625, -16.3281,  ..., -24.0000, -24.0000, -24.0000],\n",
       "        [-12.9141,  -8.6016,  -3.7539,  ..., -20.4844, -20.4844, -20.4844],\n",
       "        [-12.6797, -10.0547,  -5.6836,  ..., -19.1094, -19.1094, -19.1094]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first_2_prompt_ids_logits[0].log_softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_4_prompt_completion_ids_logits = model(first_4_prompt_completion_ids).logits\n",
    "# first_2_prompt_completion_ids_logits = model(first_2_prompt_completion_ids).logits\n",
    "# # first_1_prompt_completion_ids_logits = model(first_1_prompt_completion_ids).logits\n",
    "# # all_8_prompt_completion_ids_logits = model(all_8_prompt_completion_ids).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-12.9922, -17.0781, -16.8594,  ..., -14.4844, -14.4922, -14.4844],\n",
       "        [-11.8828, -12.4375, -16.4062,  ..., -21.2812, -21.2812, -21.2812],\n",
       "        [-13.7031, -16.0000, -17.9375,  ..., -20.6875, -20.7031, -20.6875],\n",
       "        ...,\n",
       "        [-16.4844, -17.0625, -16.3281,  ..., -24.0000, -24.0000, -24.0000],\n",
       "        [-12.9219,  -8.5938,  -3.7461,  ..., -20.4688, -20.4688, -20.4688],\n",
       "        [-12.6797, -10.0547,  -5.6797,  ..., -19.1094, -19.1094, -19.1094]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first_4_prompt_completion_ids_logits[0, :prompt_length, :].log_softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-13.0156, -17.0781, -16.8750,  ..., -14.4844, -14.4844, -14.4844],\n",
       "        [-11.8828, -12.4375, -16.3906,  ..., -21.2656, -21.2656, -21.2656],\n",
       "        [-13.7031, -16.0000, -17.9375,  ..., -20.6875, -20.7031, -20.6875],\n",
       "        ...,\n",
       "        [-16.4844, -17.0625, -16.3125,  ..., -24.0000, -24.0000, -24.0000],\n",
       "        [-12.9219,  -8.6016,  -3.7598,  ..., -20.4688, -20.4688, -20.4688],\n",
       "        [-12.6797, -10.0547,  -5.6875,  ..., -19.1094, -19.1094, -19.1094]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first_2_prompt_completion_ids_logits[0, :prompt_length, :].log_softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test the new approach\n",
    "# interleaved_prompt_inputs = {}\n",
    "# for k in prompt_inputs:\n",
    "#     interleaved_prompt_inputs[k] = prompt_inputs[k].repeat_interleave(G, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.inference_mode():\n",
    "#     prompt_out = model(**prompt_inputs, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-12.9922, -17.0938, -16.8594,  ..., -14.4922, -14.4922, -14.4922],\n",
       "        [-11.8750, -12.4453, -16.4062,  ..., -21.2812, -21.2812, -21.2812],\n",
       "        [-13.7031, -15.9922, -17.9375,  ..., -20.7031, -20.7031, -20.7031],\n",
       "        ...,\n",
       "        [-16.4844, -17.0625, -16.3281,  ..., -24.0000, -24.0000, -24.0000],\n",
       "        [-12.9141,  -8.6016,  -3.7539,  ..., -20.4844, -20.4844, -20.4844],\n",
       "        [-12.6797, -10.0547,  -5.6836,  ..., -19.1094, -19.1094, -19.1094]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt_out.logits[0, :, :].log_softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0625, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>),\n",
       " tensor(0.0060, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt_out.logits.shape\n",
    "\n",
    "# # verify that up to this step, the logits are the same\n",
    "# torch.allclose(first_4_prompt_completion_ids_logits[0, :prompt_length, :].log_softmax(dim=-1), prompt_out.logits[0, :, :].log_softmax(dim=-1))\n",
    "# diff = abs(first_4_prompt_completion_ids_logits[0, :prompt_length, :].log_softmax(dim=-1) - prompt_out.logits[0, :, :].log_softmax(dim=-1))\n",
    "# torch.max(diff), torch.mean(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Method \n",
    "\n",
    "```\n",
    "Start: 0.00 Gb\n",
    "Model loaded: 1.00 Gb\n",
    "after generation: 1.00 Gb\n",
    "after running logits: 10.26 Gb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 0.00 Gb\n",
      "Model loaded: 1.00 Gb\n",
      "after generation: 1.00 Gb\n",
      "after running old logps: 4.72 Gb\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the per-token log probabilities for the completions for the model and the reference model\n",
    "def get_per_token_logps(model, input_ids):\n",
    "    logits = model(input_ids).logits  # (B, L, V)\n",
    "    logits = logits[:, :-1, :]  # (B, L-1, V), exclude the last logit: it corresponds to the next token pred\n",
    "    input_ids = input_ids[:, 1:]  # (B, L-1), exclude the first input ID since we don't have logits for it\n",
    "    # Compute the log probabilities for the input tokens. Use a loop to reduce memory peak.\n",
    "    per_token_logps = []\n",
    "    for logits_row, input_ids_row in zip(logits, input_ids):\n",
    "        log_probs = logits_row.log_softmax(dim=-1)\n",
    "        token_log_prob = torch.gather(log_probs, dim=1, index=input_ids_row.unsqueeze(1)).squeeze(1)\n",
    "        per_token_logps.append(token_log_prob)\n",
    "    return torch.stack(per_token_logps)\n",
    "\n",
    "per_token_logps = get_per_token_logps(model, prompt_completion_ids)\n",
    "# Get rid of the prompt (-1 because of the shift done in get_per_token_logps)\n",
    "per_token_logps = per_token_logps[:, prompt_length - 1 :]\n",
    "\n",
    "add_vram_checkpoint(\"after running old logps\")\n",
    "\n",
    "# save the logits to disk\n",
    "torch.save(per_token_logps, \"logits.pt\")\n",
    "\n",
    "# 10.26 Gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "interleaved_prompt_inputs = []\n",
    "for k in prompt_inputs:\n",
    "    prompt_inputs[k] = prompt_inputs[k].repeat_interleave(G, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 32])\n"
     ]
    }
   ],
   "source": [
    "completion_ids = prompt_completion_ids[:, prompt_length:]\n",
    "print(completion_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# def compare_outputs_with_position_ids(\n",
    "#     prompt: str,\n",
    "#     position_ids_list: list,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Compare model outputs using different position_ids for the same input.\n",
    "    \n",
    "#     Args:\n",
    "#         prompt: Input text to tokenize and process\n",
    "#         position_ids_list: List of position_id tensors to compare\n",
    "#         model_name: Name of the HuggingFace model to use\n",
    "    \n",
    "#     Returns:\n",
    "#         Dictionary containing logits for each position_ids configuration\n",
    "#     \"\"\"\n",
    "#     # Tokenize input\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "#     input_ids = inputs[\"input_ids\"]\n",
    "#     attention_mask = inputs[\"attention_mask\"]\n",
    "    \n",
    "#     # Store results for each position_ids configuration\n",
    "#     results = {}\n",
    "\n",
    "#     # Baseline (without position_ids)\n",
    "#     results[\"baseline\"] = {\n",
    "#         \"position_ids\": None,\n",
    "#         \"logits\": model(input_ids, attention_mask=attention_mask).logits.squeeze()\n",
    "#     }\n",
    "    \n",
    "#     for i, position_ids in enumerate(position_ids_list):\n",
    "#         # Ensure position_ids matches input shape\n",
    "#         position_ids = position_ids.unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "#         # Forward pass with custom position_ids\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(\n",
    "#                 input_ids=input_ids,\n",
    "#                 attention_mask=attention_mask,\n",
    "#                 position_ids=position_ids\n",
    "#             )\n",
    "        \n",
    "#         results[f\"config_{i}\"] = {\n",
    "#             \"position_ids\": position_ids.squeeze().tolist(),\n",
    "#             \"logits\": outputs.logits.squeeze()\n",
    "#         }\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# # Example usage\n",
    "# prompt = \"The quick brown fox\"\n",
    "\n",
    "# # Create different position_ids configurations\n",
    "# normal_positions = torch.arange(4)  # [0, 1, 2, 3, 4]\n",
    "# reversed_positions = torch.tensor([3, 2, 1, 0])\n",
    "# repeated_positions = torch.tensor([0, 0, 0, 0])\n",
    "# shifted_positions = torch.tensor([11, 12, 13, 14])\n",
    "\n",
    "# position_ids_list = [\n",
    "#     normal_positions.to(model.device),\n",
    "#     reversed_positions.to(model.device),\n",
    "#     repeated_positions.to(model.device),\n",
    "#     shifted_positions.to(model.device),\n",
    "# ]\n",
    "\n",
    "# # Compare outputs\n",
    "# results = compare_outputs_with_position_ids(prompt, position_ids_list)\n",
    "\n",
    "# # Analyze differences\n",
    "# for config, data in results.items():\n",
    "#     print(f\"\\n{config}:\")\n",
    "#     print(f\"Position IDs: {data['position_ids']}\")\n",
    "#     print(f\"Logits shape: {data['logits'].shape}\")\n",
    "    \n",
    "#     # Compare logits with the first configuration\n",
    "#     if config != \"baseline\":\n",
    "#         logits_diff = torch.mean(torch.abs(\n",
    "#             data['logits'] - results['config_0']['logits']\n",
    "#         )).item()\n",
    "#         print(f\"Mean absolute difference from normal positions: {logits_diff:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Compute just with prompt cache but no position ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_logps_with_prompt_cache(\n",
    "    model: torch.nn.Module,\n",
    "    prompt_inputs: dict,\n",
    "    completion_ids: torch.LongTensor,\n",
    "    requires_grad_for_completion: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Forward pass on the prompt with torch.no_grad() to get `past_key_values`.\n",
    "    2) Forward pass (with or without grad) on the completion tokens using that cache.\n",
    "    3) Compute per-token log probabilities for the completion.\n",
    "\n",
    "    Args:\n",
    "      model (nn.Module): A causal LM (transformers.AutoModelForCausalLM) or similar.\n",
    "      prompt_inputs (dict): The dict of prompt tensors, e.g. {\"input_ids\", \"attention_mask\", ...}.\n",
    "      completion_ids (torch.LongTensor): Shape [B, completion_len].\n",
    "      requires_grad_for_completion (bool): Whether to enable gradient for the completion pass.\n",
    "\n",
    "    Returns:\n",
    "      per_token_logps (torch.FloatTensor): shape [B, completion_len],\n",
    "        where per_token_logps[b, t] is the logprob of completion_ids[b, t]\n",
    "        given all preceding tokens in the prompt + the partial completion up to t-1.\n",
    "    \"\"\"\n",
    "    \n",
    "    B = prompt_inputs[\"input_ids\"].size(0)\n",
    "    g = completion_ids.size(0) // B\n",
    "    print(g)\n",
    "\n",
    "    # 1) No-grad forward pass over prompt\n",
    "    with torch.no_grad():\n",
    "        prompt_out = model(**prompt_inputs, use_cache=True)\n",
    "        \n",
    "    \n",
    "\n",
    "        # Only keep the last prompt logit, immediately convert to log probabilities\n",
    "        prompt_last_logps = prompt_out.logits[:, -1:, :].log_softmax(dim=-1)\n",
    "        prompt_last_logps = prompt_last_logps.repeat_interleave(G, dim=0)\n",
    "\n",
    "        # Gather the these log probs as they relates to the first completion token\n",
    "        first_completion_token_logps = torch.gather(\n",
    "            prompt_last_logps,\n",
    "            dim=-1,\n",
    "            index=completion_ids[:, :1].unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "        \n",
    "        # Free up memory\n",
    "        del prompt_last_logps\n",
    "\n",
    "    # Interleave the past key values for the completion pass\n",
    "    prompt_out.past_key_values.batch_repeat_interleave(G)\n",
    "    \n",
    "    # 2) Forward the new completion tokens\n",
    "    if requires_grad_for_completion:\n",
    "        completion_out = model(\n",
    "            input_ids=completion_ids,\n",
    "            past_key_values=prompt_out.past_key_values,\n",
    "            use_cache=False,\n",
    "        )\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            completion_out = model(\n",
    "                input_ids=completion_ids,\n",
    "                past_key_values=prompt_out.past_key_values,\n",
    "                use_cache=False,\n",
    "            )\n",
    "\n",
    "    # 3) Process completion logits efficiently\n",
    "    logits = completion_out.logits[:, :-1, :]  # [B, completion_len - 1, vocab_size]\n",
    "    \n",
    "    # Convert to log probabilities and gather relevant tokens in one operation\n",
    "    completion_token_logps = torch.gather(\n",
    "        logits.log_softmax(dim=-1),\n",
    "        dim=-1,\n",
    "        index=completion_ids[:, 1:].unsqueeze(-1)\n",
    "    ).squeeze(-1)\n",
    "    \n",
    "    # Combine first token logps with the rest\n",
    "    per_token_logps = torch.cat([first_completion_token_logps, completion_token_logps], dim=1)\n",
    "    \n",
    "    return per_token_logps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get an interleaved version\n",
    "# interleaved_prompt_inputs = {}\n",
    "# for k in prompt_inputs:\n",
    "#     interleaved_prompt_inputs[k] = prompt_inputs[k].repeat_interleave(G, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 225, 64])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkv[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  -9.4219,   -3.9590,   -6.0898,  ...,   41.7812, -145.3750,\n",
       "            74.0000],\n",
       "         [ -10.8203,   -8.7734,   -7.0781,  ...,   41.7812, -145.3750,\n",
       "            74.0000],\n",
       "         [  -2.2695,  -10.0156,   -6.8242,  ...,   41.7812, -145.3750,\n",
       "            74.0000],\n",
       "         ...,\n",
       "         [  -1.7266,   -0.6914,   -2.8027,  ...,   40.9688, -145.3750,\n",
       "            72.4375],\n",
       "         [   8.4141,   -6.3438,   -5.5430,  ...,   40.6250, -145.1250,\n",
       "            71.2500],\n",
       "         [  11.1328,   -9.7500,   -6.6602,  ...,   41.9375, -145.5000,\n",
       "            73.6875]],\n",
       "\n",
       "        [[ -11.6641,    2.2402,    7.6758,  ...,   66.0625,  120.5000,\n",
       "          -128.8750],\n",
       "         [  35.6250,   -8.6562,    7.6016,  ...,   66.0625,  120.5000,\n",
       "          -128.8750],\n",
       "         [  50.1562,  -16.0312,    6.1953,  ...,   66.0625,  120.5000,\n",
       "          -128.8750],\n",
       "         ...,\n",
       "         [  50.2188,    6.0156,    5.1523,  ...,   64.5625,  118.1875,\n",
       "          -129.3750],\n",
       "         [  12.8594,   -5.5156,    6.7109,  ...,   63.0312,  117.0625,\n",
       "          -129.6250],\n",
       "         [ -34.0000,  -12.0547,    7.8359,  ...,   66.2500,  120.0625,\n",
       "          -128.6250]]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkv[0][0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkv.batch_repeat_interleave(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 225, 64])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interleaved_pkv[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old(\n",
    "    model: torch.nn.Module,\n",
    "    prompt_inputs: dict,\n",
    "    completion_ids: torch.LongTensor,\n",
    "    requires_grad_for_completion: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Forward pass on the prompt with torch.inference_mode() to get `past_key_values`.\n",
    "    2) Forward pass (with or without grad) on the completion tokens using that cache.\n",
    "    3) Compute per-token log probabilities for the completion.\n",
    "\n",
    "    Args:\n",
    "      model (nn.Module): A causal LM (transformers.AutoModelForCausalLM) or similar.\n",
    "      prompt_inputs (dict): The dict of prompt tensors, e.g. {\"input_ids\", \"attention_mask\", ...}.\n",
    "      completion_ids (torch.LongTensor): Shape [B, completion_len].\n",
    "      requires_grad_for_completion (bool): Whether to enable gradient for the completion pass.\n",
    "\n",
    "    Returns:\n",
    "      per_token_logps (torch.FloatTensor): shape [B, completion_len],\n",
    "        where per_token_logps[b, t] is the logprob of completion_ids[b, t]\n",
    "        given all preceding tokens in the prompt + the partial completion up to t-1.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) No-grad forward pass over prompt\n",
    "    with torch.inference_mode():\n",
    "        prompt_out = model(**prompt_inputs, use_cache=True)\n",
    "        # shape of prompt_out.logits is [B, prompt_len, vocab_size]\n",
    "        # shape of prompt_out.past_key_values is a tuple (one per layer) with shape reflecting batch=B.\n",
    "\n",
    "    # 2) Forward the new completion tokens.\n",
    "    # Optionally enable grads on the completion pass if `requires_grad_for_completion=True`.\n",
    "    if requires_grad_for_completion:\n",
    "        completion_out = model(\n",
    "            input_ids=completion_ids,\n",
    "            past_key_values=prompt_out.past_key_values,\n",
    "            use_cache=False  # you can set True or False depending on if you need caching beyond this\n",
    "        )\n",
    "    else:\n",
    "        # For a reference model or purely inference pass, do it all in inference_mode\n",
    "        with torch.inference_mode():\n",
    "            completion_out = model(\n",
    "                input_ids=completion_ids,\n",
    "                past_key_values=prompt_out.past_key_values,\n",
    "                use_cache=False\n",
    "            )\n",
    "\n",
    "    # shape of completion_out.logits is [B, completion_len, vocab_size]\n",
    "    logits = completion_out.logits\n",
    "\n",
    "    # 3) Compute per-token log probabilities\n",
    "    # Typically, we shift logits by 1 to align with the \"label\" tokens weâ€™re predicting.\n",
    "    # i.e., the logit at time t predicts completion_ids[:, t].\n",
    "    # So the logit dimension is (seq_len - 1) for the tokens that actually got predicted.\n",
    "    #\n",
    "    # If you want logprobs for *all* tokens including the very first one, you can pad or shift as desired.\n",
    "    # Commonly, we exclude the last logit because there's no \"next token\" for it, but\n",
    "    # it depends on how you handle your loss/advantages. We'll do the standard shift here.\n",
    "\n",
    "    # Exclude the final logit (nothing to predict after the last token)\n",
    "    logits = logits[:, :-1, :]  # shape [B, completion_len - 1, vocab_size]\n",
    "    \n",
    "    # We actually need the \"last\" logit from prompt_out because that corresponds to the first token in completion_ids\n",
    "    logits = torch.cat([prompt_out.logits[:, -1:, :], logits], dim=1)\n",
    "\n",
    "    # Compute the log probabilities for the input tokens. Use a loop to reduce memory peak.\n",
    "    per_token_logps = []\n",
    "    for logits_row, input_ids_row in zip(logits, completion_ids):\n",
    "        log_probs = logits_row.log_softmax(dim=-1)\n",
    "        token_log_prob = torch.gather(log_probs, dim=1, index=input_ids_row.unsqueeze(1)).squeeze(1)\n",
    "        per_token_logps.append(token_log_prob)\n",
    "    \n",
    "    return torch.stack(per_token_logps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take in consideration of position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# def compute_logps_with_prompt_cache_and_position_ids(\n",
    "#     model: nn.Module,\n",
    "#     prompt_inputs: dict,\n",
    "#     completion_ids: torch.LongTensor,\n",
    "#     requires_grad_for_completion: bool = True,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     1) Run a no-grad forward pass on the prompt to get `past_key_values`.\n",
    "#     2) Run a second forward pass on the completion tokens, with position IDs\n",
    "#        continuing from the prompt length.\n",
    "#     3) Return a tensor of per-token log probabilities for the completion.\n",
    "\n",
    "#     Args:\n",
    "#       model (nn.Module): a causal LM (e.g. transformers.AutoModelForCausalLM).\n",
    "#       prompt_inputs (dict): must contain:\n",
    "#           - \"input_ids\": [B, prompt_len]\n",
    "#           - \"attention_mask\": [B, prompt_len]\n",
    "#         If you have more keys (e.g. token_type_ids), include them similarly.\n",
    "#       completion_ids (torch.LongTensor): shape [B, completion_len].\n",
    "#       requires_grad_for_completion (bool): Whether to enable gradient for\n",
    "#         the completion pass.\n",
    "\n",
    "#     Returns:\n",
    "#       per_token_logps (torch.FloatTensor): shape [B, completion_len].\n",
    "#         per_token_logps[b,t] is the log-prob of completion_ids[b,t]\n",
    "#         given the prompt + the partial completion up to t-1.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # ---------------------------\n",
    "#     # 1) Prompt pass (no_grad)\n",
    "#     # ---------------------------\n",
    "#     with torch.inference_mode():\n",
    "#         prompt_out = model(\n",
    "#             input_ids=prompt_inputs[\"input_ids\"],\n",
    "#             attention_mask=prompt_inputs[\"attention_mask\"],\n",
    "#             use_cache=True,\n",
    "#         )\n",
    "#         # We mainly need prompt_out.past_key_values for the next pass\n",
    "#         # prompt_out.logits is typically not used here (or only for logging).\n",
    "\n",
    "#     # Batch size, lengths\n",
    "#     B = prompt_inputs[\"input_ids\"].size(0)\n",
    "#     prompt_len = prompt_inputs[\"input_ids\"].size(1)\n",
    "#     completion_len = completion_ids.size(1)\n",
    "\n",
    "#     # ---------------------------\n",
    "#     # 2) Completion pass\n",
    "#     # ---------------------------\n",
    "#     # We explicitly define position_ids so that the first completion token is\n",
    "#     # assigned position = prompt_len, second is prompt_len+1, and so on.\n",
    "#     position_ids = torch.arange(\n",
    "#         prompt_len, prompt_len + completion_len,\n",
    "#         device=completion_ids.device, dtype=torch.long\n",
    "#     ).unsqueeze(0).repeat(B, 1)  # shape [B, completion_len]\n",
    "\n",
    "#     print(completion_ids.shape)\n",
    "#     print(position_ids)\n",
    "\n",
    "#     # completion_attention_mask = torch.ones_like(completion_ids, dtype=prompt_inputs[\"attention_mask\"].dtype)\n",
    "#     # completion_attention_mask[completion_ids == tokenizer.pad_token_id] = 0\n",
    "\n",
    "#     if requires_grad_for_completion:\n",
    "#         completion_out = model(\n",
    "#             input_ids=completion_ids,\n",
    "#             past_key_values=prompt_out.past_key_values,\n",
    "#             position_ids=position_ids,\n",
    "#             # attention_mask=completion_attention_mask,\n",
    "#             use_cache=False,\n",
    "#         )\n",
    "#     else:\n",
    "#         # Fully inference mode\n",
    "#         with torch.inference_mode():\n",
    "#             completion_out = model(\n",
    "#                 input_ids=completion_ids,\n",
    "#                 past_key_values=prompt_out.past_key_values,\n",
    "#                 position_ids=position_ids,\n",
    "#                 # attention_mask=completion_attention_mask,\n",
    "#                 use_cache=False,\n",
    "#             )\n",
    "\n",
    "#     # completion_out.logits: [B, completion_len, vocab_size]\n",
    "#     logits = completion_out.logits\n",
    "\n",
    "#     # ---------------------------\n",
    "#     # 3) Compute per-token log-probs\n",
    "#     # ---------------------------\n",
    "#     # Standard practice: skip the final logit since there's no \"next token\" after the last.\n",
    "#     # logits[:, 0, :] corresponds to the distribution predicting completion_ids[:, 0].\n",
    "#     # logits[:, 1, :] -> completion_ids[:, 1], etc.\n",
    "#     # We'll shift by 1 to align tokens with the \"logit that predicted them\".\n",
    "\n",
    "#     logits = logits[:, :-1, :]       # shape [B, completion_len - 1, vocab_size]\n",
    "#     logits = torch.cat([prompt_out.logits[:, -1:, :], logits], dim=1)\n",
    "    \n",
    "#     # Compute the log probabilities for the input tokens. Use a loop to reduce memory peak.\n",
    "#     per_token_logps = []\n",
    "#     for logits_row, input_ids_row in zip(logits, completion_ids):\n",
    "#         log_probs = logits_row.log_softmax(dim=-1)\n",
    "#         token_log_prob = torch.gather(log_probs, dim=1, index=input_ids_row.unsqueeze(1)).squeeze(1)\n",
    "#         per_token_logps.append(token_log_prob)\n",
    "    \n",
    "#     return torch.stack(per_token_logps)\n",
    "\n",
    "#     # labels = completion_ids[:, 1:]   # shape [B, completion_len - 1]\n",
    "\n",
    "#     # log_probs = logits.log_softmax(dim=-1)  # [B, c_len - 1, vocab_size]\n",
    "#     # gathered = torch.gather(log_probs, dim=-1, index=labels.unsqueeze(-1)).squeeze(-1)\n",
    "#     # # shape [B, c_len - 1]\n",
    "\n",
    "#     # # Optionally pad to shape [B, completion_len], so\n",
    "#     # # per_token_logps[b,0] is the log-prob of the first token\n",
    "#     # zero_pad = torch.zeros((B, 1), device=gathered.device, dtype=gathered.dtype)\n",
    "#     # per_token_logps = torch.cat([zero_pad, gathered], dim=1)\n",
    "\n",
    "#     # return per_token_logps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Start: 0.00 Gb\n",
      "Model loaded: 1.00 Gb\n",
      "after generation: 1.00 Gb\n",
      "after running old logps: 4.72 Gb\n",
      "after running new metdho: 5.34 Gb\n",
      "after running new metdho: 6.80 Gb\n",
      "after running new metdho: 7.41 Gb\n"
     ]
    }
   ],
   "source": [
    "new_log_probs = compute_logps_with_prompt_cache(\n",
    "    model=model,\n",
    "    prompt_inputs=prompt_inputs,\n",
    "    completion_ids=completion_ids,\n",
    "    requires_grad_for_completion=True\n",
    ")\n",
    "\n",
    "add_vram_checkpoint(\"after running new metdho\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.0547e+00, -5.8008e-01, -4.0747e-01, -4.3750e-01, -1.8091e-01,\n",
       "         -3.3594e+00, -2.3535e+00, -2.1545e-02, -1.8250e-01, -2.4841e-02,\n",
       "         -3.8594e+00, -3.1602e+00, -3.0664e-01, -2.2766e-01, -4.6328e+00,\n",
       "         -8.3191e-02, -2.1439e-03, -2.7930e-01, -2.7447e-03, -6.7711e-04,\n",
       "         -2.3279e-01, -6.9763e-02, -3.5973e-03, -4.0558e-02, -5.7650e-04,\n",
       "         -4.4060e-04, -8.7341e-02, -1.2177e-02, -1.3876e-03, -4.9248e-03,\n",
       "         -4.3631e-04, -2.0134e-04],\n",
       "        [-4.4531e+00, -4.8984e+00, -3.9473e+00, -6.0547e-01, -1.6963e+00,\n",
       "         -3.4814e-01, -3.5405e-04, -9.9869e-03, -6.2218e-03, -7.7896e-03,\n",
       "         -1.8988e-03, -7.6256e-03, -2.3592e-04, -1.4305e-04, -2.5253e-03,\n",
       "         -9.2545e-03, -2.1765e-01, -1.2231e-01, -4.5593e-02, -1.0490e-05,\n",
       "         -4.4346e-05, -9.2864e-05, -1.8394e-04, -8.3208e-05, -5.5389e-03,\n",
       "         -7.7934e-03, -2.4185e-03, -3.4022e-04, -1.6284e-04, -1.8728e-04,\n",
       "         -2.0218e-04, -8.0185e-03],\n",
       "        [-1.7812e+00, -1.4185e-01, -3.6348e+00, -1.7285e+00, -6.3359e+00,\n",
       "         -2.6348e+00, -6.4307e-01, -1.2344e+00, -2.1802e-01, -1.7080e+00,\n",
       "         -2.9621e-03, -4.3652e-01, -2.3086e+00, -5.5391e+00, -5.0018e-02,\n",
       "         -8.8930e-04, -6.4219e+00, -3.0352e+00, -2.5859e+00, -1.3992e-02,\n",
       "         -1.8402e-02, -1.2061e+00, -4.3115e-01, -4.3213e-01, -1.8516e+00,\n",
       "         -1.0020e+00, -8.1299e-02, -1.2939e+00, -4.4556e-03, -3.6640e-03,\n",
       "         -1.1025e+00, -4.8682e-01],\n",
       "        [-4.4531e+00, -4.6484e+00, -1.3171e-01, -2.8379e+00, -3.1504e+00,\n",
       "         -2.2617e+00, -4.9609e+00, -1.3936e+00, -2.5215e+00, -1.2871e+00,\n",
       "         -3.4609e+00, -2.2715e+00, -6.8115e-02, -6.0692e-03, -5.4207e-03,\n",
       "         -6.7520e-03, -3.4370e-03, -9.2316e-03, -1.0462e-03, -3.8743e-04,\n",
       "         -8.5545e-04, -9.1076e-04, -3.2544e-04, -9.6798e-04, -6.2323e-04,\n",
       "         -4.9210e-04, -3.1257e-04, -5.5194e-05, -7.3552e-05, -2.2244e-04,\n",
       "         -4.5586e-04, -1.4191e-03],\n",
       "        [-1.5430e+01, -8.1953e+00, -4.7119e-01, -4.2456e-01, -1.0771e+00,\n",
       "         -1.9326e+00, -1.3539e+01, -6.4844e-01, -2.9831e-02, -1.2305e+00,\n",
       "         -9.7422e+00, -1.1320e+01, -1.0203e+01, -1.1164e+01, -1.1172e+01,\n",
       "         -1.1070e+01, -1.0992e+01, -1.0820e+01, -1.0805e+01, -1.0781e+01,\n",
       "         -1.0664e+01, -1.0500e+01, -1.0422e+01, -1.0359e+01, -1.0219e+01,\n",
       "         -1.0203e+01, -1.0211e+01, -1.0117e+01, -1.0055e+01, -9.9531e+00,\n",
       "         -9.8984e+00, -9.8203e+00],\n",
       "        [-1.5305e+01, -9.5469e+00, -4.8008e+00, -8.2080e-01, -3.7383e+00,\n",
       "         -2.2891e+00, -4.6600e-02, -1.2108e-02, -4.2539e+00, -7.1533e-01,\n",
       "         -8.2578e+00, -3.5566e+00, -6.1920e-02, -2.4922e+00, -3.0469e+00,\n",
       "         -2.7031e+00, -2.1738e+00, -3.4241e-02, -3.1230e+00, -4.1680e+00,\n",
       "         -2.0566e+00, -2.5299e-02, -4.3008e+00, -1.0602e+01, -1.3545e+00,\n",
       "         -1.8965e+00, -3.4805e+00, -6.0791e-01, -6.9824e-01, -3.1836e+00,\n",
       "         -2.0664e+00, -7.3164e+00],\n",
       "        [-1.2297e+01, -9.4531e+00, -4.2109e+00, -8.6484e+00, -1.7275e+00,\n",
       "         -5.3828e+00, -1.1938e+01, -6.7017e-02, -5.1953e+00, -1.9385e+00,\n",
       "         -4.6289e+00, -3.3398e+00, -7.2412e-01, -2.2148e+00, -1.9639e+00,\n",
       "         -5.8086e+00, -1.0602e-01, -2.3340e+00, -1.6562e+00, -4.4507e-01,\n",
       "         -6.4941e-01, -3.4766e+00, -3.1660e+00, -3.2500e+00, -1.2962e-02,\n",
       "         -3.8496e+00, -3.4375e+00, -1.5703e+00, -6.5381e-01, -2.3379e+00,\n",
       "         -8.9746e-01, -3.0645e+00],\n",
       "        [-1.2297e+01, -9.4531e+00, -4.0781e+00, -2.8564e-01, -1.3340e+00,\n",
       "         -3.0488e+00, -1.5727e+01, -1.3604e+00, -4.3086e+00, -2.9316e+00,\n",
       "         -8.3643e-01, -2.9141e+00, -1.4756e+00, -6.1797e+00, -2.7461e+00,\n",
       "         -2.2227e+00, -6.9844e+00, -5.1719e+00, -6.1953e+00, -3.4453e+00,\n",
       "         -1.5684e+00, -1.5986e+00, -1.1729e+00, -1.7617e+00, -2.1016e+00,\n",
       "         -7.4072e-01, -4.1641e+00, -4.5742e+00, -4.3164e+00, -9.8828e-01,\n",
       "         -9.8291e-01, -3.2148e+00]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_token_logps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.0508e+00, -5.8105e-01, -4.0967e-01, -4.3506e-01, -1.8091e-01,\n",
       "         -3.3555e+00, -2.3535e+00, -2.1896e-02, -1.8323e-01, -2.5131e-02,\n",
       "         -3.8535e+00, -3.1582e+00, -3.0688e-01, -2.2363e-01, -4.6445e+00,\n",
       "         -8.3435e-02, -2.1210e-03, -2.8638e-01, -2.7580e-03, -6.7949e-04,\n",
       "         -2.3279e-01, -6.9763e-02, -3.5801e-03, -4.0466e-02, -5.7650e-04,\n",
       "         -4.3869e-04, -8.7341e-02, -1.2344e-02, -1.3924e-03, -4.9171e-03,\n",
       "         -4.3249e-04, -2.0075e-04],\n",
       "        [-4.4570e+00, -4.9023e+00, -3.9434e+00, -6.0254e-01, -1.7012e+00,\n",
       "         -3.5010e-01, -3.5596e-04, -9.9792e-03, -6.2637e-03, -7.9193e-03,\n",
       "         -1.8902e-03, -7.7171e-03, -2.3568e-04, -1.4412e-04, -2.5616e-03,\n",
       "         -9.2545e-03, -2.1753e-01, -1.2280e-01, -4.6295e-02, -1.0610e-05,\n",
       "         -4.5061e-05, -9.3937e-05, -1.8477e-04, -8.3447e-05, -5.5428e-03,\n",
       "         -7.7248e-03, -2.4471e-03, -3.3903e-04, -1.6260e-04, -1.8418e-04,\n",
       "         -2.0194e-04, -8.1406e-03],\n",
       "        [-1.7764e+00, -1.4136e-01, -3.6406e+00, -1.7236e+00, -6.3438e+00,\n",
       "         -2.6348e+00, -6.4453e-01, -1.2324e+00, -2.1802e-01, -1.7090e+00,\n",
       "         -2.9106e-03, -4.3750e-01, -2.3066e+00, -5.5391e+00, -4.9988e-02,\n",
       "         -9.0170e-04, -6.4336e+00, -3.0332e+00, -2.5918e+00, -1.3992e-02,\n",
       "         -1.8402e-02, -1.1953e+00, -4.3140e-01, -4.3188e-01, -1.8516e+00,\n",
       "         -1.0010e+00, -8.2092e-02, -1.2822e+00, -4.5242e-03, -3.7174e-03,\n",
       "         -1.1025e+00, -4.8657e-01],\n",
       "        [-4.4570e+00, -4.6523e+00, -1.3159e-01, -2.8340e+00, -3.1426e+00,\n",
       "         -2.2598e+00, -4.9648e+00, -1.3936e+00, -2.5234e+00, -1.2949e+00,\n",
       "         -3.4688e+00, -2.2695e+00, -6.7688e-02, -5.9853e-03, -5.3940e-03,\n",
       "         -6.6948e-03, -3.4180e-03, -9.2392e-03, -1.0433e-03, -3.8743e-04,\n",
       "         -8.4305e-04, -9.1028e-04, -3.2544e-04, -9.6512e-04, -6.2323e-04,\n",
       "         -4.9114e-04, -3.0923e-04, -5.4717e-05, -7.2718e-05, -2.2352e-04,\n",
       "         -4.5180e-04, -1.4143e-03],\n",
       "        [-4.2852e+00, -5.4443e-01, -1.5015e-01, -2.2510e-01, -3.2275e-01,\n",
       "         -9.9316e-01, -5.3086e+00, -1.6159e-02, -1.1528e-02, -3.1567e-01,\n",
       "         -8.8037e-01, -1.1898e+01, -1.4248e-03, -2.0172e-02, -3.5132e-01,\n",
       "         -8.2129e-01, -1.0977e+00, -1.2188e+00, -1.2979e+00, -1.3154e+00,\n",
       "         -1.2920e+00, -1.2373e+00, -1.2383e+00, -1.2197e+00, -1.1543e+00,\n",
       "         -1.1152e+00, -1.1641e+00, -1.1445e+00, -1.0957e+00, -1.0029e+00,\n",
       "         -9.7998e-01, -9.3164e-01],\n",
       "        [-3.0410e+00, -8.7812e+00, -3.2891e+00, -6.3086e-01, -4.2578e+00,\n",
       "         -2.1934e+00, -2.2736e-02, -1.1642e-02, -3.4512e+00, -4.3970e-01,\n",
       "         -8.2656e+00, -2.3203e+00, -4.4525e-02, -1.5762e+00, -1.4287e+00,\n",
       "         -2.5254e+00, -1.5850e+00, -3.1403e-02, -3.3691e+00, -5.4922e+00,\n",
       "         -2.7402e+00, -2.0309e-02, -5.7148e+00, -9.6562e+00, -1.1104e+00,\n",
       "         -1.7725e+00, -3.1562e+00, -7.9834e-01, -6.4307e-01, -2.9980e+00,\n",
       "         -2.1973e+00, -4.3945e+00],\n",
       "        [-1.2754e+00, -2.6489e-01, -2.9492e+00, -7.8945e+00, -1.4150e+00,\n",
       "         -3.5781e+00, -1.1367e+01, -1.4404e-02, -4.6406e+00, -1.8164e+00,\n",
       "         -7.5391e+00, -2.3984e+00, -4.1699e-01, -1.6406e+00, -2.1758e+00,\n",
       "         -5.7891e+00, -1.0303e-01, -1.6289e+00, -6.9727e-01, -4.4531e-01,\n",
       "         -5.4883e-01, -3.2852e+00, -2.1426e+00, -1.4395e+00, -1.4534e-02,\n",
       "         -3.4102e+00, -1.8701e+00, -9.2285e-01, -1.2598e-01, -2.8984e+00,\n",
       "         -8.0664e-01, -2.7188e+00],\n",
       "        [-1.2754e+00, -2.6489e-01, -4.1602e+00, -2.3767e-01, -1.0693e+00,\n",
       "         -2.5078e+00, -1.4898e+01, -1.7686e+00, -6.1914e+00, -2.0566e+00,\n",
       "         -7.1582e-01, -2.9844e+00, -9.4141e-01, -5.1992e+00, -2.7227e+00,\n",
       "         -2.5684e+00, -7.4570e+00, -4.6875e+00, -6.4492e+00, -3.1992e+00,\n",
       "         -1.7793e+00, -1.2939e+00, -1.3105e+00, -1.4883e+00, -1.4414e+00,\n",
       "         -3.6816e-01, -4.5312e+00, -3.3984e+00, -4.5234e+00, -9.0918e-01,\n",
       "         -8.3887e-01, -1.7676e+00]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify if per_token_logps and new_logprobs are the same\n",
    "torch.allclose(per_token_logps, new_log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.9062e-03, 9.7656e-04, 2.1973e-03, 2.4414e-03, 0.0000e+00, 3.9062e-03,\n",
       "         0.0000e+00, 3.5095e-04, 7.3242e-04, 2.8992e-04, 5.8594e-03, 1.9531e-03,\n",
       "         2.4414e-04, 4.0283e-03, 1.1719e-02, 2.4414e-04, 2.2888e-05, 7.0801e-03,\n",
       "         1.3351e-05, 2.3842e-06, 0.0000e+00, 0.0000e+00, 1.7166e-05, 9.1553e-05,\n",
       "         0.0000e+00, 1.9073e-06, 0.0000e+00, 1.6785e-04, 4.7684e-06, 7.6294e-06,\n",
       "         3.8147e-06, 5.9605e-07],\n",
       "        [3.9062e-03, 3.9062e-03, 3.9062e-03, 2.9297e-03, 4.8828e-03, 1.9531e-03,\n",
       "         1.9073e-06, 7.6294e-06, 4.1962e-05, 1.2970e-04, 8.5831e-06, 9.1553e-05,\n",
       "         2.3842e-07, 1.0729e-06, 3.6240e-05, 0.0000e+00, 1.2207e-04, 4.8828e-04,\n",
       "         7.0190e-04, 1.1921e-07, 7.1526e-07, 1.0729e-06, 8.3447e-07, 2.3842e-07,\n",
       "         3.8147e-06, 6.8665e-05, 2.8610e-05, 1.1921e-06, 2.3842e-07, 3.0994e-06,\n",
       "         2.3842e-07, 1.2207e-04],\n",
       "        [4.8828e-03, 4.8828e-04, 5.8594e-03, 4.8828e-03, 7.8125e-03, 0.0000e+00,\n",
       "         1.4648e-03, 1.9531e-03, 0.0000e+00, 9.7656e-04, 5.1498e-05, 9.7656e-04,\n",
       "         1.9531e-03, 0.0000e+00, 3.0518e-05, 1.2398e-05, 1.1719e-02, 1.9531e-03,\n",
       "         5.8594e-03, 0.0000e+00, 0.0000e+00, 1.0742e-02, 2.4414e-04, 2.4414e-04,\n",
       "         0.0000e+00, 9.7656e-04, 7.9346e-04, 1.1719e-02, 6.8665e-05, 5.3406e-05,\n",
       "         0.0000e+00, 2.4414e-04],\n",
       "        [3.9062e-03, 3.9062e-03, 1.2207e-04, 3.9062e-03, 7.8125e-03, 1.9531e-03,\n",
       "         3.9062e-03, 0.0000e+00, 1.9531e-03, 7.8125e-03, 7.8125e-03, 1.9531e-03,\n",
       "         4.2725e-04, 8.3923e-05, 2.6703e-05, 5.7220e-05, 1.9073e-05, 7.6294e-06,\n",
       "         2.8610e-06, 0.0000e+00, 1.2398e-05, 4.7684e-07, 0.0000e+00, 2.8610e-06,\n",
       "         0.0000e+00, 9.5367e-07, 3.3379e-06, 4.7684e-07, 8.3447e-07, 1.0729e-06,\n",
       "         4.0531e-06, 4.7684e-06],\n",
       "        [1.1141e+01, 7.6523e+00, 3.2104e-01, 1.9946e-01, 7.5439e-01, 9.3945e-01,\n",
       "         8.2344e+00, 6.3232e-01, 1.8311e-02, 9.1504e-01, 8.8594e+00, 5.7812e-01,\n",
       "         1.0203e+01, 1.1141e+01, 1.0820e+01, 1.0250e+01, 9.8906e+00, 9.6016e+00,\n",
       "         9.5078e+00, 9.4688e+00, 9.3750e+00, 9.2656e+00, 9.1875e+00, 9.1406e+00,\n",
       "         9.0625e+00, 9.0859e+00, 9.0469e+00, 8.9688e+00, 8.9609e+00, 8.9531e+00,\n",
       "         8.9219e+00, 8.8906e+00],\n",
       "        [1.2266e+01, 7.6562e-01, 1.5117e+00, 1.8994e-01, 5.1953e-01, 9.5703e-02,\n",
       "         2.3865e-02, 4.6539e-04, 8.0273e-01, 2.7563e-01, 7.8125e-03, 1.2363e+00,\n",
       "         1.7395e-02, 9.1602e-01, 1.6182e+00, 1.7773e-01, 5.8887e-01, 2.8381e-03,\n",
       "         2.4609e-01, 1.3242e+00, 6.8359e-01, 4.9896e-03, 1.4141e+00, 9.4531e-01,\n",
       "         2.4414e-01, 1.2402e-01, 3.2422e-01, 1.9043e-01, 5.5176e-02, 1.8555e-01,\n",
       "         1.3086e-01, 2.9219e+00],\n",
       "        [1.1023e+01, 9.1875e+00, 1.2617e+00, 7.5391e-01, 3.1250e-01, 1.8047e+00,\n",
       "         5.7031e-01, 5.2612e-02, 5.5469e-01, 1.2207e-01, 2.9102e+00, 9.4141e-01,\n",
       "         3.0713e-01, 5.7422e-01, 2.1191e-01, 1.9531e-02, 2.9907e-03, 7.0508e-01,\n",
       "         9.5898e-01, 2.4414e-04, 1.0059e-01, 1.9141e-01, 1.0234e+00, 1.8105e+00,\n",
       "         1.5717e-03, 4.3945e-01, 1.5674e+00, 6.4746e-01, 5.2783e-01, 5.6055e-01,\n",
       "         9.0820e-02, 3.4570e-01],\n",
       "        [1.1023e+01, 9.1875e+00, 8.2031e-02, 4.7974e-02, 2.6465e-01, 5.4102e-01,\n",
       "         8.2812e-01, 4.0820e-01, 1.8828e+00, 8.7500e-01, 1.2061e-01, 7.0312e-02,\n",
       "         5.3418e-01, 9.8047e-01, 2.3438e-02, 3.4570e-01, 4.7266e-01, 4.8438e-01,\n",
       "         2.5391e-01, 2.4609e-01, 2.1094e-01, 3.0469e-01, 1.3770e-01, 2.7344e-01,\n",
       "         6.6016e-01, 3.7256e-01, 3.6719e-01, 1.1758e+00, 2.0703e-01, 7.9102e-02,\n",
       "         1.4404e-01, 1.4473e+00]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AbsBackward0>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = abs(per_token_logps - new_log_probs)\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.2656, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(abs(per_token_logps - new_log_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.6265e-02, 2.3346e-02, 7.5989e-02, 4.5685e-02, 2.3000e+02, 2.9812e+01,\n",
       "        3.9594e+01, 3.4062e+01], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff.shape\n",
    "torch.sum(diff, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(abs(new_log_probs - probs_without_position_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(abs(new_log_probs - probs_without_position_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
